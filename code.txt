### Data
"""pyodbc doc: http://mkleehammer.github.io/pyodbc/
"""

import os
import pyodbc
import pandas as pd
import numpy as np
import datetime
import numbers
import xlrd
import dateutil.parser
#import struct


### Version: get currently running version from Control DB ####################
control_db_name = 'Control.accdb'
control_db_location =  os.getcwd() + '\\'
#    control_db_location = '\\\\ntfmao01\\ERMShared\\Treasury Risk\\' + \
#        'Portfolio VaR\\POINT Tools\\Scenarios\\Production\\'
conn = pyodbc.connect('DRIVER={Microsoft Access Driver (*.mdb, *.accdb)};'+
        'DBQ=' + control_db_location + control_db_name)
running_version_qry = "select * from Versions where (Running > 0);"
running_version_df = pd.read_sql(running_version_qry,conn)
if len(running_version_df) > 1:
    print '\nError with Versions table, errors may occur with data \n'
running_version = running_version_df['Overall'][0]
list_db_version = running_version_df['list_databases'][0]
list_tables_version = running_version_df['list_tables'][0]
### DB and tables information: get data from Control DB #######################
### Get databases information
db_info_qry = "select * from List_Databases where (Version = '" +\
    list_db_version + "');"
db_info = pd.read_sql(db_info_qry,conn) 
### Get tables information
tables_info_qry = "select * from List_Tables where (Version = '" +\
    list_tables_version + "');"
tables_info = pd.read_sql(tables_info_qry,conn) 
conn.close()

### Database formats: for file fetching conditions ############################
date_formats = ['DATE','DATETIME','TIMESTAMP','TIME','YEAR']
number_formats = ['TINYINT','SMALLINT','MEDIUMINT','INT','BIGINT','FLOAT',
    'DOUBLE','DECIMAL']
text_formats = ['CHAR','VARCHAR','TINYTEXT','TEXT','BLOB','MEDIUMTEXT',
    'MEDIUMBLOB','LONGTEXT','LONGBLOB','ENUM','SET']
# Separator to use in a query depending on column type
cols_type_separator = {'date':"#",'number':"",'text':"'"}
### End of database information ###############################################

###############################################################################  
def table_col_types(filename,db_cursor):
    """ filename: name of file (as used in code) for which we want to get the 
    list of columns and the type of each
    db_cursor: cursor on the database in which our filetype's table is located
    Fetches the information specific to a table and returns a dictionary
    containing the list of columns as keys and the type of each column as
    values (date, number or text)
    """
    # Get name of table as it appears in the Access database
    tablename = tables_info.loc[
        tables_info.Code_TableName == filename,'DB_TableName'].values[0]
        
    # Get cursor on the table: this object can be iterated  
    table_info_iterable = db_cursor.columns(table = tablename)
    
    # All information on the table's structure
    table_info_raw = pd.DataFrame.from_records(table_info_iterable)
    
    # Details on contents of object table_info_raw
    table_info_desc = pd.DataFrame(list(table_info_iterable.description))
    
    # Get index of columns contains our DB table's columns name and types
    # in the object table_info_raw
    table_info_name_index = np.where(table_info_desc[0] == 'column_name')[0][0] 
    table_info_type_index = np.where(table_info_desc[0] == 'type_name')[0][0]
    
    # Keep column names and types only in result dictionary
    cols_dict = {}
    for i in range(len(table_info_raw)):
        col_name = table_info_raw.iloc[i,table_info_name_index]
        col_type_val = table_info_raw.iloc[i,table_info_type_index]
        if col_type_val in date_formats:
            col_type = 'date'
        elif col_type_val in number_formats:
            col_type = 'number'
        elif col_type_val in text_formats:
            col_type = 'text'
        else:
            col_type = 'NewColType: ' +  np.str(col_type_val)
        cols_dict.update({col_name:col_type})
    
    return cols_dict
###############################################################################  
def fetch_files(files,cond={},columns={}):
    """ 
    files: list of the files needed. The function returns the files in the 
    same order they were passed in. Format: either a single string with the
    name of the file to fetch, eg: files = 'HFV_daily', or a list of the files
    to fecth, eg: files = ['FRY14Q','country_code_map']. If a list is passed 
    as input, a list is returned, otherwise if a single string is passed as
    input the dataframe result is returned directly.
    cond: dictionary containing the conditions to be passed for each file
    that is to be fecthed. The keys must match the file names passed in the
    argument files, and the values are lists of list, so that each sublist
    contains three elements: condition on which the column applies, then the
    condition operator (=,>,<,<>), and finally the value of the condition. If
    several values are possible (OR condition in SQL) then the possible values
    need to be in the same condition list, as a list themselves.
    So a condition to fetch the FRY14Q for dates greater than 6/30/2016 would be
    passed as: cond = {'FRY14Q':[['AsOfDate','>','6/30/2016']]}. To get all
    versions of data existing, the condition should be passed using: 
    cond = {'FRY14Q':[['Version','','']]}. To get several CUSIPs from an HFV
    file the condition would be:
    cond = {'HFV_monthly':[['Security','=',['CusipA','CusipB']]]}
    columns: dictionary containing the columns of a file to be fetched. File
    names should be passed as keys, and the columns as values in a list. If 
    no column argument is passed for a file then all columns are returned. So
    for specific columns of the FRY14Q: columns = {'FRY14Q':['AsOfDate',
    'Security','Currency']}.
    This function fetches the data passed in the files variable based on
    the defined conditions, and returns the specified conditions. By default
    the condition that the Version of data fetched is equal the Version running
    is added, it can be removed by passing a Version condition with no other
    argument.
    This function can be called:
    y = fetch_files('filename',...), y = fetch_files(['filename']) or with 
    several files as input: x,y,z = fetch_files(['file1','file2','file3'],...)
    """
    
    ### Check type of input: if files is a string, convert to list
    single_input = 0
    if isinstance(files,str):
        files = [files]
        single_input = 1
    
    ### Check the input files requested are indeed in the databases, otherwise
    ### exit the function right away
    if not(set(files).issubset(set(tables_info['Code_TableName']))):
        for filename in files:
            if not(filename in tables_info['Code_TableName'].tolist()):
                print '\n File ' + filename + ' could not be found \n'
                return
    
    ### Loop through files
    results_list = []
    for filename in files:
        # Find which database and tablename the file to fetch is in
        db = tables_info.loc[tables_info.Code_TableName == filename,'DB_Name'].values[0]
        db_location = db_info.loc[db_info.DB_Name == db,'FileLocation'].values[0]
        db_name = db_info.loc[db_info.DB_Name == db,'FileName'].values[0]
        tablename = tables_info.loc[\
            tables_info.Code_TableName == filename,'DB_TableName'].values[0]
        
        ### Create connection
        conn = pyodbc.connect('DRIVER={Microsoft Access Driver (*.mdb, *.accdb)};'+
            'DBQ=' + db_location + db_name)
        cursor = conn.cursor()
        
        ### Get table information: columns list and columns types
        col_types = table_col_types(filename,cursor)
        
        ### Create select query based on passed arguments
        ## Get list of columns, check they are indeed in the table queried
        columns_needed = '*'    
        if filename in columns.keys():
            if set(columns[filename]).issubset(set(col_types.keys())):
                columns_needed = '[' + '],['.join(columns[filename]) + ']'
            else:
                print '\nError with the columns provided for ' + filename + \
                    '. All columns are selected instead.\n'
            
        ## Get conditions, checking if Version is passed
        # Define condition of version by default 
        file_running_version = running_version_df[filename][0]
        version_cond = "(Version = '" + file_running_version + "')"
        qry_cond = ''
        if filename in cond.keys():
            for iCond in range(0,len(cond[filename])):
                # Get column, operator and value of condition
                cond_col = cond[filename][iCond][0]
                cond_oper = cond[filename][iCond][1]
                cond_val = cond[filename][iCond][2]
                
                # A condition on version is treated separately
                if cond_col == 'Version':
                    if cond_oper == '':
                        version_cond = ''
                    else:
                        version_cond = "(Version " + cond_oper +\
                            " '" + cond_val + "')"
                else:
                    if cond_col in col_types.keys():
                        # To be able to have multiple values for a single
                        # column we use list, and if only one string is passed
                        # we transform it into a list
                        if isinstance(cond_val,basestring):
                            cond_val = [cond_val]
                        
                        qry_cond = qry_cond + '('
                        for iVal in range(len(cond_val)):
                            current_cond_val = cond_val[iVal]
                            qry_cond = qry_cond + "(" + cond_col + " " + cond_oper + " "
                            # Find type of column to format the condition value
                            col_type = col_types[cond_col]
                            # Get separator value: ' for text, # for date,
                            # nothing for numbers
                            if col_type in ['date','number','text']:
                                sep = cols_type_separator[col_type]
                                qry_cond = qry_cond + sep + current_cond_val +\
                                           sep + ')'
                            else:
                                print 'Format of column ' + cond_col + ' in file ' +\
                                    filename + ' not found. Treated as text.'
                                qry_cond = qry_cond + "'" + cond_val + "')"
                            qry_cond = qry_cond + ' OR '
                        
                        qry_cond = qry_cond[:-4] + ") AND "
                    else:
                        print '\nError with file ' + filename + ', the column ' +\
                            cond_col + ' does not exist: no condition applied.\n'
        
        if len(version_cond)>0:
            qry_cond = "WHERE (" + qry_cond + version_cond + ")"
        elif len(qry_cond)>0:
            qry_cond = "WHERE " + qry_cond[:-5] + ")"
            
        ## Create query and fetch data, add dataframe to results 
        qry = "SELECT " + columns_needed +" FROM "+ tablename+" " + qry_cond +";"
        qry_data = pd.read_sql(qry,conn)
        conn.close()
        results_list = results_list + [qry_data]
    
    if single_input > 0:
        return results_list[0]
    else:
        return results_list
###############################################################################  
### Function to check if a string contains a valid date or not
def is_date(string):
    if string is not None:
        try: 
            dateutil.parser.parse(string)
            return True
        except ValueError:
            return False
    else:
        return False
###############################################################################  
### Function to check if string contains a number in Excel date format
def is_excel_date(string):
    if string is not None:
        try: 
            xlrd.xldate_as_tuple(np.float(string),0)
            return True
        except ValueError:
            return False
    else:
        return False  
###############################################################################
### Function to upload a dataframe into an Access table
def upload_df_to_db(conn,table,df,upload_version=None):
    """conn: connection to database we are writing on
        table: name of table (as a string) we are writing data on
        df: dataframe containing the data to be written in the table. The 
    columns of df must be a subset of the columns of table or the upload 
    will fail.
        upload_version: if this input is passed it is used as the Version
    name to upload the data with instead of the running version.
        Take all the data in the dataframe df and upload it into the Access
    database table called table. The database is defined by the connection
    conn. Input data is adjusted before upload to match the target format: 
    datetime are transformed into strings and number entries with no values
    (nan in Python) are modified to None.
    """
    
    cursor = conn.cursor()
    df.loc[:,'Version'] = running_version
    if upload_version is not None:
        df.loc[:,'Version'] = upload_version
    df.loc[:,'TimeStamp'] = datetime.datetime.now().strftime("%m/%d/%Y %I:%M:%S %p")
    
    ### Replace all null values with None
    for col in df.columns:
        df.loc[:,col] = df.loc[:,col].apply(lambda x: x if pd.notnull(x) else None)
    
    ### Get target table column names and data types
    table_cols = {}
    for row in cursor.columns(table = table):
        table_cols.update({row.column_name: row.type_name})
    
    ### Check dataframe columns are all in the target table
    if not(df.columns.isin(table_cols.keys()).all()):
        print 'DataFrame columns are not all in the target table'
        return
    
    ### Get target table date, text and number columns    
    date_columns = [col for col in df.columns if (table_cols[col] in date_formats)]
    number_columns = [col for col in df.columns if (table_cols[col] in number_formats)]
    text_columns= [col for col in df.columns if (table_cols[col] in text_formats)]
    
    
    ### Clean data to prepare for upload
    ## Dates: make values strings, and replace empty entries (NaT) with None
    if len(date_columns)>0:
        df.loc[:,date_columns] = df.loc[:,date_columns].applymap(lambda x: np.str(x))
        df.loc[:,date_columns] = df.loc[:,date_columns].applymap(lambda x:\
            xlrd.xldate.xldate_as_datetime(np.float(x),0).strftime("%m/%d/%Y")\
            if (is_excel_date(x)) else x)
        df.loc[:,date_columns] = df.loc[:,date_columns]. \
            replace(to_replace={'NaT':None, 'None': None, '': None})
        df.loc[:,date_columns] = df.loc[:,date_columns].applymap(lambda x:\
           x if is_date(x) else None)
    
    ## Numbers: replace empty entries (NaN) by None
    # First find empty entries, set a dummy value, then replace by None
    # Replace non numeric entries by None as well
    for col in number_columns:
        null_entries = df.loc[:,col].isnull()
        df.loc[:,col] = df.loc[:,col].apply(lambda x: x if\
            isinstance(x,numbers.Number) else None)
        df.loc[null_entries,col] = ''   
        df.loc[null_entries,col] = None
    
    ## Text: make sure text columns are already in text format
    if len(text_columns)>0:
        df.loc[:,text_columns] = df.loc[:,text_columns].applymap(
            lambda x: np.str(x).encode('utf-8') if (x is not None) else None)
        
    
    ### Prepare query to upload data: list of columns in dataframe
    target_cols = '('
    target_vals = '('
    for col in df.columns:
        target_cols = target_cols + '[' + col + '],'
        target_vals = target_vals + '?,'
    target_cols = target_cols[:-1] + ')'
    target_vals = target_vals[:-1] + ')'
    sql = "insert into " + table + target_cols + " values " + target_vals + ';'
    ## Upload rows, setting values by rows in tuples
    upload_rows = [tuple(x) for x in df.values]
    cursor.executemany(sql, upload_rows)
    
    ### No return for the function, we simply commit the results in the DB
    cursor.commit()
    return
#    ## To update row by row
#    row_counter = 0
#    for row in upload_rows:
#        row_counter += 1
#        cursor.execute(sql, row)
#    ## Insert values in db value by value to find potential issues
#    for i in range(len(df)):
#        for j in range(len(df.columns)):
#            current_col = np.str(df.columns[j])
#            current_val = df.iloc[i,j]
#            sql = "insert into " + table + "([" + current_col + "]) values (?);"
#            cursor.execute(sql,current_val)
#            
#    # Print type of each column in the dataframe and in the database
#    for col in df.columns:
#        print np.str(col) + ':\n' + np.str(df[col].dtype) + ' in code\n' +\
#            table_cols[col] + ' in DB\n'
###############################################################################  
def hfv_upload_prep(hfv):
    """
    This functions takes the hfv file in the form of a dataframe (either 
    monthly or daily version, this is determined from the columns included
    in the header) and transforms it to be ready for upload:
       - NA values are replaced
       - daily or monthly file is determined
       - add Security names for monthly file if needed
       - add AsOfDate column for daily file if needed
       - delete some columns
       - rename some columns
    """
    na_values,file_identify,column_actions = fetch_files(
        ['hfv_na_values','hfv_file_identify','hfv_column_actions'])
    hfv_cols = hfv.columns.tolist()
    
    ## Identify file type: HFV_monthly or HFV_daily
    file_types = np.unique(file_identify.FileType)
    type_found = '' # 'Monthly' or 'Daily'
    for file_type in file_types:
        criterias = file_identify.query("FileType == '" + file_type +"'")[
            ['Criteria','ColumnName']]
        contain_columns = criterias.query("Criteria == 'Contains'")['ColumnName']
        exclude_columns = criterias.query("Criteria == 'Excludes'")['ColumnName']
        contain_conditions = set(contain_columns).issubset(hfv_cols)
        if len(exclude_columns)>0:
            exclude_conditions = not(set(exclude_columns).issubset(hfv_cols))
        else:
            exclude_conditions = True
        if contain_conditions & exclude_conditions:
            type_found = file_type
    
    if type_found == 'Monthly':
        no_security = hfv.loc[:,'Security'].isnull()
        hfv.loc[no_security,'Security'] = hfv.loc[no_security,'CUSIP']
    elif type_found == 'Daily':
        no_date_col = column_actions.query("(FileType == 'Daily') & " +\
            "(Action == 'AsOfDate')")['ColumnName'].values[0]
        if no_date_col in hfv_cols:
            no_date_col_index = hfv_cols.index(no_date_col)
            daily_as_of_date = hfv_cols[no_date_col_index+1].strftime('%m/%d/%Y')
            del hfv[hfv_cols[no_date_col_index+1]]
            hfv.loc[:,'AsOfDate'] = daily_as_of_date
    else:
        type_found = 'Other'
        result = ['',type_found]
        return result
    
    ## Get full list of columns to delete
    cols_to_delete_raw = column_actions.query("(FileType == '" + type_found +\
        "') & (Action == 'Delete')")[['ColumnName','Criteria']]
    cols_to_delete = cols_to_delete_raw.query("Criteria == 'Exact'")\
        ['ColumnName'].tolist()
    cols_to_delete_starts = cols_to_delete_raw.query(\
        "Criteria == 'StartsWith'")['ColumnName'].tolist()    
    cols_to_delete_ends = cols_to_delete_raw.query(\
        "Criteria == 'EndsWith'")['ColumnName'].tolist()
    
    for i in range(len(cols_to_delete_starts)):
        starts_with_col = cols_to_delete_starts[i]
        cols_to_delete = cols_to_delete + [x for x in hfv_cols if
            np.str(x).startswith(starts_with_col)]
    
    for i in range(len(cols_to_delete_ends)):
        ends_with_col = cols_to_delete_ends[i]
        cols_to_delete = cols_to_delete + [x for x in hfv_cols if
            np.str(x).endswith(ends_with_col)]
    
    # Go through list of columns and delete the ones found in the file
    hfv_cols = hfv.columns.tolist()
    for col in cols_to_delete:
        if col in hfv_cols:
            del hfv[col]
    
    ## Rename columns
    hfv_cols = hfv.columns.tolist()
    cols_to_rename_raw = column_actions.query("(FileType == '" + type_found +\
        "') & (Action == 'Rename')")[['ColumnName','NewName']]
    cols_to_rename_dict = {}
    for i in range(len(cols_to_rename_raw)):
        oldCol = cols_to_rename_raw.iloc[i,0]
        if oldCol in hfv_cols:
            newCol = cols_to_rename_raw.iloc[i,1]
            cols_to_rename_dict.update({oldCol:newCol})
    hfv.rename(index=str,columns=cols_to_rename_dict,inplace=True)
    
    ## Replace NA values with none
    NA_values = na_values['List'].tolist() + [pd.NaT,np.NaN,'NaT','NaN','nan']
    for col in hfv.columns:
        hfv.loc[:,col] = hfv.loc[:,col].apply(lambda x:\
            x.strip() if ((x is not None) &\
            (isinstance(x,str) or isinstance(x,unicode))) else x)
        col_type = hfv.loc[:,col].dtype
        if not(col_type in ['<M8[ns]']):
            if not(col_type in [np.int64,np.float64]):
                hfv.loc[:,col] = hfv.loc[:,col].apply(lambda x:
                    '' if (x in NA_values) else x)
            null_entries = hfv.loc[:,col].isnull()
            hfv.loc[null_entries,col] = ''
            empty_entries = (hfv.loc[:,col].apply(lambda x: np.str(x) == ''))
            hfv.loc[empty_entries,col] = None
    
    valid_sec = (hfv.loc[:,'Security'] is not None) & \
        (hfv.loc[:,'Security'] != 'None')
    hfv = hfv.loc[valid_sec,:]
    
    result = [hfv,type_found]
    return result
############################################################################### 
### Function to load an Excel file's specific tab into a dataframe
def excel_tab_to_df(pd_wb,filename,tab_name,header_offset=0,footer_offset=0):
    """
    pd_wb: workbook loaded through pandas with pd_wb = pd.ExcelFile(filename)
    filename: name of the file we are loading (need to be in the current folder)
    tab: file's tab name we are trying to load
    header_offset and footer_offset: number of rows to skip at the top and at
    the bottom of the file we are reading (corresponds to arguments header and
    skip_footer from pandas' read_excel function)
    """
    try:
        df = pd_wb.parse(tab_name,header=header_offset,skip_footer=footer_offset)
    except: 
        workbook = xlrd.open_workbook(filename,on_demand=True)
        worksheet = workbook.sheet_by_name(tab_name)
        data = np.array([[cell for cell in row] for row in worksheet.get_rows()])
        df_columns = [x.value for x in data[header_offset]]
        if footer_offset > 0:
            df = pd.DataFrame(data[(header_offset+1):(-footer_offset)],\
                columns=df_columns)
        else:
            df = pd.DataFrame(data[(header_offset+1):],columns=df_columns)
        df = df.applymap(lambda x: x.value)
        workbook.unload_sheet(tab_name)
        data = None
        worksheet = None
        workbook = None
    
    return df
###############################################################################
def upload_file(filetype,filename='',file_location='',df=None,tab=None,
                upload_version=None):
    """
    This function takes an Excel file, as defined by its filename (eg:
    "HFV 2016 09 30.xlsx" and its location eg: "P:\\GTRM\\"), as well as the
    tabs containing the file(s) to be uploaded (either string or list of
    strings), and uploads the data into the tables of the files passed in the
    variable filetype (eg: "HFV_daily_final","HFV_daily_prelim",
    "HFV_monthly")
    filetype: name of the file to upload, as used in the code. It can either be
    a string or a list of string if there are several files to parse from the
    Excel file passed as input
    filename: entire name of the Excel file to parse
    file_location: location of the Excel file
    tab: NOT IMPLEMENTED YET
    upload_version: if this input is passed it is used as the Version
    name to upload the data with instead of the running version.
         either string containing the tab to parse, or a list of strings 
    containing the tabs to parse if there are several files (in the same order
    as the files in the variable filetype in that case), or nothing if the 
    filetype variable contains ['HFV_monthly','HFV_daily_final'], the tabs are
    determined based on the columns in the file.
    """
    # List of files using standard upload
    standard_upload = ['IP_stress_pnl','IP_mapped','IP_not_mapped',\
        'market_data_rates','bbg_parameters','IP_csrm',\
        'portfolio_sensitivities','security_sensitivities',\
        'qrm_outputs_IP_details','qrm_outputs_krs','security_agg_map',\
        'value_breakdown_ac_ccy','sd_bond_mapping','sd_ac_metric_agg',\
        'sd_activity_agg','sd_bond_mapping_country','sd_liquidity_agg',\
        'sd_changes_agg','sd_sensitivity_agg','sd_stress_agg',\
        'summit_extract','WSS_extract','non_intrader_bonds',\
        'VaR_port_bdown_details','VaR_output_market_vol',\
        'VaR_output_sensitivities_agg','VaR_results_factors_VaR',\
        'VaR_results_factors_vol','VaR_results_grouped_VaR',\
        'VaR_results_grouped_vol','VaR_results_largest_factors_VaR',\
        'VaR_results_largest_factors_vol','VaR_results_main',\
        'VaR_results_run_details','market_data_spreads_csim',\
        'qrm_parallel_results']
    
    # Differentiate upload/prep process between different types of files
    if isinstance(filetype,list):
        if ('HFV_monthly' in filetype) & ('HFV_daily_final' in filetype):
                os.chdir(file_location)
                pd_wb = pd.ExcelFile(filename)
                for tab_name in pd_wb.sheet_names:
                    df = excel_tab_to_df(pd_wb,filename,tab_name)
                    hfv_upload_ready,hfv_type = hfv_upload_prep(df)
                    df = None
                    
                    if hfv_type in ['Daily','Monthly']:
                        if hfv_type == 'Daily':
                            filetype = 'HFV_daily_final'
                        elif hfv_type == 'Monthly':
                            filetype = 'HFV_monthly'
                       
                        db = tables_info.loc[tables_info.Code_TableName ==\
                            filetype,'DB_Name'].values[0]
                        db_location = db_info.loc[db_info.DB_Name == db,\
                            'FileLocation'].values[0]
                        db_name = db_info.loc[db_info.DB_Name == db,\
                            'FileName'].values[0]
                        table_name = tables_info.loc[
                            tables_info.Code_TableName == filetype,\
                            'DB_TableName'].values[0]
                        
                        conn = pyodbc.connect(
                            'DRIVER={Microsoft Access Driver (*.mdb, *.accdb)};'+
                            'DBQ=' + db_location + db_name)
                        upload_df_to_db(conn,table_name,hfv_upload_ready)
                        
    else: # No list, one single string
        table_name = tables_info.loc[tables_info.Code_TableName == \
            filetype,'DB_TableName'].values[0]
        db = tables_info.loc[tables_info.Code_TableName == filetype,\
            'DB_Name'].values[0]
        db_location = db_info.loc[db_info.DB_Name == db,'FileLocation'].values[0]
        db_name = db_info.loc[db_info.DB_Name == db,'FileName'].values[0]
        
        if (filetype == 'POINT_report'):
            os.chdir(file_location)
            pd_wb = pd.ExcelFile(filename)
            tab_name = pd_wb.sheet_names[0]
            df = excel_tab_to_df(pd_wb,filename,tab_name,header_offset=6,\
                footer_offset=3)
            
            # Get as of date of the report
            date_info = excel_tab_to_df(pd_wb,filename,tab_name,\
                footer_offset=len(df)+4)
            date_info = [x for x in pd.unique(date_info.unstack().values) 
                if (np.str(x)[:7] == 'As Of :')]
            as_of_date = dateutil.parser.parse(date_info[0][-10:]).\
                strftime("%m/%d/%Y")
            df.loc[:,'AsOfDate'] = as_of_date
            
            ## Format data to match the database
            # Depending on method used to load date the data might have a 
            # named index which we need to move to a normal column
            if 'Total' in df.index:
                df.reset_index(inplace=True)
                df.rename(index=str,columns={'index':'Security'},inplace=True)
            else:
                df.rename(index=str,columns={'':'Security'},inplace=True)
            
            # Rename some columns and delete the Count column
            rename_cols = {'Market Value [%]':'Market Value Percent',
                           'KRDExposure 0.5yr':'KRDExposure 0_5yr'}
            df.rename(index=str,columns=rename_cols,inplace=True)
            del df['Count']
            
            ## Upload to database
            conn = pyodbc.connect(\
                'DRIVER={Microsoft Access Driver (*.mdb, *.accdb)};'+
                'DBQ=' + db_location + db_name)
            
            upload_df_to_db(conn,table_name,df)
            
        elif (filetype == 'POINT_hfv_merged'):
            # We only keep a subset of columns for the merged reports
            keep_columns = [
                'AsOfDate','Security','SEC 1','Par/Curr_Face USD_Equiv',
                'Fair_Value(USD_Equiv)','Identifier','Position Amount (Base)',
                'Market Value','Custom Portfolio 1','comment','HFV_Version']
            df = df[keep_columns]            
            
            ## Upload to database
            conn = pyodbc.connect('DRIVER={Microsoft Access Driver (*.mdb, *.accdb)};'+
                'DBQ=' + db_location + db_name)
            upload_df_to_db(conn,table_name,df)    
        
        elif (filetype == 'FRY14Q'):
            # Load data from Excel
            os.chdir(file_location)
            pd_wb = pd.ExcelFile(filename)
            tab_name = pd_wb.sheet_names[0]
            df = excel_tab_to_df(pd_wb,filename,tab_name)
            # We only keep a subset of the columns
            keep_columns = [
                'AsOfDate','UniqueID','TransactionID','IdentifierType',
                'Security','SecurityDescription1','AmortizedCost_USD',
                'MarketValue_USD','AFS_HTM','Currency','Country']
            keep_columns_df = [x for x in df.columns if x in keep_columns]
            df = df[keep_columns_df]
            df.replace(to_replace="'",value="",regex=True,inplace=True)
            df.replace(to_replace='"',value='',regex=True,inplace=True)
            ## Upload to database
            conn = pyodbc.connect('DRIVER={Microsoft Access Driver (*.mdb, *.accdb)};'+
                'DBQ=' + db_location + db_name)
            upload_df_to_db(conn,table_name,df)
        
        elif (filetype == 'HFV_daily_prelim'):
            os.chdir(file_location)
            pd_wb = pd.ExcelFile(filename)
            for tab_name in pd_wb.sheet_names:
                df = excel_tab_to_df(pd_wb,filename,tab_name)
                hfv_upload_ready,hfv_type = hfv_upload_prep(df)
                df = None
                
                if hfv_type == 'Daily':
                    db = tables_info.loc[tables_info.Code_TableName == \
                        filetype,'DB_Name'].values[0]
                    db_location = db_info.loc[db_info.DB_Name == db,\
                        'FileLocation'].values[0]
                    db_name = db_info.loc[db_info.DB_Name == db,\
                        'FileName'].values[0]
                    table_name = tables_info.loc[
                        tables_info.Code_TableName == filetype,\
                        'DB_TableName'].values[0]
                    
                    conn = pyodbc.connect(
                        'DRIVER={Microsoft Access Driver (*.mdb, *.accdb)};'+
                        'DBQ=' + db_location + db_name)
                    upload_df_to_db(conn,table_name,hfv_upload_ready)
                else:
                    print '\nError with file, it does not correspond to HFV daily'

        elif (filetype in standard_upload):
            ## Upload to database
            conn = pyodbc.connect(\
                'DRIVER={Microsoft Access Driver (*.mdb, *.accdb)};'+
                'DBQ=' + db_location + db_name)
            upload_df_to_db(conn,table_name,df,upload_version)
        else:
            print('Default filetype upload for: ' + filetype)
            ## Upload to database
            conn = pyodbc.connect(\
                'DRIVER={Microsoft Access Driver (*.mdb, *.accdb)};'+
                'DBQ=' + db_location + db_name)
            upload_df_to_db(conn,table_name,df,upload_version)
    return
###############################################################################
def delete_data(filetype,cond=''):
    """ 
    filetype: name of the file we want to delete data from.
              eg: filetype = 'HFV_daily', or a list of the files
    cond: similar to the input of the function fetch_files. Since we only have
    one type of file possible at once this is a list of 2-item list where the
    first element corresponds to the column and the second to the values of
    that column (which can be a list when several values are deleted).
    This function only deletes entries for specific values given. For instance
    we cannot delete all entries older than 1/1/2015 using <#1/1/2015#, all 
    values need to be entered, this limits mistakes where data is deleted all
    at once.
    Here are examples of valid ways to call this function:
    cond = [['AsOfDate','9/30/2016']]
    cond = [['Version','TestVersion'],['AsOfDate','9/30/2016']]
    cond = [['Security',['CusipA','CusipB']],['AsOfDate','9/30/2016']]
    This function deletes the rows from the table of filetype according to the
    conditions passed in cond. If cond is not passed then the function fails to
    avoid the deletion of all the data in a table.
    """
    # If no condition is passed we do not delete any data for safety
    if cond == '':
        print('No conditions passed to delete data from ' + np.str(filetype) +\
            ' no deletion happened')
        return

    # Get table details
    table_name = tables_info.loc[tables_info.Code_TableName == filetype,\
        'DB_TableName'].values[0]
    db = tables_info.loc[tables_info.Code_TableName == filetype,'DB_Name'].values[0]
    db_location = db_info.loc[db_info.DB_Name == db,'FileLocation'].values[0]
    db_name = db_info.loc[db_info.DB_Name == db,'FileName'].values[0]
    conn = pyodbc.connect('DRIVER={Microsoft Access Driver (*.mdb, *.accdb)};'+
                          'DBQ=' + db_location + db_name)
    cursor = conn.cursor()
    
    # Get table information: columns list and columns types
    col_types = table_col_types(filetype,cursor)

    # Create WHERE condition of the query
    qry_cond = ''
    for iCond in range(len(cond)):
        # Get column and values of condition
        cond_col = cond[iCond][0]
        cond_val = cond[iCond][1]
        
        if cond_col in col_types.keys():
            # To be able to have multiple values for a single column we use
            # list: if only one string is passed we transform it into a list
            if isinstance(cond_val,basestring):
                cond_val = [cond_val]
            
            # Queries are written: WHERE (col1 in (value1,value2) AND ...)
            qry_cond = qry_cond + '(' + cond_col + ' in ('
            col_type = col_types[cond_col]
            
            if col_type in ['date','number','text']:
                col_sep = cols_type_separator[col_type]
            else:
                print 'Format of column ' + cond_col + ' in file ' +\
                        filetype + ' not found. Treated as text.'
                col_sep = "'"
                
            for iVal in range(len(cond_val)):
                current_cond_val = cond_val[iVal]
                qry_cond = qry_cond + col_sep + current_cond_val + col_sep + ','
            
            qry_cond = qry_cond[:-1] + ')) AND '
        else:
            print '\nError with file ' + filetype + ', the column ' +\
                cond_col + ' does not exist: no deletion.\n'
            return
    # Delete last AND to get final WHERE piece of the delete query
    qry_cond = qry_cond[:-5]
         
    # Get full query with conditions
    qry =  'delete from ' + table_name + ' where ' + qry_cond
    
    nb_rows_deleted = cursor.execute(qry).rowcount
    cursor.commit()
    return
###############################################################################

###VaR

# -*- coding: utf-8 -*-
import os
import sys
import struct
import pandas as pd
import numpy as np
import scipy
from scipy import stats
import scipy.linalg
import datetime
from dateutil.relativedelta import relativedelta
from scipy.stats import norm    
from scipy.stats import t
from scipy.special import btdtr as betadist
from scipy.stats.distributions import t as tdist
import gc
import xlwings as xw

######################## Inputs and Files #####################################
data_folder_python64 = os.getcwd() + '\\'
hdf_filepath = data_folder_python64 + 'VaR_data.h5'
spyder_version = np.str(struct.calcsize('P') * 8)
###############################################################################
import functions_calls as fc
reload(fc)
import ms_access_data_mgmt as data_mt
reload(data_mt)
##### File types, split between date dependent and independent ################
df_list_no_date = ['scenario_sensitivities','market_data_spreads_csim',\
    'market_data_rates','VaR_portfolio_breakdown',\
    'VaR_factor_breakdown','indices_list','value_breakdown_ac_ccy',\
    'non_intrader_bonds','qrm_valuation_param_no_cusip',\
    'portfolios_definition','sd_bond_mapping']
df_list_date_specific = ['portfolio_sensitivities','IP_mapped',\
    'security_sensitivities','security_agg_map',\
    'value_breakdown_ac_ccy','VaR_port_bdown_details']
###############################################################################
def export_data_to_HDF5(date=None,target_folder=None):
    """ date: specific as of date of the portfolio for which to update large
    tables (in df_list_per_date: sensitivities, mapping, ...)
        target_folder: name of folder in which to export the data. String in
    Python format (double slashes, eg: C:\\PythonData\\). If nothing is passed 
    the target folder is set as the current working directory, which should
    be the location of this file.
        Fetches data from Access database (requires connection to Access 
    database working) and exports it into HDF5 files.
    """
    if target_folder is not None:
        current_folder = os.getcwd()
        os.chdir(target_folder)
    
    if date is not None:
        fdate = fc.format_date(date)
    
    # Fecth data from Access. Works with Python of the same version as Access
    try:
        ## Loop through files
        for df_name in df_list_no_date:
            df = data_mt.fetch_files(df_name)
            df.to_hdf('VaR_data.h5',df_name)
        
        # For large tables loop through the dates and write to separate tables
        for df_name in df_list_date_specific:
            df_dates = [fc.format_date(x) for x in \
                np.unique(data_mt.fetch_files(df_name,\
                columns={df_name:['AsOfDate']}))]
            if date is not None:
                if fdate in df_dates:
                    df_dates = [fdate]
                else:
                    df_dates = []
            
            for d in reversed(df_dates):
                f_d = fc.format_date(d)
                df = data_mt.fetch_files(df_name,\
                        cond={df_name:[['AsOfDate','=',f_d]]})
                df_table_name = df_name + '_' + f_d.replace('/','_')
                df.to_hdf('VaR_data.h5',df_table_name)
        
    except:
        print('Wrong version of Spyder, no exports. Current version: ')
        print(sys.version)
    
    if target_folder is not None:
        os.chdir(current_folder)
    
    return
###############################################################################
def fetch_data(filetype,date=None,keep_version_column=False):   
    """ filetype: table name as used in the code
        date: as of date if table is date specific
        keep_version: True or False. By default the columns Version and 
    TimeStamp are removed from the dataframe.
        This functions fetches a table from the HDF5 file. If the file is data
    dependent then a date must be passed as input.
    """
    if date is not None:
        fdate = fc.format_date(date)
    
    if filetype in df_list_no_date:
        try:
            df = data_mt.fetch_files(filetype)
        except:
            df = pd.read_hdf(hdf_filepath,filetype)
    elif filetype in df_list_date_specific:
        try:
            df = data_mt.fetch_files(filetype,\
                cond={filetype:[['AsOfDate','=',fdate]]})
        except:
            f_d = '_' + fdate.replace('/','_')
            df = pd.read_hdf(hdf_filepath,filetype + f_d)
     
    if not keep_version_column:
        df = fc.remove_columns(df,['Version','TimeStamp'])
    
    return df
###############################################################################
def last_days_of_month(date_list):
    """
    date_list: list of dates available in a datetime format
        This function takes a list of dates at a daily frequency (business 
    days, so about 21 days per month) and returns the last business day of the 
    month for each of the months for which a date is given.
    """
    # Create dataframe with each date's year, month and day
    df = pd.DataFrame(
        data=date_list.apply(lambda x: [x,x.day,x.month,x.year]).tolist(),
        columns=['date','day','month','year'])
    
    end_of_month = []
    for y in np.unique(df.year):
        for m in np.unique(df.loc[df.year == y,'month']):
            dates = df.loc[(df.year == y) & (df.month == m),:]
            last_day = max(dates.day)
            end_of_month += [dates.loc[dates.day == last_day,'date'].values[0]]
    
    return end_of_month
###############################################################################
def fetch_market_data(filetype):
    """ filetype: 'market_data_rates' or 'market_data_spreads_csim'. Type of
    market data to get
        Fetches the market data from the database, removes columns Version and
    TimeStamp, replaces null entries by nan and orders the data according to
    the as of dates.
    """
    data = fetch_data(filetype)
    
    data = data.applymap(lambda x: x if pd.notnull(x) else np.nan)
    data.sort_values(by='AsOfDate',inplace=True)
    return data
###############################################################################
def ewm_mean(df,lambda_):
    """ df: dataframe containing the time series of the instruments for which
    we are computing the exponentially weighted moving average. Dates should
    be in the index
        lambda_: smoothing factor
    """
    # Convert input to dataframe if necessary
    if isinstance(df,pd.Series):
        df = pd.DataFrame(df)
        
    # Get dates in datetime format
    if isinstance(df.index[0],basestring):
        if len(df.index[0]) > 7:
            df.loc[:,'AsOfDate'] = fc.date_str_to_datetime(df.index.tolist())
            df.set_index('AsOfDate',inplace=True)

    # Order df and remove dates with no data for any index
    df.sort_index(inplace=True)
    df.dropna(axis='index',how='all',inplace=True)
    dates = df.index
    
    
    # Create output dataframe and loop through each index with data
    all_ewm_mean = pd.DataFrame(data=None,columns=df.columns,index=df.index)
    non_empty_idx = df.dropna(axis='columns',how='all').columns.tolist()
    for idx in non_empty_idx:
        df_idx = df.loc[:,[idx]]
        first_date_index = df_idx.index.get_loc(df_idx.first_valid_index())
        
        # Set first value, then loop through dates using iterative formula
        ewma_dict = {dates[first_date_index]:\
            df_idx.loc[dates[first_date_index]]}
        for i in range(first_date_index + 1,len(dates)):
            day_chg = df_idx.loc[dates[i]]
            prev_ewma = ewma_dict[dates[i-1]]
            new_ewma = lambda_*prev_ewma+(1-lambda_)*day_chg
            ewma_dict.update({dates[i]:new_ewma})
        
        ewm_mean = pd.DataFrame.from_dict(ewma_dict,orient='index').sort_index()
        all_ewm_mean.loc[:,idx] = ewm_mean.iloc[:,0]
    
    return all_ewm_mean
###############################################################################
def ewm_vol(df,lambda_):
    """ df: dataframe containing time series for which we compute the 
    exponentially weighted moving volatility. Dates are in the index.
        lambda_: smoothing factor
        Compute exponentially weighted moving volatility of time series in 
    the dataframe df with smoothing factor lambda according to the following
    method:
    vol[0] = abs(ts[1] - ts[0])
    vol[i]^2 = lambda * vol[i-1]^2 + (1 - lambda) * (ts[i] - ts[i-1])^2
    """
    # Convert input to dataframe if necessary
    if isinstance(df,pd.Series):
        df = pd.DataFrame(df)
    
    # Get dates in datetime format
    if isinstance(df.index[0],basestring):
        # Check dates contain days, not just year and month
        if len(df.index[0]) > 7:
            df.loc[:,'AsOfDate'] = fc.date_str_to_datetime(df.index.tolist())
            df.set_index('AsOfDate',inplace=True)
    
    # Order df and remove dates with no data for any index
    df.sort_index(inplace=True)
    df.dropna(axis='index',how='all',inplace=True)
    dates = df.index
    
    # Create output dataframe and loop through each index with data
    all_ewm_var = pd.DataFrame(data=None,columns=df.columns,index=df.index)
    non_empty_idx = df.dropna(axis='columns',how='all').columns.tolist()
    for idx in non_empty_idx:
        df_idx = df.loc[:,[idx]]
        first_date_index = df_idx.index.get_loc(df_idx.first_valid_index())
        
        ret0 = df_idx.loc[dates[first_date_index]]
        # Compute initial var, then loop through dates using iterative formula
        ewm_dict = {dates[first_date_index]:ret0**2}
        for i in range(first_date_index + 1,len(dates)):
            ret = df_idx.loc[dates[i]]
            prev_var = ewm_dict[dates[i-1]]
            new_var = lambda_ * prev_var + (1-lambda_) * ret**2
            ewm_dict.update({dates[i]:new_var})
        
        ewm_var = pd.DataFrame.from_dict(ewm_dict,orient='index').sort_index()
        all_ewm_var.loc[:,idx] = ewm_var.iloc[:,0]
            
    vol_ewm = all_ewm_var.applymap(lambda x: np.sqrt(x))
    
    return vol_ewm
###############################################################################
def ewm_cov(df,lambda_):
    """ df: time series for which we compute the exponentially weighted
    moving covariance matrix
        lambda_: smoothing factor
        Compute exponentially weighted moving covariances of time series in 
    the dataframe df with smoothing factor lambda according to the following
    method for time series x and y:
    cov[0] = x[0] * y[0]
    cov[i] = lambda * cov[i-1] + (1 - lambda) * x[i] * y[i]
        The final result is a panel: each item on the index contains the 
    covariance matrix for a different date.
    """
    
#    # Get dates in datetime format
#    if isinstance(df.index[0],basestring):
#        df.loc[:,'AsOfDate'] = fc.date_str_to_datetime(df.index.tolist())
#        df.set_index('AsOfDate',inplace=True)
    df.sort_index(inplace=True)
    
    # Get list of dates, list and number of indices
    dates = df.index
    nb_dates = len(dates)
    list_ts = df.columns.tolist()
    nb_ts = len(list_ts)
    
    # Get index for which each time series starts
    first_value = {}
    for idx in list_ts:
        if pd.notnull(df.loc[:,idx]).any():
            first_value.update({idx:
                df.index.get_loc(df.loc[:,idx].first_valid_index())})
        else:
            first_value.update({idx:nb_dates})
    
    # Create dataframe full of zeros to use as base for each date
    zero_df = pd.DataFrame(data=np.zeros(shape=(nb_ts,nb_ts)),
                           columns=list_ts,index=list_ts)
    
    # Create panel that contains final output
    cov_dict = {d: zero_df for d in dates}
    cov_panel = pd.Panel(data = cov_dict)
    
    # Loop through rows and columns, then through dates to compute covariances
    for c in range(nb_ts):
        col_index = list_ts[c]
        for r in range(c+1):
            row_index = list_ts[r]
            # Get date for which data starts for both time series
            first_date_idx = max(first_value[col_index],first_value[row_index])
            
            # Set covariance for dates before both indices have data as NA
            if first_date_idx > 0:
                cov_panel.loc[dates[:first_date_idx],
                              row_index,col_index] = np.nan
            
            # Check data exists, otherwise there is no covariance to compute
            if first_date_idx < nb_dates:
                # Get time series of the 2 indices
                ts_c = df.loc[dates[first_date_idx:],col_index]
                ts_r = df.loc[dates[first_date_idx:],row_index]
                
                # Create covariance time series by iteration
                cov = np.zeros(shape = len(ts_c))
                cov[0] = ts_c[0] * ts_r[0]
                
                for d in range(1,nb_dates-first_date_idx):
                    cov[d] = lambda_ * cov[d-1] +\
                        (1 - lambda_) * ts_c[d] * ts_r[d]
                
#                cov_panel.loc[fc.format_date(dates[first_date_idx:].tolist()),
#                              row_index,col_index] = cov
                cov_panel.loc[dates[first_date_idx:],row_index,col_index] = cov
            
            # If we are not on the diagonal copy values
            if c != r:
                cov_panel.loc[:,col_index,row_index] = \
                    cov_panel.loc[:,row_index,col_index]
                
    return cov_panel
###############################################################################
def get_covariance_mat(monthly_data,vol_weighting='ewma',
                       vol_computation='internal',half_life_months=12,
                       nb_years=3):
    """ monthly_data: data used to compute the covariance matrices, at a 
    monthly frequency. Index contains dates, columns contain time series. It 
    should already by changes
        vol_weighting: 'ewma' for exponentially weighted moving average, or 
    'equal' for equal weights for all data points
        vol_computation: chooses which function is called to compute the ewma
    covariance matrix. 'internal' for function ewm_cov written above, 'pandas'
    for pandas ewm function
        half_life_months: half life in months to use if the chosen method is 
    'ewm' nb_years: if method 'equal' is chosen the number of years of data to 
    use (NOT IMPLEMENTED yet)
        Gets covariance matrix for all available data indices using the method
    and parameters provided. Returns a time series of covariance matrices.
    """

    if vol_weighting == 'ewma':
        if vol_computation == 'internal':
            lambda_ = np.exp(np.log(0.5)/half_life_months)
            cov_df = ewm_cov(monthly_data,lambda_)
            
        elif vol_computation == 'pandas':
            cov_mat = monthly_data.ewm(halflife=half_life_months,
                ignore_na=True,adjust=True).cov(bias=False)
            # If output is panel: transform it to a multiindex dataframe
            if isinstance(cov_mat,pd.Panel):
                cov_df = cov_mat.to_frame(filter_observations=False)
                cov_df = cov_df.unstack('major').stack('AsOfDate')
                cov_df.index = cov_df.index.swaplevel(0, 1)
            else:
                cov_df = cov_mat
            cov_df.sort_index(inplace=True,level=0)
            
        return cov_df
        
    elif vol_weighting == 'equal':
        print('Method not implemented yet')
        return
###############################################################################
def get_monthly_market_data():
    """ This function fetches the market data from the database, for both rates
    and spreads, keeps only the last day of the month, and merges them.
        It returns a dataframe with the columns containing the market indices,
    and the rows correspond to different dates. Dates are in the index of the
    dataframe and only contain the year and month, eg: '2016-12' for December
    2016.
    """
    ### Load map from sensitivities to indices and get list of rates indices
    sensitivities_def = fetch_data('scenario_sensitivities')

    rates_indices = sensitivities_def.loc[\
        sensitivities_def.RiskType == 'Rates','Index'].tolist()
        
    ### Load and format market data
    # Get IR and CS market data
    data_spreads = fetch_market_data('market_data_spreads_csim')
    data_rates = fetch_market_data('market_data_rates')
    
    # Transform rates data in basis points (CS and swap spreads already are)
    data_rates_indices = [x for x in data_rates.columns if 
        (x in rates_indices)]
    data_rates.loc[:,data_rates_indices] = data_rates.loc[
        :,data_rates_indices]*100
    
    # Sort data
    data_spreads.sort_values(by = 'AsOfDate', inplace = True)
    data_rates.sort_values(by = 'AsOfDate', inplace = True)
    
    # Add column containing year and month only
    data_spreads['AsOfDate'] = data_spreads['AsOfDate'].map(lambda x: 
        x.strftime('%Y-%m'))
    data_rates['AsOfDate'] = data_rates['AsOfDate'].map(lambda x: 
        x.strftime('%Y-%m'))
    
    # Keep only last day of month data point and merge results
    monthly_spreads = data_spreads.groupby('AsOfDate').nth(-1)
    monthly_rates = data_rates.groupby('AsOfDate').nth(-1)
    market_data = pd.merge(left=monthly_rates,right=monthly_spreads,
                           left_index=True,right_index=True,how='outer')
    
    # Keep data past 1998 to reduce size of dataframe and avoid issues later
    market_data = market_data.loc[market_data.index >= '1998-01',:]
    
    return market_data
###############################################################################
def get_portfolio_sensitivities(dates,portfolios,sens_source):
    """ dates: list of dates for which to get sensitivities
        portfolios: list of portfolio names, or 'all'. If all then all the 
    portfolios are selected
        sens_source: source of sensitivities. POINT or QRM
        Fetches sensitivities from database or HDF5 file for all the dates 
	selected
    """
    ### Format input dates
    if not(isinstance(dates,list)):
        dates = [dates]
    
    ### Try to get data from database, from HDF5 file otherwise (32 vs 64-bit)
    # Loading from a database
    try:
        # Get full list of portfolios if portfolios is equal to all
        if isinstance(portfolios,basestring) and (portfolios == 'all'):
#            portfolio_def = data_mt.fetch_files(\
#                'portfolios_definition')
#            portfolios = portfolio_def.Portfolio.tolist()
            port_to_breakdown = fetch_data('VaR_portfolio_breakdown')
            portfolios = np.unique(port_to_breakdown.Portfolio)
            
        all_sensitivities = None
        for i in range(len(dates)):
            fdate = fc.format_date(dates[i])
            
            port_bdown = data_mt.fetch_files('VaR_port_bdown_details',\
                cond={'VaR_port_bdown_details':[['AsOfDate','=',fdate],
                ['Source','=',sens_source]]})
            bdown_portfolios = np.unique(port_bdown.Portfolio)
            missing_bdown = [x for x in portfolios if x not in bdown_portfolios]
            
            if len(missing_bdown) > 0:
                fc.get_subportfolio_sensitivities(fdate,sens_source)
                port_bdown = data_mt.fetch_files('VaR_port_bdown_details',\
                    cond={'VaR_port_bdown_details':[['AsOfDate','=',fdate],
                    ['Source','=',sens_source]]})
            
            port_bdown = port_bdown.loc[port_bdown.Portfolio.isin(portfolios),:]
            
            sensitivities_d = data_mt.fetch_files('portfolio_sensitivities',\
                cond={'portfolio_sensitivities':[['AsOfDate','=',fdate],
                ['Source','=',sens_source]]})
            
            all_port = port_bdown.Component.tolist()
            sensitivities_d = sensitivities_d.loc[sensitivities_d.Portfolio.\
                isin(all_port),:]
            
            # If nothing exists print it is missing
            if len(sensitivities_d) < 1:
                print('\nSensitivities could not be computed for: ' + fdate)
            
            # Save results in one dataframe
            if all_sensitivities is None:
                all_sensitivities = sensitivities_d.copy()
            else:
                all_sensitivities = all_sensitivities.append(\
                    sensitivities_d)
    
    ### Otherwise we load data from HDF5 file
    except:
        try:
            # Get full list of portfolios if portfolios is equal to all
            if isinstance(portfolios,basestring) and (portfolios == 'all'):
                portfolio_def = fetch_data('portfolios_definition')
                portfolios = portfolio_def.Portfolio.tolist()
            
            all_sensitivities = None
            for d in dates:
                hdf_file = 'portfolio_sensitivities' + '_' + \
                    fc.format_date(d).replace('/','_')
                sensitivities_d = pd.read_hdf(data_folder_python64 +\
                'VaR_data.h5',hdf_file)
                
                port_bdown = fetch_data('VaR_port_bdown_details',date=d)
                port_bdown = port_bdown.loc[port_bdown.Source == sens_source,:]
                
                all_port = port_bdown.Component.tolist()
                sensitivities_d = sensitivities_d.loc[sensitivities_d.Portfolio.\
                    isin(all_port),:]
                
                # Save results in one dataframe
                if all_sensitivities is None:
                    all_sensitivities = sensitivities_d.copy()
                else:
                    all_sensitivities = all_sensitivities.append(\
                        sensitivities_d)
            
        except:
            print('portfolio_sensitivities could not be loaded\n')
            print(sys.exc_info()[0])
            raise
            
        ## Filter results based on inputs
        all_sensitivities = all_sensitivities.loc[\
            all_sensitivities.Source == sens_source,:]
    
    # Format as of date
    all_sensitivities.loc[:,'AsOfDate'] = all_sensitivities.apply(\
        lambda x: fc.format_date(x.AsOfDate),axis=1)
    
    return all_sensitivities.loc[:,['AsOfDate','Portfolio','RiskFactor',\
        'Sensitivity']]
###############################################################################   
def param_vol(as_of_cov,index_sens,subset=''):
    """ as_of_cov covariance matrix of indices for that specific date
        index_sens: portfolio sensitivities per market index (matches row names
    and column names of as_of_cov). Contains at least 2 columns: DataIndex
    (matching as_of_cov) and Sensitivity (actual dollar sensitivities), and
    potentially other columns used in subset
        subset: list of two elements used to only select some of the 
    sensitivities from index_sens. First element contains the column on which  
    to subset, and the second one contains the values we keep. 
	eg: ['RiskType',['CS']] or ['RiskType',['Rates','SwapSpread']]. If subset
	is '' then no subset is done, we use the full index_sens.
        This function computes the volatility of the Investment Portfolio for a 
    specific date using the covariance matrix of indices passed as input.
    It outputs a dictionary containing the computed portfolio volatility for
    three different components: all the portfolio under 'All', rates and swap
    spreads components under 'IR', and credit spreads only under 'CS'.
    """
    # Check whether subsetting needs to be done
    if (subset != '') and (len(subset) == 2):
        # Convert second element of subset to list if needed
        if isinstance(subset[1],basestring):
            subset[1] = [subset[1]]
        # Select indices and sensitivities meeting the criteria chosen
        idx = index_sens.loc[\
            index_sens.loc[:,subset[0]].isin(subset[1]),'DataIndex']
        sensitivities = index_sens.loc[\
            index_sens.loc[:,subset[0]].isin(subset[1]),'Sensitivity']
    else:
        if (subset != ''):
            print('Error in function param_vol, invalid subset input: ' +\
                'No subsetting occurred')
        idx = index_sens.loc[:,'DataIndex']
        sensitivities = index_sens.loc[:,'Sensitivity']  
    
    # Compute variance with matrix product, then compute standard deviation
    var = np.dot(np.dot(sensitivities,as_of_cov.loc[idx,idx]),sensitivities)
    vol = np.sqrt(var)
    
    return vol
############################################################################### 
def vol_factor_subportfolio_details(as_of_cov,index_sens,pbd,fbd,p_vol,c,
                                    factors):
    """ as_of_cov: covariance matrix as of a specific date
        index_sens: all portfolio sensitivities for a given date
        pbd: portfolio breakdown in its subportfolios (called Component)
        fbd: factor breakdown, or how different factors are aggregated in
    different groups
        p_vol: total parametric volatility of the portfolio of interest
        c: component for which we are computing metrics (Rates, Credit Spreads
    or All for the entire set of factors)
        factors: list of factors the portfolio has exposure to given the 
    component c
        This function computes the contribution to volatility of each 
    subportfolio in each group of factors listed in pbd and fbd.
    """
    # Results saved in a list
    results_vol = []
    
    # Column on which to filter the risk factors
    f_col = 'DataIndex'
    main_port = pbd['Portfolio'].values[0]
    main_port_sens = index_sens.loc[index_sens.Portfolio == main_port,:]
    main_port_sens = main_port_sens.loc[\
        main_port_sens.DataIndex.isin(factors),:]
    main_sens = main_port_sens['Sensitivity']
    main_idx = main_port_sens['DataIndex']
    
    # Loop through subportfolios in pbd
    for subp in pbd.Component.tolist():
        # Get subportfolio indices corresponding to component (IR,CS,All)
        sensit = index_sens.loc[index_sens.Portfolio == subp,:]
        sensit = sensit.loc[sensit.DataIndex.isin(factors),:]
        # Loop through groups of factors
        for group in np.unique(fbd.Group).tolist():
            factors_bd = fbd.loc[fbd.Group == group,'Index'].tolist()
            subp_factor_sens = sensit.loc[sensit[f_col].isin(factors_bd),:]
            
            # Check the subportfolio has some exposure to the current group
            # in order to proceed
            if len(subp_factor_sens) < 1:
                continue
            
            # Get sub-portfolio factor sensitivities and names
            subp_sens = subp_factor_sens.Sensitivity
            subp_idx = subp_factor_sens.DataIndex
            
            # Compute contribution to volatility (cvol), isolated contribution 
            # to volatility (icvol) and correlated contribution to volatility
            # (corrcvol) of this group of indices of this subportfolio
            cvol = np.dot(np.dot(subp_sens,\
                as_of_cov.loc[subp_idx,main_idx]),main_sens) / p_vol
            icvol = np.dot(np.dot(subp_sens,\
                as_of_cov.loc[subp_idx,subp_idx]),subp_sens) / p_vol
            corrcvol = cvol - icvol
            results_vol += [[main_port,c,'CVol',subp,group,cvol]]
            results_vol += [[main_port,c,'ICVol',subp,group,icvol]]
            results_vol += [[main_port,c,'CorrCVol',subp,group,corrcvol]]
            
            # Compute isolated volatility and liquidation effect on volatility
            ivol = np.sqrt(np.dot(np.dot(subp_sens,\
                as_of_cov.loc[subp_idx,subp_idx]),subp_sens))
            vol_liquidation = np.sqrt(p_vol*(p_vol - cvol - corrcvol)) - p_vol
            results_vol += [[main_port,c,'IVol',subp,group,ivol]]
            results_vol += [[main_port,c,'LiquidationEffectVol',subp,group,\
                             vol_liquidation]]
    
    return results_vol
############################################################################### 
def vol_factor_details(as_of_cov,p_sens,p_vol,c,factors):
    """ as_of_cov: covariance matrix as of a specific date
        p_sens: portfolio sensitivities (factors of all components)
        p_vol: total parametric volatility of the portfolio of interest
        c: component for which we are computing metrics (Rates, Credit Spreads
    or All for the entire set of factors)
        factors: list of factors the portfolio has exposure to given the 
    component c
        This function computes the contribution to volatility of each factor
    a given portfolio has exposure to under a chosen component c.
    """
    # Results saved in a list
    results_vol = []
    
    # Column on which to filter the risk factors
    portfolio = p_sens['Portfolio'].values[0]
    sens = p_sens.loc[p_sens.DataIndex.isin(factors),:]
    p_sensitivities = sens['Sensitivity']
    p_idx = sens['DataIndex']
    
    # Loop through factors
    for f in factors:
        # Get portfolio sensitivity to this specific factor
        f_sensitivities = sens.loc[sens.DataIndex == f,:]
        f_sens = f_sensitivities.Sensitivity
        f_idx = f_sensitivities.DataIndex
            
        # Compute contribution to volatility (cvol), isolated contribution 
        # to volatility (icvol) and correlated contribution to volatility
        # (corrcvol) of this group of indices of this subportfolio
        cvol = np.dot(np.dot(f_sens,as_of_cov.loc[f_idx,p_idx]),\
                      p_sensitivities) / p_vol
        icvol = np.dot(np.dot(f_sens,as_of_cov.loc[f_idx,f_idx]),\
                       f_sens) / p_vol
        corrcvol = cvol - icvol
        results_vol += [[portfolio,c,'CVol',f,cvol]]
        results_vol += [[portfolio,c,'ICVol',f,icvol]]
        results_vol += [[portfolio,c,'CorrCVol',f,corrcvol]]
        
        # Compute isolated volatility and liquidation effect on volatility
        ivol = np.sqrt(np.dot(np.dot(f_sens,as_of_cov.loc[f_idx,f_idx]),f_sens))
        vol_liquidation = np.sqrt(p_vol*(p_vol - cvol - corrcvol)) - p_vol
        results_vol += [[portfolio,c,'IVol',f,ivol]]
        results_vol += [[portfolio,c,'LiquidationEffectVol',f,vol_liquidation]]
    
    return results_vol
############################################################################### 
def compute_vol_details(index_sens,as_of_cov,port_breakdown,factor_breakdown):
    """ index_sens: all portfolio sensitivities for a given date
        as_of_cov: covariance matrix as of a specific date
        port_breakdown: breakdown of some portfolios into subportfolios.
    This is used to determine which portfolios to compute detailed breakdowns 
    for and what that breakdown should be.
        factor_breakdown: aggregation of the risk indices used to determine
    what groups of indices to compute correlated/isolated volatilities
        Compute the breakdown of volatility of the portfolios for which we have
    breakdown parameters in portfolio_breakdown, as well as the single factor
    volatility contribution for each portfolio.
    """
    # Save results in list
    grouped_vol_details = []
    factor_vol_details = []
    
    # Different components we compute total portfolio volatitilies for
    components = {'All':['RiskType',['Total']],'CS':['RiskType',['CS']],
                  'Rates':['RiskType',['Rates','SwapSpread']]}
    
    ### Get list of portfolios for which we compute detailed breakdowns
    all_port = np.unique(index_sens.Portfolio)
    port_for_breakdown = np.unique(port_breakdown.Portfolio)
    main_port = [x for x in port_for_breakdown if x in all_port]
    
    ### Loop through all portfolios
    for p in all_port:
        p_sens = index_sens.loc[index_sens.Portfolio == p,:]
        
        # Loop through components (Rates, Credit Spreads, All)
        for c in components.keys():
            if c == 'All':
                factors = np.unique(factor_breakdown.loc[:,'Index'].\
                    tolist())
                subset_c = ''
            else:
                criteria = components[c]
                factors = np.unique(factor_breakdown.loc[\
                    factor_breakdown[criteria[0]].isin(criteria[1]),'Index'].\
                    tolist())
                subset_c = criteria
            
            p_vol = param_vol(as_of_cov,p_sens,subset=subset_c)
            factor_vol_details += vol_factor_details(\
                as_of_cov,p_sens,p_vol,c,factors)
            
            # Portfolios for which we compute subportfolio breakdowns only
            if p in main_port:
                p_breakdowns = port_breakdown.loc[\
                    port_breakdown.Portfolio == p,:]
                for p_b in np.unique(p_breakdowns.Breakdown):
                    pbd = p_breakdowns.loc[p_breakdowns.Breakdown == p_b,:]
                    
                    # All factor breakdowns can be calculated at once
                    grouped_vol_details += vol_factor_subportfolio_details(\
                        as_of_cov,index_sens,pbd,factor_breakdown,p_vol,c,\
                        factors)
     
    return grouped_vol_details, factor_vol_details
###############################################################################
def remove_na_from_cov_matrix(mat):
    """ mat: covariance matrix. Columns contain all indices we have data for
    at some point in time, while rows only those for which we have data
    on that specific date.
        This function loops through the different rows of the matrix, getting 
    the number of NA occurrences per row. It then deletes rows (and 
    corresponding columns) where there are NAs, one by one, deleting the rows
    with the most NAs first.
    """
    # Only keep indices both in rows and columns
    date_idx = mat.index.tolist()
    mat = mat.loc[date_idx,date_idx]
    
    # Find NaN indices with their number of NaN occurrences
    nan_idx = {}
    for r in mat.index:
        nb_nan = sum(pd.isnull(mat.loc[r,:]))
        if nb_nan > 0:
            nan_idx.update({r:nb_nan})
    
    # Delete row and columns of indices with the most NaN until none is left
    while pd.isnull(mat).any().any():
        max_nan = max(nan_idx.values())
        max_nan_idx = next(k for k,v in nan_idx.items() if v == max_nan)
        mat = mat.loc[mat.index != max_nan_idx,mat.columns != max_nan_idx]
        del nan_idx[max_nan_idx]
        
    return mat
###############################################################################
def remap_sens_to_avail_idx(sensitivities,cov_mat,sensitivities_def):
    """ sensitivities: sensitivities of a specific date. Contains 3 columns:
    RiskFactor with the name of the indices, Sensitivity in dollars, and list
    of portfolios in Portfolio.
        cov_mat: covariance matrix for the corresponding date, containing no
    NAs sensitivities_def: details on sensitivities, including the mapping from
    risk factor to market index and proxy indices
        This function returns a sensitivity vector, similar to the input, where
    each factor is remapped to an index for which data is available in the 
    matrix cov_mat. Sensitivities are aggregated at the market index level for
    each portfolio.
    """
    ### Get list of indices with available data
    available_data = cov_mat.iloc[0,:].apply(lambda x:\
        'Y' if pd.notnull(x) else 'N')
    available_data = available_data.reset_index()
    available_colname = available_data.columns[-1]
    index_colname = available_data.columns[0]
    available_data.rename(index=str,inplace=True,
        columns={available_colname:'AvailIndex',index_colname:'Index'})
    
    
    ### Use proxy mapping and list of available indices to select which index
    # each RiskFactor needs to be mapped to: merge list of available indices 
    # to proxy map on Index first, then Proxy and finally Proxy2, renaming
    # available_data dataframe as needed
    # Index
    sens_index_map = pd.merge(sensitivities_def.copy(deep=True),
                              available_data,on='Index',how='left')
    # Proxy
    available_data.rename(index=str,inplace=True,
        columns={'AvailIndex':'AvailProxy','Index':'IndexProxy'})
    sens_index_map = pd.merge(sens_index_map,available_data,on='IndexProxy',
                              how='left')
    # Proxy2
    available_data.rename(index=str,inplace=True,
        columns={'AvailProxy':'AvailProxy2','IndexProxy':'IndexProxy2'})
    sens_index_map = pd.merge(sens_index_map,available_data,on='IndexProxy2',
                              how='left')
    
    # Select available index for each RiskFactor for that date
    sens_index_map.loc[:,'DataIndex'] = sens_index_map.apply(lambda x:
        x.Index if x.AvailIndex == 'Y' else (
        x.IndexProxy if x.AvailProxy == 'Y' else (
        x.IndexProxy2 if x.AvailProxy2 == 'Y' else np.nan)),
        axis=1)
    
    # Get DataIndex to use for each RiskFactor
    if 'RiskType' in sensitivities.columns:
        sensitivities = pd.merge(sensitivities,
            sens_index_map[['RiskFactor','DataIndex']],
            on='RiskFactor',how='left')
    else:
        sensitivities = pd.merge(sensitivities,
            sens_index_map[['RiskFactor','DataIndex','RiskType']],
            on='RiskFactor',how='left')
    
    ### Aggregate sensitivities per available index for each portfolio
    index_sens = sensitivities.groupby(\
        ['Portfolio','DataIndex','RiskType']).sum()
    index_sens.reset_index(inplace=True)      
    
    return index_sens
###############################################################################
def cov_to_corr_mat(cov_mat):
    """ cov_mat: covariance matrix
        Correlation matrix is computed from covariance matrix and returned 
    """
    # Get correlation, compute matrix of standard deviations products
    stdev = np.sqrt(np.diag(cov_mat))
    stdev_product = np.outer(stdev,stdev)
    corr_mat = cov_mat / stdev_product
    return corr_mat
###############################################################################
def corr_to_cov_mat(corr_mat,stdev):
    """ corr_mat: correlation matrix
        stdev: standard deviations of variables in the matrix
        Covariance matrix is computed from correlation matrix and standard
    deviations, and then returned.
    """
    # Get correlation, compute matrix of standard deviations products
    stdev_product = np.outer(stdev,stdev)
    cov_mat = np.multiply(corr_mat,stdev_product)
    return cov_mat
###############################################################################
def get_vols_from_cov_mat_panel(cov_mat_panel):
    """ cov_mat: multiindex dataframe of covariance matrices. Level 0 index  
    correspond to the different dates, level 1 index and columns to the 
    variables
        This function gets the volatilities of each variable for each date by 
    taking the square root of the diagonal entries. It ouputs a dataframe
    containing the variables in the columns and each date on a row.
    """
    # Get list of dates and variables to creates empty dataframe
    dates = np.unique(cov_mat_panel.index.get_level_values(0))
    idx = cov_mat_panel.columns
    vols = pd.DataFrame(index=dates,columns=idx)
    
    # Loop through dates to get diagonal of covariance matrix
    for d in dates:
        d_cov_mat = cov_mat_panel.loc[d]
        d_idx = d_cov_mat.index.tolist()
        vols.loc[d,d_idx] = np.sqrt(np.diag(d_cov_mat.loc[d_idx,d_idx]))
    
    return vols  
###############################################################################
def make_eigenvalues_positive(corr_mat):
    """ corr_mat: correlation matrix containing no NA
        This function takes a correlation matrix and makes it positive definite
    by getting all eigen values and making them slightly positive.
    It then outputs the matrix that has been tweaked (only slightly if
    eigenvalues were not very negative) and is positive definite.
    """
    # Start by assuming matrix is negative, then modify eigen values until
    # it is positive
    negative = True
    nb_iterations = 0
    
    while negative:
        nb_iterations = nb_iterations + 1
        # Do eigen values/vectors decomposition of matrix
        eigen = np.linalg.eigh(corr_mat)
        eigen_values = eigen[0]
        eigen_vector = eigen[1]
        
        # Replace negative eigen values with small positive number
        eigen_values_new = np.where(eigen_values<=0, 1e-9, eigen_values)
        
        # Reconstruct original matrix using new eigen values
        corr_mat_new = np.dot(np.dot(eigen_vector,\
            np.diag(eigen_values_new)), np.transpose(eigen_vector))
        
        # Normalize new correlation matrix (potentially has elements not equal
        # to 1 on the diagonal afetr tweaking)
        corr_mat_new = corr_mat_new / \
            np.sqrt(np.dot(np.array([np.diag(corr_mat_new)]).T,
                           np.array([np.diag(corr_mat_new)])))
        corr_mat = corr_mat_new

        try: 
            # Check if matrix is positive by attemptimp Cholesky decomposition
            scipy.linalg.cholesky(corr_mat_new, lower=True)
            negative = False
        except scipy.linalg.LinAlgError:# as err:
            #print err
            if nb_iterations > 100:
                print('The correlation matrix cannot be modified to be' + \
                       ' positive definite after 100 iterations')
                sys.exit()            
            else:
                pass
        
    return corr_mat_new
###############################################################################
def make_corr_mat_positive(corr_mat):
    """ corr_mat: correlation matrix
        This function takes a correlation matrix and checks whether it is 
    positive definite (by attempting Cholesky decomposition). If not it is made
    positive by calling the function make_eigenvalues_positive.
    """
    # Check no NAs exist in the matrix
    if corr_mat.isnull().values.any() == False:
        # Check whether matrix is positive by attemping Cholesky decomposition
        try: 
            scipy.linalg.cholesky(corr_mat, lower=True)
        except scipy.linalg.LinAlgError:# as err:
            # If not positive, tweak eigen values and make it positive
            #print err
            positive_corr_mat = make_eigenvalues_positive(corr_mat)        
        except ValueError:
            pass
    else:
        print "There are NA in the correlation matrix"
  
    positive_corr_mat = pd.DataFrame(positive_corr_mat,index=corr_mat.index,
                                     columns=corr_mat.columns)

    return positive_corr_mat
###############################################################################
def multivariate_t_rvs(m, S, df=np.inf, n=1):
    ''' Generate random variables of multivariate t distribution
    From: https://github.com/statsmodels/statsmodels/blob/master/
              statsmodels/sandbox/distributions/multivariate.py
    Parameters
    ----------
    m : array_like
        mean of random variable, length determines dimension of random variable
    S : array_like
        square array of covariance  matrix
    df : int or float
        degrees of freedom
    n : int
        number of observations, return random array will be (n, len(m))
    Returns
    -------
    rvs : ndarray, (n, len(m))
        each row is an independent draw of a multivariate t distributed
        random variable
    '''
    m = np.asarray(m)
    d = len(m)
    if df == np.inf:
        x = 1.
    else:
        x = np.random.chisquare(df, n)/df
    z = np.random.multivariate_normal(np.zeros(d),S,(n,))
    
    # Output format is the same as random.multivariate_normal
    return m + z/np.sqrt(x)[:,None]
############################################################################### 
def var_factor_subportfolio_details(index_sens,pbd,factor_breakdown,
                                    p_c_l_factor_var):
    """ index_sens: all portfolio sensitivities for a given date
        pbd: portfolio breakdown in its subportfolios (called Component)
        fbd: factor breakdown, or how different factors are aggregated in
    different groups
        p_c_l_factor_var: contribution to VaR and Expected Shortfall of our  
    main portfolio broken down per risk factor. p_c_l: portfolio, component 
    (IR, CS, all) and level (VaR/ES percentile specific) specific
        This function computes the contribution to Value-at-Risk and ES of each 
    subportfolio in each group of factors listed in pbd and fbd.
    """
    # Results saved in a list
    results_var = []
    
    # Copy contributions dataframe
    contributions = p_c_l_factor_var.copy()
    
    # Restrict sensitivities to those in the portfolio of interest
    indices = np.unique(contributions.Factor)
    sens_needed = index_sens.loc[index_sens.DataIndex.isin(indices),:]
    
    # Column name containing factors
    f_col = 'DataIndex'
    
    # Get portfolio sensitivities and factor contributions
    main_port = pbd['Portfolio'].values[0]
    main_port_sens = sens_needed.loc[sens_needed.Portfolio == main_port,:]
    contributions.set_index('Factor',inplace=True)
    main_CVaR = contributions.loc[contributions['Factor Measure'] == \
                                     'Component','Value']
    main_ES = contributions.loc[contributions['Factor Measure'] == \
                                     'ES','Value']
    
    # Save contribution of each index in each subportfolio in a list
    CVaR_ES_details = []
    
    # Loop through subportfolios in pbd
    for subp in pbd.Component.tolist():
        # Get subportfolio sensitivities
        sens_subp = sens_needed.loc[sens_needed.Portfolio == subp,:]
        
        # Compute contribution of each factor in the subportfolio to VaR and 
        # ES: computed as contribution of a factor to total portfolio times 
        # ratio of this factor's sensitivity in this subportfolio to total
        # portfolio's sensitivity to that factor
        for idx in sens_subp[f_col]:
            main_port_idx_sens = main_port_sens.loc[main_port_sens[f_col] == \
                idx,'Sensitivity'].values[0]
            # Only compute ratio if main portfolio has non-zero exposure
            if np.abs(main_port_idx_sens) > 0:
                ratio = sens_subp.loc[sens_subp[f_col] == idx,'Sensitivity'].\
                    values[0] / main_port_idx_sens
            else:
                ratio = 0.
            idx_CVaR = main_CVaR[idx] * ratio
            idx_ES = main_ES[idx] * ratio
            CVaR_ES_details += [[subp,idx,idx_CVaR,idx_ES]]

    
    # Save results in dataframe
    details_df = pd.DataFrame(CVaR_ES_details,columns=['SubPortfolio',\
        'Index','CVaR','ES'])
    
    
    # Loop through groups of factors and aggregate results
    for group in np.unique(factor_breakdown.Group).tolist():
        factors = factor_breakdown.loc[factor_breakdown.Group == group,\
            'Index'].tolist()
        group_details = details_df.loc[details_df.Index.isin(factors),:]
        group_agg = group_details.groupby('SubPortfolio').sum()
        group_agg.reset_index(inplace=True)
        # Save results in list
        for i,row in group_agg.iterrows():
            results_var += [['VaR',row['SubPortfolio'],group,row['CVaR']]]
            results_var += [['ES',row['SubPortfolio'],group,row['ES']]]
    
    return results_var
###############################################################################
def compute_grouped_VaR(index_sens,factor_VaR_d,port_breakdown,
                        factor_breakdown):
    """ index_sens: all portfolio sensitivities for a given date
        factor_VaR_d: details of VaR (and ES) per risk factor for each 
    portfolio for a given date. Includes component, marginal and contribution.
        port_breakdown: breakdown of some portfolios into subportfolios.
    This is used to determine which portfolios to compute detailed breakdowns 
    of and what that breakdown should be.
        factor_breakdown: aggregation of the risk indices in groups used to
    determine risk factor breakdown.
        This function computes the attribution of VaR and ES to the portfolios
    per the parameters listed in port_breakdown (including which portfolio to
    break down) and per the different factor groups listed in factor_breakdown.
    """
    # Save results in list: contains main portfolio, component (IR, CS, all),
    # VaR level, measure type, subportfolio, group of indices and the value of
    # the measure
    var_breakdown = []
    
    ### Get list of portfolios for which we compute detailed breakdowns
    all_port = np.unique(index_sens.Portfolio)
    port_for_breakdown = np.unique(port_breakdown.Portfolio)
    main_port = [x for x in port_for_breakdown if x in all_port]
    
    ### Loop through portfolios, component, VaR levels and different portfolio
    # breakdowns
    for p in main_port:
        # Get portfolio specific factor VaR results
        p_factor_var = factor_VaR_d.loc[factor_VaR_d.Portfolio == p,:]
        p_breakdowns = port_breakdown.loc[port_breakdown.Portfolio == p,:]
        
        # Different components (IR, CS, all)
        for c in np.unique(p_factor_var.Component):
            p_c_factor_var = p_factor_var.loc[p_factor_var.Component == c,:]
            
            # Different VaR level
            for l in np.unique(p_c_factor_var['VaR Level']):
                p_c_l_factor_var = p_c_factor_var.loc[\
                    p_c_factor_var['VaR Level'] == l,:]
                    
                # Different portfolio breakdowns
                for p_b in np.unique(p_breakdowns.Breakdown):
                    pbd = p_breakdowns.loc[p_breakdowns.Breakdown == p_b,:]
                
                    # Compute breakdown for all factor aggregates at once
                    var_details = var_factor_subportfolio_details(\
                        index_sens,pbd,factor_breakdown,p_c_l_factor_var)
                    for r in var_details:
                        var_breakdown += [[p,c,l] + r]
    
    return var_breakdown
###############################################################################
def fit_t_distribution(all_ts,all_ts_vols,df_floor=3.,method='normalized'):
    """ all_ts: time series of the variables to fit (corresponds to changes)
        all_ts_vols: time series of volatilities of the variables (could either
    be computed using exponential or equal weighting). Same dates as all_ts.
        df_floor: int or ''. Floor applied to degrees of freedom parameters 
    computed. If '' then no floor is applied.
        method: normalized or regular. normalized means returns are divided by
    the volatility computed just before the returns occurred (return from t-1 
    to t is normalized with volatility computed as of t-1)
        This functions fits a t distribution to each of the variables in 
    all_ts. It returns a dictionary using the variables as keys, and the values 
    are lists of 2 elements: first the floored degrees of freedom, and then the 
    scale parameter computed using floor df and volatility as of time t passed 
    in the input all_ts_vols.
    """
    ### Loop through indices: fit distribution and save parameters to 
    # dictionary with: {index_name:[dof,scale]}
    fitted_dist = {}
    for idx in all_ts.columns.tolist():
        idx_vols = all_ts_vols[idx]
        idx_vols.replace(0,np.nan,inplace=True)
        
        # Get time series of changes depending on method
        if method == 'normalized':
            lagged_vols = idx_vols[:-1]
            lagged_vols.index = idx_vols.index[1:]
            ts_idx = all_ts[idx].iloc[1:] / lagged_vols
        else:
            ts_idx = all_ts[idx]
        
        # Get rid of NAs
        ts_idx.dropna(inplace=True)
        
        # Fit t-distribution with mean 0 and get degrees of freedom parameter
        par_est = stats.t.fit(ts_idx,floc=0)
        dof = par_est[0]
        
        # Floor degrees of freedom to 3 if needed
        if df_floor != '':
            if dof < df_floor:          
                #print "dof of %s < %i: %f" %(idx,df_floor, dof)
                dof = df_floor
        
        # Save index's degrees of freedom and scale parameter
        scale_est = idx_vols[-1] * np.sqrt((dof-2.)/dof)
        fitted_dist.update({idx:[dof,scale_est]})
    
    return fitted_dist
###############################################################################
def MC_simulations(nb_simulations,positive_corr_mat,fitted_dist,
                   copula='normal',copula_df=10):
    """ nb_simulations: integer. Number of simulations we run for Monte Carlo
        positive_corr_mat: correlation matrix that has been made positive 
	definite. It only contains the indices needed for the simulation.
        fitted_dist: dictionary containing the parameters to generate the 
    realizations of the variables. Keys are indices, and each value is a list
    containing the degrees of freedom and scale parameter of variable.
        copula: type of copula to use. normal or t
        copula_df: degrees of freedom of the t copula (if t is chosen)
        This function generates nb_simulations realizations of the correlated
    vector of indices contained in the correlation matrix, using a copula
    for the joint distribution, and then each factor is fit to a 
    t-distribution (with degrees of freedom floored to 0).
    The output is a dataframe containing nb_simulations, and a column for each
    index in the correlation matrix. Each row corresponds to a simulated
    realization vector of the correlated indices.
    """
    ### Get list of indices from dictionary
    nb_idx = len(positive_corr_mat.columns)
    
    ### Break down simulation number in blocks to avoid memory issues
    # in 32-bit version
    if spyder_version == '32':
        if nb_simulations >= 10000:
            block_size = 10000
        else:
            block_size = nb_simulations
    elif spyder_version == '64':
        block_size = nb_simulations
    
    nb_blocks = max(np.int(nb_simulations / block_size),1)
    # Adjust nb of blocks if nb of simulation not multiple of block size
    if (nb_simulations - nb_blocks * block_size) > 0:
        nb_blocks += 1
        block_size = nb_simulations / nb_blocks
    
    # Create df to contain probability of each variable in each simulation
    uni_rand = pd.DataFrame(index=range(block_size),\
                        columns=positive_corr_mat.columns)
    
    # Create arrays to store realizations (final and blocks)
    all_var = np.empty((nb_simulations,nb_idx))
    temp_var = np.zeros(shape = (block_size,nb_idx))
    for b in range(nb_blocks):
        #print(np.str(b+1) + ' block / ' + np.str(nb_blocks))
        ### Generate multivariate joint distribution based on chosen copula
        # and compute probability of each variable with cdf
        if copula == 'normal':
            copula_rand = np.random.multivariate_normal([0]*nb_idx,\
                np.float64(positive_corr_mat),block_size)
            uni_rand.loc[:,:] = norm.cdf(copula_rand)
        elif copula == 't':
            copula_rand = multivariate_t_rvs([0]*nb_idx,\
                np.float64(positive_corr_mat),copula_df,block_size)
            uni_rand.loc[:,:] = t.cdf(copula_rand, copula_df)

        # Compute realizations based on random uniform numbers and 
        # t-distributions fitted earlier
        for iIdx in range(nb_idx):
            idx = positive_corr_mat.columns[iIdx]
            temp_var[:,iIdx] = tdist.ppf(uni_rand[idx],fitted_dist[idx][0],\
                                         loc=0,scale=fitted_dist[idx][1])
        if spyder_version == '32':
            all_var[range(b*block_size,(b+1)*block_size),:] = temp_var
        elif spyder_version == '64':
            all_var = temp_var

    return pd.DataFrame(data=all_var,index=range(nb_simulations),\
                                columns=positive_corr_mat.columns)
###############################################################################
def marginal_incremental_var(index_sens_p,idx_simulations,percentile):
    """ index_sens_p: sensitivities of a portfolio. Contains columns DataIndex
    on which we loop and Sensitivity which contains the Sensitivity to each
    index.
        idx_simulations: output from MC_simulation function. Each row 
    corresponds to a simulation, each column to an index.
        percentile: number corresponding to the level of VaR (eg: 99).
        This function computes the marginal and incremental VaR of each
    DataIndex in index_sens_p. Incremental VaR of factor f is computed by 
    increasing the sensitivity of factor f (by $-1/bp), then computing the new
    portfolio VaR and taking the difference with normal portfolio VaR.
    Marginal VaR is computed by taking the difference between total 
    portfolio VaR, and portfolio VaR without a specific index.
    """
    ### Compute total portfolio VaR
    idx = index_sens_p.loc[:,'DataIndex']
    sensitivities = index_sens_p.loc[:,'Sensitivity']
    pnl_vector = np.dot(idx_simulations.loc[:,idx],sensitivities)
    VaR = np.percentile(pnl_vector,100-percentile)
    
    ### Loop through factors
    factor_var = []
    for f in idx:
        f_sens = index_sens_p.copy()
        f_row = f_sens[f_sens.DataIndex == f].index
        
        # Incremental VaR: increase sensitivity by $-1/bp
        f_sens.loc[f_row,'Sensitivity'] = f_sens.loc[f_row,'Sensitivity'] - 1
        pnl_vector = np.dot(idx_simulations.loc[:,idx],f_sens['Sensitivity'])
        f_iVaR = np.percentile(pnl_vector,100-percentile) - VaR
        
        # Marginal VaR: set sensitivity of factor to 0
        f_sens.loc[f_row,'Sensitivity'] = 0
        pnl_vector = np.dot(idx_simulations.loc[:,idx],f_sens['Sensitivity'])
        f_mVaR = VaR - np.percentile(pnl_vector,100-percentile)
        
        # Save results
        factor_var += [[f,'Incremental',f_iVaR]]
        factor_var += [[f,'Marginal',f_mVaR]]
    
    return factor_var
###############################################################################
def kernel_smoothing_weights(kernel,data,alpha,threshold=1e-8,
                             param=None):
    """ kernel: string to choose the smoothing kernel to use. eg: triangle, 
    gaussian, HD (beta function based), percentile (scenario extraction if
    exact scenario corresponds to percentile, otherwise linear interpolation
    between the 2 closest scenarios)
        data: vector containing the total portfolio PnL through Monte Carlo
    simulation (of length 200,000 for instance)
        alpha: confidence level of our Value-at-Risk. eg: 99 means weights
    will be arranged the 1% * sample_size part of the ordered scenario
    simulations
        threshold: value under which weights are set to zero
        param: optional depending on the type of Kernel chosen. If Gaussian
    this is the value of sigma. If triangle this is the distance to the alpha
    percentile of portfolio returns. This needs to be a list, even if it 
    contains only one parameter.
        The function returns a vector of weights of the same size as the input
    data. It assumes that the portfolio returns are ordered in ascending order
    so that losses come at the beginning and gains at the end. So for 99% VaR
    and sample size 200,000 the values around index 198,000 will be different
    from zero. Weights are normalized to add up to 1.
    """
    # Get number of observations in data vector
    sample_size = len(data)
    
    # Get index VaR's quantile, and list of numbers from 0 to (sample size-1)
    #var_idx = (1.-alpha/100.) * sample_size
    indices = np.arange(sample_size) * 1.
    
    # Order PnL vector of data and get VaR value
    data.sort_values(inplace=True)
    VaR = np.percentile(data,100-alpha)
    
    # Gaussian Kernel requires a sigma parameter passed in param
    if kernel == 'gaussian':
        if (param is not None) and (len(param) == 1):
            weights = np.array(np.exp(-np.power(data-VaR,2)/ \
                (2*np.power(np.float(param[0]),2))))
        else:
            print('Error with smoothing Kernel: Gaussian Kernel requires ' + \
                'a parameter sigma')
            return np.zeros(sample_size)
    # Triangle Kernel requires a distance parameter: number of non zero
    # parameters on each side of the VaR quantile
    elif kernel == 'triangle':
        if (param is not None) and (len(param) == 1):
            weights = np.array(\
                np.maximum(1-np.abs(data-VaR)/np.float(param[0]),0))
        else:
            print('Error with smoothing Kernel: Triangular Kernel requires ' + \
                'a distance sigma')
            return np.zeros(sample_size)
    # Harrell and Davis estimator: uses incomplete beta function
    # Only depends on the number of observations and the VaR percentile
    elif kernel == 'HD':
        a = (sample_size + 1) * alpha/100.
        b = (sample_size + 1) * (1 - alpha/100.)
        weights = betadist(a,b,(sample_size-indices)/sample_size) - \
            betadist(a,b,(sample_size-indices-1)/sample_size)
    # Same as percentile function, finds 1 or 2 closest scenarios to percentile
    elif kernel == 'percentile':
        weights = np.zeros(sample_size)
        # Get exact index corresponding to percentile
        idx = np.float(sample_size - 1) * (100-alpha) / 100.
        # Determine weight depending on whether idx is an integer or not
        if idx.is_integer():
            weights[np.int(idx)] = 1.
        else:
            # Determine floor and ceiling of index
            f, c = np.int(np.floor(idx)), np.int(np.ceil(idx))
            weights[f] = c - idx
            weights[c] = idx - f
    
    # Set weights below threshold to 0 and normalize weights to add up to 1
    if pd.isnull(weights).all():
        weights = kernel_smoothing_weights('percentile',data,alpha)
    else:
        weights[weights < threshold] = 0
        weights = weights / sum(weights)
    return weights
###############################################################################
def component_var(pnl_matrix,VaR,weights):
    """ pnl_matrix: dataframe containing the PnL of each factor (factors are
    columns) in each simulation (rows). The dataframe is already ordered from
    smallest (most negative) in first rows to biggest.
        VaR: value of VaR at the chosen percentile. Used to normalize results.
        weights: used for smoothing. They are roughly centered around the row
    corresponding to VaR.
        This function computes the Component VaR of each factor (column) in the
    PnL matrix according to the pre-calculated VaR and weights.
    """
    # Output results in a list of list
    output = []
    
    # Compute normalizing value so some of CVaR adds up to VaR
    normalize = VaR / sum(pnl_matrix.sum(axis=1) * weights)
    
    # Loop through factors
    for f in pnl_matrix.columns:
        CVaR = sum(pnl_matrix.loc[:,f] * weights) * normalize
        output += [[f,'Component',CVaR]]
        
    return output
###############################################################################
def MC_var(nb_simulations,positive_corr_mat,all_ts,all_ts_vols,
           index_sens,fitting_method,copula,copula_df=10,subset='',
           kernel=['gaussian','vol'],factor_VaR_percentile=99,
           parametric_vols=None,ES_percentile=99,compute_factor_results=True):
    """ nb_simulations,positive_corr_mat: parameters for the MC_simulations
    function
        all_ts: time series of the variables (corresponds to changes). Last  
    date corresponds to the date in the correlation matrix if regular VaR (not
    SVaR).
        all_ts_vols: time series of volatilities of the variables (could either
    be computed using exponential or equal weighting). Same dates as all_ts.
        index_sens: portfolio sensitivities per market index (matches row names
    and column names of positive_corr_mat), per portfolio. Contains at least 
    3 columns: DataIndex (matching as_of_cov), Sensitivity (actual dollar 
    sensitivities) and Portfolio, and potentially other columns used in subset.
        fitting_method: when fitting the t-distribution this selects whether
    results are normalized using then volatilities or not. 'normalized' or
    anything else for no normalization
        copula: type of copula to use. normal or t
        copula_df: degrees of freedom of the t copula (if t is chosen)
        subset: '' if no subsetting is done. Otherwise a dictionary with keys
    containing the name of the subset, and the values containing a dictionary
    as in function param_vol: two elements used to only select some of the
    sensitivities from index_sens. First element contains the column on which
    to subset, and  the second one contains the values we keep. eg: 
    subset={'CS':['RiskType','CS'],'Rates':['RiskType',['Rates','SwapSpread']]}
    An empty dictionary means no subset done, eg: subset={'All':[]}
        kernel: the Kernel to be used in the determination of the weights
    used to compute Component VaR with smoothing. List containing the Kernel
    name first, and any parameters in the following elements.
        factor_VaR_percentile: single number (or list of) defining the VaR
    levels for which incremental and marginal VaR of factors are computed.
        parametric_vols: volatility of each portfolio for each component using
    the parametric formula. It is used to determine the kernel smoother
    weights for the component VaR.
        ES_percentile: 95, 99, or whatever Expected Shortfall threshold we 
    want. Either a single number or a list of numbers.
        compute_factor_results: True or False. Whether to compute results at
    factor level (marginal, incremental, component). If false two lists are 
    still returned, but the factor results will be an empty list.
        This returns two things: the first one is a list that contains the 
    distribution of portfolio returns as well as Expected Shortfall based on
    the percentiles passed as input for each of the subset keys passed (if no 
    subsetting is done then all). The second output is a dataframe containing
    the marginal, incremental and component VaR for each factor in each
    portfolio.
    """
    ### Fit t-distributions to indices
    fitted_dist = fit_t_distribution(all_ts,all_ts_vols,df_floor=3.,
                                     method=fitting_method)
    
    ### Get vectors of Monte Carlo realizations
    idx_simulations = MC_simulations(nb_simulations,positive_corr_mat,
                                     fitted_dist,copula,copula_df)
    
    # Create results arrays
    results = []
    factor_results = []
    
    # Convert percentile inputs to lists if needed
    if not(isinstance(ES_percentile,list)):
        ES_percentile = [ES_percentile]
    
    if not(isinstance(factor_VaR_percentile,list)):
        factor_VaR_percentile = [factor_VaR_percentile]
    
    # If no subsetting is done add empty criteria values
    if subset == '':
        subset = {'All':[]}
    
    ### Loop through portfolios
    for p in np.unique(index_sens.Portfolio):
        index_sens_p = index_sens.loc[index_sens.Portfolio == p,:]
        ### Loop through keys and criterias of subsetting
        for key, criteria in subset.iteritems():
            # Do subsetting only if length of criteria is 2 as expected
            if len(criteria) != 2:
                # No subsetting done
                index_sens_p_subset = index_sens_p
            else:
                # Convert second element of criteria to list if needed
                if isinstance(criteria[1],basestring):
                    criteria[1] = [criteria[1]]
                # Restrict indices to those meeting the criteria chosen
                index_sens_p_subset = index_sens_p.loc[index_sens_p.loc[:,\
                    criteria[0]].isin(criteria[1]),:]
            
            # Check the subset of portfolio has some sensitivity to continue
            if len(index_sens_p_subset) < 1:
                continue

            # Select indices and sensitivities
            idx = index_sens_p_subset.loc[:,'DataIndex']
            sensitivities = index_sens_p_subset.loc[:,\
                ['DataIndex','Sensitivity']].set_index('DataIndex')
            
            # Compute PnL matrix and vector: order from smallest to largest
            # using the total PnL
            pnl_matrix = idx_simulations.loc[:,idx].multiply(\
                sensitivities.T.squeeze(),axis='columns')
            pnl_matrix.loc[:,'Total'] = pnl_matrix.sum(axis=1)
            pnl_matrix.sort_values(by='Total',inplace=True)
            pnl_vector = pnl_matrix['Total']
            

            # Compute volatility of returns, their distribution and Expected
            # Shortfall for each ES percentile chosen (and each factor)
            returns_vol = np.std(pnl_vector)
            results += [[p,'PnL Volatility',key,returns_vol]]
            
            for percentile in range(0,101):
                confidence = np.percentile(pnl_vector,100-percentile)
                results += [[p,np.str(percentile) + '%',key,confidence]]
                
            for percentile in ES_percentile:
                VaR = np.percentile(pnl_vector,100-percentile)
                pnl_matrix_tail = pnl_matrix.loc[pnl_matrix.Total <= VaR,:]
                ES = np.mean(pnl_matrix_tail.Total)
                results += [[p,'ES ' + np.str(percentile) + '%',key,ES]]
                if compute_factor_results:
                    for factor in idx.tolist():
                        ES_f = np.mean(pnl_matrix_tail[factor])
                        factor_results += [[p,percentile,key,factor,'ES',ES_f]]
            
            # Remove total column from PnL matrix
            pnl_matrix.drop('Total',axis=1,inplace=True)
            
            # If we compute factor VaR results
            if compute_factor_results:
                # Check if kernel and parameter to compute component VaR are
                # gaussian and vol: in this case get portfolio volatility
                # Copy parameter to all
                kern = kernel[:]
                if (kernel[0] == 'gaussian') & (kernel[1] == 'vol'):
                    kern[1] = parametric_vols.loc[\
                        (parametric_vols.Portfolio == p) &\
                        (parametric_vols.Component == key),'Value'].values[0]
                
                for pctile in factor_VaR_percentile:
                    # Get marginal and incremental VaR of each factor       
                    factor_VaR = marginal_incremental_var(\
                        index_sens_p_subset,idx_simulations,pctile)
                        
                    # Get Component VaR (CVaR)
                    weights = kernel_smoothing_weights(\
                        kern[0],pnl_vector[:],pctile,param=kern[1:])
                    VaR = np.percentile(pnl_vector,100-pctile)
                    factor_CVaR = component_var(pnl_matrix,VaR,weights)
                        
                    # Add factor results to factor only results
                    for r in factor_VaR:
                        factor_results += [[p,pctile,key] + r]
                    for r in factor_CVaR:
                        factor_results += [[p,pctile,key] + r]
            
    return results, factor_results
###############################################################################
def update_VaR_template(excel_file,excel_filename=None,df_list=None,
                        tab_list=None,VaR_parameters=None):
    """ excel_file: full path name of existing Excel template to use
        excel_filename: if we are not overriding existing file the new file
    is saved with this name in the same folder as the current template
        df_list: list of dataframes to paste in template
        tab_list: list of tabs corresponding to dataframes to update (same
    length and same order)
        VaR_details: if not passing dataframes directly this list contains the
    details of the run for which data is fetched from the database
        This function updates an existing Excel file (our VaR report template)
    using either the list of dataframes passed as input in df_list to the tabs
    in tab_list, or using the VaR run specs passed in VaR_details in which case
    data is downloaded from the database first according to VaR_details.
    """
    # Get folder of existing file and filename
    template_folder = '\\'.join(excel_file.split('\\')[:-1]) + '\\'
    template_name = excel_file.split('\\')[-1]
    update_excel = False
    
    # If updating using list of dataframes and tabs passed as input
    if df_list is not None:
        if len(df_list) != len(tab_list):
            print('\nInput dataframe and tab lists of different lengths, ' +\
                  'no update of VaR template\n')
        else:
            update_excel = True
    
    # If downloading data from database using VaR_details
    elif VaR_parameters is not None:
        run_details = get_existing_db_runIDs(VaR_parameters)
        # If data exists we can query results using the run IDs
        if len(run_details) > 0:
            ### Get market volatilities
            market_vols = None
            vols_columns = ['Date','Method','Factor','Value']
            for _,row in run_details.iterrows():
                # We only need volatilities for VaR and Stressed VaR
                run_type = row['RunID'].split('_')[-1]
                if run_type not in ['VaR','SVaR']:
                    continue
                elif run_type == 'VaR':
                    VaR_method = 'Regular'
                elif run_type == 'SVaR':
                    VaR_method = 'Stressed'
                
                vols = data_mt.fetch_files('VaR_output_market_vol',\
                    cond = {'VaR_output_market_vol':[\
                    ['VolMethod','=',row['VolWeighting']],\
                    ['Month','=',row['CovarianceDate']]]})
                vols.loc[:,'Date'] = row['AsOfDate']
                vols.loc[:,'Method'] = VaR_method
                
                if market_vols is not None:
                    market_vols = market_vols.append(vols[vols_columns],\
                                                     ignore_index=True)
                else:
                    market_vols = vols[vols_columns].copy()
            
            ### Get all sensitivities
            port_sens_all_dates = get_portfolio_sensitivities(\
                VaR_parameters[0],'all',VaR_parameters[4])
            portfolio_sens_df = format_sensitivities_output(\
                port_sens_all_dates,fetch_data('scenario_sensitivities'))
            # Aggregate sensitivities
            agg_sensitivities = data_mt.fetch_files(\
                'VaR_output_sensitivities_agg',cond={\
                'VaR_output_sensitivities_agg':[\
                ['Date','=',VaR_parameters[0]]]})
            
            ### Factor list and portfolios values breakdowns
            idx_tab,value_bdown = var_reporting(VaR_parameters[0],'all')
            
            ### Download rest of results using the run IDs
            IDs = run_details.RunID.tolist()
            names = ['VaR_results_main','VaR_results_factors_VaR',\
                'VaR_results_factors_vol','VaR_results_grouped_vol',\
                'VaR_results_grouped_VaR','VaR_results_largest_factors_VaR',\
                'VaR_results_largest_factors_vol']
            df_list = data_mt.fetch_files(\
                names,cond={\
                names[0]:[['RunID','=',IDs]],names[1]:[['RunID','=',IDs]],\
                names[2]:[['RunID','=',IDs]],names[3]:[['RunID','=',IDs]],\
                names[4]:[['RunID','=',IDs]],names[5]:[['RunID','=',IDs]],\
                names[6]:[['RunID','=',IDs]]})
            
            df_list += [run_details,portfolio_sens_df,agg_sensitivities,\
                market_vols,idx_tab,value_bdown]
            tab_list = ['MainResults','FactorsVaR','FactorsVol','GroupedVol',\
                'GroupedVaR','LargestFactorsVaR','LargestFactorsVol',\
                'RunDetails','Sensitivities','SensitivitiesAgg','MarketVols',\
                'Factors','PortfolioAmounts']
            
            ### Add ID column to some dataframe
            # Get order of columns
            cols_order = fc.remove_columns(data_mt.fetch_files(\
                'VaR_excel_cols_order'),['Version','TimeStamp'])
            for _,row in cols_order.iterrows():
                # Get tab name and order of columns
                tab = row['Tab']
                order = row.drop('Tab')
                order.sort_values(inplace=True)
                order.sort_index(inplace=True)
                order = order[pd.notnull(order)].tolist()
                
                # Get dataframe and add ID column
                df_idx = tab_list.index(tab)
                if 'ID' not in df_list[df_idx].columns.tolist():
                    df_list[df_idx] = fc.format_df_output(df_list[df_idx],\
                           id_cols=order)
            
            update_excel = True
        else:
            print('No data found in database for parameters passed, no update')
    
    # Update Excel file
    if update_excel:
        update_excel_file(template_name,df_list,tab_list,\
                folder=template_folder,new_name=excel_filename)
    
    return
###############################################################################
def get_existing_db_runIDs(VaR_parameters):
    """ VaR_parameters: list of parameters needed to identify run IDs, it 
    contains the following parameters (in order): portfolio as of dates, half
    life for volatilities, volatility weighting type (EWMA, Garch), volatility 
    computation function, sensitivities source, number of MC simulations,
    whether factor returns are normalized in fitting t-distribution (takes 
    value normalized or anything else), copula type, copula degrees of freedom,
    smoothing kernel type, smoothing kernel parameter, VaR_method (MonteCarlo,
    param, MonteCarloSVaR), number of months the
    covariance matrix is lagged
        This function queries the table Results_RunDetails in the database VaR
    using the run parameters passed as input and returns the corresponding
    rows. Using the RunIDs of these rows the VaR results in the rest of the
    database can be fetched.
    """
    ### Get list of parameters of the run from VaR_parameters
    # Dates
    dates = VaR_parameters[0]
    if not(isinstance(dates,list)):
        dates = [fc.format_date(dates)]
    else:
        dates = [fc.format_date(x) for x in dates]
    
    # Volatility half life, weighting scheme and computing function
    halflife = np.str(VaR_parameters[1])
    vol_weighting = VaR_parameters[2]
    vol_function = VaR_parameters[3]
    
    # Sensitivities source
    source = VaR_parameters[4]
    
    # Number of Monte Carlo simulations
    nb_sim = np.str(VaR_parameters[5])
    
    # Whether returns are normalized in t-distribution fitting
    returns_norm = VaR_parameters[6]

    # Copula details
    if VaR_parameters[7] == 'normal':
        VaR_parameters[8] = ''
    copula_details = '|'.join(map(str,VaR_parameters[7:9]))
    
    # Smoothing kernel details
    if isinstance(VaR_parameters[9],basestring):
        VaR_parameters[9] = [VaR_parameters[9]]
    smoothing_kernel = '|'.join(map(str,VaR_parameters[9]))
    
    # VaR type that were run: impacts covariance matrix date
    VaR_method = VaR_parameters[10]
    svar_calibration_date = '2008-12'
    
    # Covariance matrix calibration months for each date
    covariance_lag = VaR_parameters[11]
    calibration_dates = []
    for d in dates:
        if ('MonteCarlo' in VaR_method) or ('param' in VaR_method):
            if 'MonteCarloSVaR' in VaR_method:
                calibration_dates += [[get_calibration_month(d,covariance_lag),\
                                      svar_calibration_date]]
            else:
                calibration_dates += [get_calibration_month(d,covariance_lag)]
        else:
            calibration_dates += [svar_calibration_date]
    
    # Get list of runs in the database corresponding to inputs
    run_details = None
    d_count = 0
    for d in dates:
        run_details_d = data_mt.fetch_files('VaR_results_run_details',
            cond = {'VaR_results_run_details':[
                    ['AsOfDate','=',d],
                    ['HalfLife','=',halflife],
                    ['VolWeighting','=',vol_weighting],
                    ['VolFunction','=',vol_function],
                    ['SensitivitiesSource','=',source],
                    ['NumberSimulations','=',nb_sim],
                    ['ReturnsNormalized','=',returns_norm],
                    ['Copula','=',copula_details],
                    ['SmoothingKernel','=',smoothing_kernel],
                    ['CovarianceDate','=',calibration_dates[d_count]]]})
        d_count += 1
        
        if run_details is None:
            run_details = run_details_d.copy()
        else:
            run_details = run_details.append(run_details_d,ignore_index=True)
    
    return run_details
###############################################################################
def get_index_lists():
    """ This function gets indices that belong to a similar group in the same
    column. This is used for VaR reporting. For instance rates indices are in
    the same column, so are credit spreads, US rates, GBP rates, etc.
    """
    ### Load map from sensitivities to indices and get list of rates indices
    s_def = fetch_data('scenario_sensitivities')
    
    ## Create conditions to filter specific risk factors
    # Risk Type
    rates_cond = (s_def.RiskType == 'Rates')
    cs_cond = (s_def.RiskType == 'CS')
    ssp_cond = (s_def.RiskType == 'SwapSpread')
    # Tenors
    ir_6m_cond = (s_def.Tenor == '0.5y')
    ir_2y_cond = (s_def.Tenor == '02y')
    ir_5y_cond = (s_def.Tenor == '05y')
    ir_10y_cond = (s_def.Tenor == '10y')
    ir_20y_cond = (s_def.Tenor == '20y')
    ir_30y_cond = (s_def.Tenor == '30y')
    
    # Get lists of indices
    rates_factors = fc.col_df(s_def.loc[rates_cond,['RiskFactor']])
    rates_indices = fc.col_df(s_def.loc[rates_cond,['Index']])
    cs_indices = fc.col_df(s_def.loc[cs_cond,['RiskFactor']])
    ssp_factors = fc.col_df(s_def.loc[ssp_cond,['RiskFactor']])
    ssp_indices = fc.col_df(s_def.loc[ssp_cond,['Index']])
    ir_6m_factors = fc.col_df(s_def.loc[rates_cond & ir_6m_cond,['RiskFactor']])
    ir_6m_indices = fc.col_df(s_def.loc[rates_cond & ir_6m_cond,['Index']])
    ir_2y_factors = fc.col_df(s_def.loc[rates_cond & ir_2y_cond,['RiskFactor']])
    ir_2y_indices = fc.col_df(s_def.loc[rates_cond & ir_2y_cond,['Index']])
    ir_5y_factors = fc.col_df(s_def.loc[rates_cond & ir_5y_cond,['RiskFactor']])
    ir_5y_indices = fc.col_df(s_def.loc[rates_cond & ir_5y_cond,['Index']])
    ir_10y_factors = fc.col_df(s_def.loc[rates_cond & ir_10y_cond,['RiskFactor']])
    ir_10y_indices = fc.col_df(s_def.loc[rates_cond & ir_10y_cond,['Index']])
    ir_20y_factors = fc.col_df(s_def.loc[rates_cond & ir_20y_cond,['RiskFactor']])
    ir_20y_indices = fc.col_df(s_def.loc[rates_cond & ir_20y_cond,['Index']])
    ir_30y_factors = fc.col_df(s_def.loc[rates_cond & ir_30y_cond,['RiskFactor']])
    ir_30y_indices = fc.col_df(s_def.loc[rates_cond & ir_30y_cond,['Index']])
    
    ### Create final dataframe to contain all columns
    out_df = pd.DataFrame()
    out_df = pd.concat([out_df,rates_indices,rates_factors,cs_indices,\
        ssp_indices,ssp_factors,ir_6m_factors,ir_6m_indices,ir_2y_factors,\
        ir_2y_indices,ir_5y_factors,ir_5y_indices,ir_10y_factors,\
        ir_10y_indices,ir_20y_factors,ir_20y_indices,ir_30y_factors,\
        ir_30y_indices],ignore_index=True,axis=1)
    out_df.columns = ['Rates_Indices','Rates_Factors','CreditSpread_Factors',\
        'SwapSpread_Indices','SwapSpread_Factors','0.5y_Factors','0.5y_Indices',\
        '02y_Factors','02y_Indices','05y_Factors','05y_Indices','10y_Factors',\
        '10y_Indices','20y_Factors','20y_Indices','30y_Factors','30y_Indices']
    
    ## Get indices and sensitivity factors of rates only of each currency
    currencies = np.unique(s_def.loc[pd.notnull(s_def.Currency),'Currency'])
    for c in currencies:
        c_cond = (s_def.Currency == c)
        c_factors = fc.col_df(s_def.loc[c_cond & rates_cond,['RiskFactor']]).\
            rename(columns={'RiskFactor':c + '_Rates_Factors'})
        c_indices = fc.col_df(s_def.loc[c_cond & rates_cond,['Index']]).\
            rename(columns={'Index':c + '_Rates_Indices'})
        out_df = pd.concat([out_df,c_factors,c_indices],axis=1)
    
    ### Add map from index to risk factor
    factor_map = s_def.loc[:,['Index','RiskFactor']].rename(columns=\
        {'Index':'Map_Index','RiskFactor':'Map_Factor'})
    out_df = pd.concat([out_df,factor_map],axis=1)
    
    return out_df
###############################################################################
def var_reporting(dates,portfolio):
    """ date: single date or list of dates we are reporting on
        portfolio: single portfolio or list of we are reporting on
        This function gets data needed for reporting of VaR that is not 
    computed in the VaR function: lists of factors and portfolio breakdown in
    asset classes per currency.
    """
    ### Format inputs
    if isinstance(portfolio,basestring):
        if portfolio != 'all':
            portfolio = [portfolio]
        
    if isinstance(dates,basestring):
        dates = [fc.format_date(dates)]
    else:
        dates = [fc.format_date(x) for x in dates]
    
    
    ### Load portfolio data either from h5 file or database
    try:
        ac_ccy_bdown = data_mt.fetch_files('value_breakdown_ac_ccy')
        dates_bdown = [fc.format_date(x) for x in \
                       np.unique(ac_ccy_bdown.AsOfDate)]
        missing_dates = [x for x in dates if x not in dates_bdown]
        # If no data exists we compute it first
        if len(missing_dates) > 0:
            for d in missing_dates:
                fc.values_assetclass_currency_breakdown(d)
            ac_ccy_bdown = data_mt.fetch_files('value_breakdown_ac_ccy')
    # Otherwise we load data from HDF5 file
    except:
        try:
            ac_ccy_bdown = pd.read_hdf(data_folder_python64 +\
                'VaR_data.h5','value_breakdown_ac_ccy')
        except:
            print('Data for reporting could not be loaded\n')
            print(sys.exc_info()[0])
            raise
    
    # Restrict data to chosen portfolio and dates
    ac_ccy_bdown = fc.remove_columns(ac_ccy_bdown,['Version','TimeStamp'])
    ac_ccy_bdown = ac_ccy_bdown.loc[ac_ccy_bdown.AsOfDate.isin(dates),:]
    if not(isinstance(portfolio,basestring)):
        ac_ccy_bdown = ac_ccy_bdown.loc[\
            ac_ccy_bdown.Portfolio.isin(portfolio),:]
    ac_ccy_bdown = fc.format_df_output(ac_ccy_bdown,\
        id_cols=['AsOfDate','Portfolio','Currency','Metric'])
    
    ## List of indices
    indices_tab = get_index_lists()
    
    return indices_tab, ac_ccy_bdown
###############################################################################
def update_excel_file(filename,df_list,tabs_list,folder='',new_name=None):
    """ filename: name of existing Excel file we are updating.
        df_list: list of dataframes to paste in Excel file
        tabs: names of the tabs to update in the Excel file
    eg: tabs_list = ['Agg_Data','Agg_Activity_Data']
        folder: location of the Excel file to update. By default it corresponds
    to the current folder
        new_name: if not overriding existing file, this is the name given to
    the new file
        This function updates the tabs of the Excel file with the dataframes
    passed as inputs (erases current content).
    """
    current_folder = os.getcwd() + '\\'
    # If no folder is passed as input we use the current folder
    if folder == '':
        folder = os.getcwd() + '\\'
     
    # Check tabs list and df list are the same length
    if len(df_list) != len(tabs_list):
        print('Issue with function update_excel_file, df_list and tabs_list' +\
            ' must be the same length')
        return
    
    os.chdir(folder)
    try:
        wb = xw.Workbook(folder + filename)
    except:
        wb = xw.Book(folder + filename)
    
    # Get the dataframes in df_list and add the ID column
    for i in range(len(df_list)):
        gc.collect()
        # Get table and destination tab names
        df = fc.remove_columns(df_list[i].copy(),['Version','TimeStamp'])
        tab = tabs_list[i]
        print(tab)
        if 'ID' in df.columns.tolist():
            df.set_index('ID',inplace=True)
        elif 'RunID' in df.columns.tolist():
            df.set_index('RunID',inplace=True)
            
        try:
            ws = xw.Sheet(tab,wkb=wb)
        except:
            ws = wb.sheets[tab]
        ws.clear_contents()
        
#        # Set empty cells in column Value to 0
#        if 'Value' in df.columns.tolist():
#            df.loc[pd.isnull(df.Value),'Value'] = 0.
        if 'Factor' in df.columns.tolist():
            df.loc[:,'Factor'] = df.loc[:,'Factor'].astype(np.str)
            
        # Paste data by blocks
        block_size = 20000
        nb_blocks = len(df) / block_size + 1
        for j in range(nb_blocks):
            gc.collect()
            if j < 1:
                try:
                    ws_range = xw.Range(ws,'A1')
                except:
                    ws_range = ws.range('A1')
                ws_range.value = df[:block_size]
            else:
                paste_row = block_size * j + 2
                try:
                    ws_range = xw.Range(ws,'A' + np.str(paste_row))
                except:
                    ws_range = ws.range('A' + np.str(paste_row))
                df_start = block_size * j
                df_end = block_size * (j + 1)
                ws_range.options(header=False).value = df[df_start:df_end]
    
    if new_name is not None:
        wb.save(folder + new_name)
    else:
        wb.save()
    wb.close()
    
    # Go back to working folder
    os.chdir(current_folder)
    return
###############################################################################
def get_largest_contributors_var(factors_df,portfolio):
    """ factors_df: dataframe containing the contribution to VaR and ES or 
    volatility of each factor per portfolio
        portfolio: list of portfolios for which we want to get the largest
    contributors
        This function returns the largest contributors (in absolute value) to
    VaR and ES or volatility for each date, run ID, portfolio, method and VaR 
    level (if VaR, Metric if volatility) and component contained in the 
    dataframe.
    """
    ## If input dataframe is empty we return an empty dataframe
    if len(factors_df) < 1:
        return pd.DataFrame(columns=['Portfolio'])
    
    ## Select number of largest contributors we want to keep
    nb_largest = 25
    
    ## Keep data we need only
    df = fc.remove_columns(factors_df.copy(),'ID')
    df = df.loc[df.Portfolio.isin(portfolio),:]
    
    ## Create an ID column to specific portfolio, run, method, ..., groups
    df_cols = df.columns.tolist()
    var_id_cols = ['Date','RunID','Method','Portfolio','VaR Level',\
        'Component','Factor Measure']
    vol_id_cols = ['Date','RunID','Method','Portfolio','Component','Metric']
    # Check whether input dataframe corresponds to volatilities or VaR
    if set(var_id_cols).issubset(set(df_cols)):
        id_cols = var_id_cols
        filetype = 'VaR'
    else:
        id_cols = vol_id_cols
        filetype = 'Vol'
    
    df.loc[:,'ID'] = df[id_cols].astype(str).sum(axis=1)
    
    # Add absolute value column
    df.loc[:,'AbsValue'] = np.abs(df.loc[:,'Value'])
    
    # Save results in dataframe
    df_largest = None
    
    # Loop through unique groups
    for i in np.unique(df.ID):
        dfi = df.loc[df.ID == i,:]
        dfi = dfi.nlargest(nb_largest,'AbsValue')
        dfi.loc[:,'Order'] = range(1,len(dfi)+1)
        
        if df_largest is None:
            df_largest = dfi.copy()
        else:
            df_largest = df_largest.append(dfi,ignore_index=True)
    
    ## Remove unnecessary columns
    df_largest = fc.remove_columns(df_largest,['AbsValue','ID'])
    out_id_cols = id_cols[:] + ['Order']
    out_id_cols.remove('RunID')
    
    ## Format output results
    if filetype == 'VaR':
        id_cols = ['Date','Method','Portfolio','VaR Level','Component',\
            'Factor Measure','Order']
    else:
        id_cols = ['Date','Method','Portfolio','Component','Metric','Order']
    df_largest = fc.format_df_output(df_largest,id_cols=id_cols)
    out_cols = df_largest.columns.tolist()
    out_cols.remove('Factor')
    out_cols.remove('Value')
    out_cols += ['Factor','Value']
    
    return df_largest.loc[:,out_cols]
###############################################################################
def aggregate_sensitivities(sensitivities):
    """ sensitivities: dataframe containing sensitivities per date per 
    portfolio per factor
        This function computes aggregated sensitivites for each portfolio in
    the passed dataframe according to some of the lists in idx_list.
    The Credit Spreads factors exlcude NO_SPREAD since it does not carry risk.
    """
    # Get dataframe containing lists of risk factors (and indices) grouped
    idx_list = get_index_lists()
    
    # Only keep columns containing facotr names and transform dataframe into
    # long format
    cols = [x for x in idx_list.columns.tolist() if 'Factors' in x]
    indices = pd.melt(idx_list.loc[:,cols],\
                      var_name='FactorGroup',value_name='Factor')
    indices = indices.loc[pd.notnull(indices.Factor),:]
    
    # Add group to the list of sensitivities, only keep factors in a group
    df = pd.merge(sensitivities,indices,on='Factor',how='left')
    df = df.loc[pd.notnull(df.FactorGroup),:]
    
    # Aggregate sensitivities per date, portfolio and group
    agg = df.groupby(['Date','Portfolio','FactorGroup']).sum().reset_index()
    
    # Add ID column
    agg = fc.format_df_output(agg)
    
    return agg
###############################################################################
def format_sensitivities_output(port_sens_all_dates,sensitivities_def):
    """ port_sens_all_dates: portfolio sensitivities for portfolios and dates
    of interest for the chosen sensitivities source
        sensitivities_def: table scenario_sensitivities
        This function reformats portfolio sensitivities that can be output to
    the Excel report template.
    """
    # Get index names, removing risk factor with no associated index
    # Keep results with risk factor and index names (not the same for rates 
    # and swap spread factors)
    portfolio_sens_df = pd.merge(port_sens_all_dates,sensitivities_def[[\
        'RiskFactor','Index']],on='RiskFactor',how='left')
    portfolio_sens_risk_factor = portfolio_sens_df.loc[\
        portfolio_sens_df.RiskFactor != portfolio_sens_df.Index,:]
    portfolio_sens_df = portfolio_sens_df.loc[\
        pd.notnull(portfolio_sens_df.Index),:]
    portfolio_sens_df = fc.remove_columns(portfolio_sens_df.rename(\
        columns={'AsOfDate':'Date','Sensitivity':'Value','Index':'Factor'}),\
        'RiskFactor')
    portfolio_sens_risk_factor = fc.remove_columns(\
        portfolio_sens_risk_factor.rename(columns={'AsOfDate':'Date',\
        'Sensitivity':'Value','RiskFactor':'Factor'}),'Index')       
    portfolio_sens_df = portfolio_sens_df.append(portfolio_sens_risk_factor)
    portfolio_sens_df = fc.format_df_output(portfolio_sens_df)
    
    return portfolio_sens_df
###############################################################################
def get_calibration_month(date,delta_months):
    """ date: portfolio as of date
        delta_months: lag in months
        This function returns the calibration month used for the covariance
    matrix using the portfolio as of date and the lag in month. Results is in
    the format 'yyyy-mm'
    """
    calibration_date = (fc.date_str_to_datetime(date) - \
            relativedelta(months=delta_months)).strftime('%Y-%m')
    return calibration_date
###############################################################################
def compute_var(dates,VaR_method='MonteCarlo',halflife=12,portfolio='all',
                vol_weighting='ewma',vol_computation='internal',
                sens_source='POINT',nb_simulations=400000,
                factor_VaR_percentile=99,fitting_method='normalized',
                copula='normal',copula_df=10,kernel=['gaussian','vol'],
                covariance_lag=1,compute_factor_results=True,ES_percentile=99,
                VaR_seed=None,upload=False,update_template=False,
                excel_file=None,excel_filename=None,export_excel=False):
    """ dates: list (or string if single) of dates for which to compute the
    portfolio's Value-at-Risk
        VaR_method: 'param' to get volatility of portfolio using covariance
    matrix, 'MonteCarlo' for Monte Carlo simulation, 'MonteCarloSVaR' for SVaR
    with Monte Carlo
        halflife: half life in months with which to decay weights of older data 
    points for the computation of EWMA metrics
        vol_weighting: method used to compute volatilities and covariance. ewma
    for exponentially weighted or equal (not implemented yet)
        vol_computation: selects the function used to compute the covariance 
    matrix (hence the volatilities). internal selects the function ewm_cov 
    (which is slow) and pandas uses the function in pandas library.
        portfolio: portfolio on which to run the VaR. Either All or the 
    specific portfolio (needs to have sensitivities)
        sens_source: source of portfolio sensitivities (POINT or QRM)
        nb_simulations: number of simulations run in Monte Carlo simulation
        factor_VaR_percentile: number or list of numbers. Percentile used to 
    compute factors VaR (incremental, marginal, component)
        fitting_method: when fitting the t-distribution this selects whether
    results are normalized using then volatilities or not. 'normalized' or
    anything else for no normalization
        Value-at-Risk and Expected Shortfall
        copula: 'normal' or 't'. Type of copula used in Monte Carlo simulation
        copula_df: degrees of freedom parameter of t-copula
        kernel: the Kernel to be used in the determination of the weights
    used to compute Component VaR with smoothing. If the Kernel chosen needs a
    parameter then the input should be a list containing the kernel name first
    and the parameters after. eg: 'HD', ['gaussian',2], ['gaussian','vol']. For
    the gaussian kernel the parameter vol is used to indicate that we want to
    use the specific portfolio's volatility.
        covariance_lag: lag in months used for the covariance matrix date
    compared to the portfolio as of date. eg: for 2 with a portfolio as of the
    end of May the covariance matrix will used data up to the end of March
        compute_factor_results: True or False. Whether results are computed
    at the factor and groups of factors levels.
        ES_percentile: 95, 99, or whatever Expected Shortfall threshold we 
    want. Either a single number or a list of numbers
        VaR_seed: if something is passed as input it is the seed to use to
    generate the random numbers for VaR computation
        upload: whether to upload results to database
        update_template: whether to update a template Excel file. If True then
    the template file must be passed. If this is False and upload is False then
    results are saved in a new Excel file.
        excel_file: full path of Excel file to use as template for update if
    update_template is True.
        excel_filename: name to give Excel filename in which results are 
    exported if not overriding file passed in excel_file.
        export_excel: whether to export results into a new Excel file. Done
    by default if results are not saved into database or Excel template.
        This function computes portfolio volatility and VaR/ES using a
    parametric approach and a Monte Carlo simulation.
    It returns a dataframe containing the computed metrics for the selected
    dates and methods.
    """
    # Check VaR method is chosen and make it a list if necessary
    if VaR_method == '':
        print('No method chosen, no VaR computation')
        return
    elif isinstance(VaR_method,basestring):
        VaR_method = [VaR_method]
    
    # Calibration date for SVaR
    svar_calibration_date = '2008-12'
    
    # Check if data already exists with inputs: if it does no need to run
    VaR_parameters = [dates,halflife,vol_weighting,vol_computation,\
        sens_source,nb_simulations,fitting_method,copula,copula_df,kernel,\
        VaR_method,covariance_lag]
    run_details = get_existing_db_runIDs(VaR_parameters)
    if len(run_details) > 0:
        run_IDs = run_details.RunID.tolist()
        run_types = [x.split('_')[-1] for x in run_IDs]
        # If any of the runs existing correspond to one of the chosen methods
        # to run there is no run
        VaR_exists, vol_exists, SVaR_exists = '','',''
        if ('MonteCarlo' in VaR_method) and ('VaR' in run_types):
            VaR_exists = '\tRegular VaR results already exist\n'
        if ('param' in VaR_method) and ('vol' in run_types):
            vol_exists = '\tVolatility results already exist\n'
        if ('MonteCarloSVaR' in VaR_method) and ('SVaR' in run_types):
            SVaR_exists = '\tStressed VaR results already exist\n'
        
        # If any of the above conditions was met there is no run
        if (len(VaR_exists) + len(vol_exists) + len(SVaR_exists)) > 0:
            print('\nVaR computation was not run, the following already' + \
                  ' exists for at least one of the selected dates: ')
            print(VaR_exists + vol_exists + SVaR_exists)
            return
        
    
    # If input dates is a single string date make it into a list
    if not(isinstance(dates,list)):
        dates = [dates]
    
    # Format portfolio input
    if (portfolio != 'all') & (isinstance(portfolio,basestring)):
        portfolio = [portfolio]
    
    # Format Kernel input and copula parameters
    if isinstance(kernel,basestring):
        kernel = [kernel]
    if copula == 'normal':
        copula_df = ''
    
    ### Get monthly market data
    market_data = get_monthly_market_data()
    mkt_data_chg = market_data.diff(periods=1)
    
    ### Get aggregation reporting parameters for VaR and volatilities
    port_to_breakdown = fetch_data('VaR_portfolio_breakdown')
    factor_breakdown = fetch_data('VaR_factor_breakdown')
    if isinstance(portfolio,basestring) and (portfolio == 'all'):
        portfolio = np.unique(port_to_breakdown.Portfolio)
    
    ### Get all portfolio sensitivities for all dates
    port_sens_all_dates = get_portfolio_sensitivities(\
        dates,portfolio,sens_source)
    
    ### Get covariance matrices for all dates
    data_ewm_cov = get_covariance_mat(mkt_data_chg,vol_weighting,
        vol_computation=vol_computation,half_life_months=halflife)
    
    ### Get volatilities out of covariance matrix
    all_vols = get_vols_from_cov_mat_panel(data_ewm_cov)
    
    
    ### Load map from sensitivities to indices and get list of rates indices
    sensitivities_def = fetch_data('scenario_sensitivities')
    
    # Add risk type to risk factors in factor_breakdown
    factor_breakdown = pd.merge(factor_breakdown,sensitivities_def.loc[:,\
        ['Index','RiskType']],left_on='Index',right_on='Index',how='left')
    
    ### Save results in list with columns listed in results_col:
    # Date, Method, Metric (vol, VaR 99, ES), Component (IR, CS or All), Value
    results_col = ['Date','RunID','Method','Portfolio','Metric','Component','Value']
    results = []
    Sresults = []
    grouped_vol_col = results_col[:4] + ['Component','Metric',\
        'SubPortfolio','FactorGroup','Value']
    grouped_vol = []
    factor_vol_col = results_col[:4] + ['Component','Metric','Factor','Value']
    factor_vol = []
    factor_VaR_col = results_col[:4] + \
        ['VaR Level','Component','Factor','Factor Measure','Value']
    factor_VaR = []
    grouped_VaR_col= results_col[:4] + \
        ['Component','Level','Measure','SubPortfolio','FactorGroup','Value']
    grouped_VaR = []
    missing_SVaR_idx = {}
    # Factor volatilities (market indices, not portfolio)
    factor_mkt_vols_df = pd.DataFrame(columns=['Date','Method','Factor','Value'])
    # Run details
    run_details = pd.DataFrame(\
        columns=['RunID','AsOfDate','CovarianceDate','Seed'])
    run_details_dict = {}
    
    
    for d in dates:
        run_id = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')
        # Format date, and get value of previous month for matrix lag
        fdate = fc.format_date(d)
        d_out = pd.to_datetime(fdate)
#        calibration_date = (fc.date_str_to_datetime(fdate) - \
#            datetime.timedelta(days=31)).strftime('%Y-%m')
        calibration_date = get_calibration_month(fdate,covariance_lag)
        print(fdate)
        
        # Portfolio breakdowns
        port_breakdown = data_mt.fetch_files('VaR_port_bdown_details',\
            cond={'VaR_port_bdown_details':[['AsOfDate','=',fdate],
            ['Source','=',sens_source]]})
        port_breakdown = port_breakdown.loc[port_breakdown.Portfolio.isin(\
            portfolio),:]
        
        # Get this date's sensitivities
        try:
            port_sens = port_sens_all_dates.loc[\
                port_sens_all_dates.AsOfDate == fdate,\
                ['Portfolio','RiskFactor','Sensitivity']]
        except:
            print('Error in function compute_var.\n'+\
                'Could not get portfolio sensitivities for date: ' + fdate)
            pass
        
        # Get covariance matrix, cleaned of NAs
        try:
            as_of_cov = data_ewm_cov.loc[calibration_date].copy()
            as_of_cov = remove_na_from_cov_matrix(as_of_cov)
        except:
            print('Error in function compute_var.\n'+\
                'Could not get covariance matrix for date: ' + fdate)
            pass
        
        # Get sensitivities mapped to available indices for that date
        try:
            index_sens = remap_sens_to_avail_idx(
                port_sens,as_of_cov,sensitivities_def)
        except:
            print('Error in function compute_var.\n'+\
                'Could not get portfolio index_sens for date: ' + fdate)
            pass
        
        # Get tweaked covariance and correlation matrix made positive definite
        try:
            as_of_corr = cov_to_corr_mat(as_of_cov)
            positive_corr_mat = make_corr_mat_positive(as_of_corr)
        except:
            print('Error in function compute_var.\n'+\
                'Could not get tweaked correlation matrix for date: ' + fdate)
            pass
        
        # Parametric volatility for all 3 components
        for p in np.unique(port_sens.Portfolio):
            try:
                index_sens_p = index_sens.loc[index_sens.Portfolio == p,:]
                all_vol = param_vol(as_of_cov,index_sens_p)
                IR_vol = param_vol(as_of_cov,index_sens_p,
                    subset=['RiskType',['Rates','SwapSpread']])
                CS_vol = param_vol(as_of_cov,index_sens_p,
                    subset=['RiskType','CS'])
                
                results += [[d_out,run_id + '_vol','Parametric',p,\
                    'Volatility','All',all_vol]]
                results += [[d_out,run_id + '_vol','Parametric',p,\
                    'Volatility','Rates',IR_vol]]
                results += [[d_out,run_id + '_vol','Parametric',p,\
                    'Volatility','CS',CS_vol]]
            except:
                print('Error in compute_var with Param with date: ' +\
                      fdate)
        parametric_vols = pd.DataFrame(results,columns=results_col)
        # Erase parametric results from final output if parametric not chosen
        if 'param' not in VaR_method:
            results = []
        
        # Compute parametric volatility details: results broken down per
        # subportfolio and index group
        if 'param' in VaR_method:
            if compute_factor_results:
                grouped_vol_details, factor_vol_details = compute_vol_details(\
                    index_sens,as_of_cov,port_breakdown,factor_breakdown)
                
                for r in grouped_vol_details:
                    grouped_vol += [[d_out,run_id + '_vol','Regular'] + r]
                for r in factor_vol_details:
                    factor_vol += [[d_out,run_id + '_vol','Regular'] + r]
            
            # Save run details
            run_details_dict.update(\
                    {run_id + '_vol':[fdate,calibration_date,'']})
            
        # Get Monte Carlo results for all 3 components      
        if 'MonteCarlo' in VaR_method:
            try:
                # Get time series and vols of indices in scope up to that date
                data_start_date = '2001-02'
                # Changes
                dates_indices = (mkt_data_chg.index >= data_start_date) & \
                    (mkt_data_chg.index <= calibration_date)
                all_ts = mkt_data_chg.loc[dates_indices,
                                         positive_corr_mat.columns]
                # Volatilities
                dates_indices_vols = (all_vols.index >= data_start_date) & \
                    (all_vols.index <= calibration_date)
                all_ts_vols = all_vols.loc[dates_indices_vols,
                                         positive_corr_mat.columns]
                
                # Run Monte Carlo simulation and save results: set random seed
                if VaR_seed is not None:
                    seed = VaR_seed
                else:
                    seed = np.random.randint(0,2147483647)
                #seed = 2111035307 ###############################################
                #print('\nChange seed back to random\n')
                np.random.seed(seed)
                subset={'All':[],'CS':['RiskType','CS'],
                    'Rates':['RiskType',['Rates','SwapSpread']]}
                MC_results, MC_factor_results = MC_var(
                    nb_simulations,positive_corr_mat,all_ts,\
                    all_ts_vols,index_sens,fitting_method,\
                    copula,copula_df,subset,kernel,factor_VaR_percentile,
                    parametric_vols,ES_percentile,compute_factor_results)
                
                # Add Monte Carlo results to all results
                for r in MC_results:
                    results += [[d_out,run_id + '_VaR','Monte Carlo'] + r]
                for r in MC_factor_results:
                    factor_VaR += [[d_out,run_id + '_VaR','Monte Carlo'] + r]
                
                # Get factor VaR results aggregate
                if compute_factor_results:
                    factor_VaR_d = pd.DataFrame(MC_factor_results,\
                        columns=factor_VaR_col[3:])
                    grouped_VaR_d = compute_grouped_VaR(index_sens,factor_VaR_d,
                        port_breakdown,factor_breakdown)
                    
                    for r in grouped_VaR_d:
                        grouped_VaR += [[d_out,run_id +'_VaR','Monte Carlo']+r]
                else:
                    grouped_VaR = []
                
                # Save run details
                run_details_dict.update(\
                    {run_id + '_VaR':[fdate,calibration_date,seed]})
                    
            except:
                print('Error in compute_var with MC with date: ' + fdate)
                print(sys.exc_info()[0])
        
        # Get SVaR results with Monte Carlo for all 3 components      
        if 'MonteCarloSVaR' in VaR_method:
            # Calibration date is December 2008: get data for that date
            as_of_cov = data_ewm_cov.loc[svar_calibration_date].copy()
            as_of_cov = remove_na_from_cov_matrix(as_of_cov)
            index_sens = remap_sens_to_avail_idx(
                port_sens,as_of_cov,sensitivities_def)
            as_of_corr = cov_to_corr_mat(as_of_cov)
            positive_corr_mat = make_corr_mat_positive(as_of_corr)
            
            # Stressed Parametric volatility for all 3 components
            for p in np.unique(port_sens.Portfolio):
                try:
                    index_sens_p = index_sens.loc[index_sens.Portfolio == p,:]
                    all_vol = param_vol(as_of_cov,index_sens_p)
                    IR_vol = param_vol(as_of_cov,index_sens_p,
                        subset=['RiskType',['Rates','SwapSpread']])
                    CS_vol = param_vol(as_of_cov,index_sens_p,
                        subset=['RiskType','CS'])
                    
                    Sresults += [[d_out,run_id +'_Svol','Stressed Parametric',\
                        p,'Volatility','All',all_vol]]
                    Sresults += [[d_out,run_id +'_Svol','Stressed Parametric',\
                        p,'Volatility','Rates',IR_vol]]
                    Sresults += [[d_out,run_id +'_Svol','Stressed Parametric',\
                        p,'Volatility','CS',CS_vol]]
                except:
                    print('Error in compute_var with Param with date: ' +\
                          fdate)
            parametric_vols = pd.DataFrame(Sresults,columns=results_col)
            # Erase parametric results from final output if parametric not chosen
            if 'param' not in VaR_method:
                Sresults = []
            else:
                results += Sresults
            
            # Compute parametric volatility details: results broken down per
            # subportfolio and index group
            if 'param' in VaR_method:
                if compute_factor_results:
                    grouped_vol_details, factor_vol_details = \
                        compute_vol_details(\
                        index_sens,as_of_cov,port_breakdown,factor_breakdown)
                    
                    for r in grouped_vol_details:
                        grouped_vol += [[d_out,run_id + '_Svol','Stressed'] +r]
                    for r in factor_vol_details:
                        factor_vol += [[d_out,run_id + '_Svol','Stressed'] + r]
                
                # Save run details
                run_details_dict.update(\
                        {run_id + '_Svol':[fdate,svar_calibration_date,'']})
            
            try:
                # Get time series and vols of indices in scope for SVaR
                data_start_date = '2001-02'
                # Changes
                dates_indices = (mkt_data_chg.index >= data_start_date) & \
                    (mkt_data_chg.index <= svar_calibration_date)
                all_ts = mkt_data_chg.loc[dates_indices,
                                         positive_corr_mat.columns]
                # Volatilities
                dates_indices_vols = (all_vols.index >= data_start_date) & \
                    (all_vols.index <= svar_calibration_date)
                all_ts_vols = all_vols.loc[dates_indices_vols,
                                         positive_corr_mat.columns]
                
                # Check whether some positions have no data index
                no_index_for_sens = [x for x in index_sens.DataIndex if \
                    x not in positive_corr_mat.columns.tolist()]
                if len(no_index_for_sens) > 0:
                    keep_indices = [i for i in index_sens.index.values if \
                    index_sens.loc[i,'DataIndex'] in \
                    positive_corr_mat.columns.tolist()]
                    index_sens = index_sens.loc[keep_indices,:]
                
                # Run Monte Carlo simulation and save results: set random seed
                seed = np.random.randint(0,2147483647)
                np.random.seed(seed)
                subset={'All':[],'CS':['RiskType','CS'],
                    'Rates':['RiskType',['Rates','SwapSpread']]}
                MC_results, MC_factor_results = MC_var(\
                    nb_simulations,positive_corr_mat,all_ts,\
                    all_ts_vols,index_sens,fitting_method,
                    copula,copula_df,subset,kernel,factor_VaR_percentile,
                    parametric_vols,ES_percentile,compute_factor_results)
                
                missing_SVaR_idx.update({d_out:no_index_for_sens})
                # Add Monte Carlo results to all results
                for r in MC_results:
                    results += [[d_out,run_id + '_SVaR',\
                        'SVaR Monte Carlo'] + r]
                for r in MC_factor_results:
                    factor_VaR += [[d_out,run_id + '_SVaR',\
                        'SVaR Monte Carlo'] + r]
                
                # Get factor VaR results aggregate
                if compute_factor_results:
                    factor_VaR_d = pd.DataFrame(MC_factor_results,\
                        columns=factor_VaR_col[3:])
                    grouped_VaR_d = compute_grouped_VaR(index_sens,factor_VaR_d,
                        port_breakdown,factor_breakdown)
                    
                    for r in grouped_VaR_d:
                        grouped_VaR += [[d_out,run_id + '_SVaR',\
                            'SVaR Monte Carlo'] + r]
                else:
                    grouped_VaR = []
                
                # Save run details
                run_details_dict.update(\
                    {run_id + '_SVaR':[fdate,svar_calibration_date,seed]})
                    
            except:
                print('Error in compute_var with SVaR MC with date: ' + fdate)
        
        # Get factor volatilities for this date
        d_vols = all_vols.loc[calibration_date,:].reset_index().rename(
            columns={'major':'Factor','index':'Factor',\
                     calibration_date:'Value'})
        d_vols.loc[:,'Date'] = d_out
        d_vols.loc[:,'Method'] = 'Regular'
        factor_mkt_vols_df = factor_mkt_vols_df.append(\
            d_vols[factor_mkt_vols_df.columns.tolist()])
        
        if 'MonteCarloSVaR' in VaR_method:
            d_vols = all_vols.loc[svar_calibration_date,:].reset_index().\
                rename(columns={'major':'Factor','index':'Factor',\
                     svar_calibration_date:'Value'})
            d_vols.loc[:,'Date'] = d_out
            d_vols.loc[:,'Method'] = 'Stressed'
            factor_mkt_vols_df = factor_mkt_vols_df.append(\
                d_vols[factor_mkt_vols_df.columns.tolist()])
    
    ### Prepare data to be saved in the database
    # Details of run
    run_details = pd.DataFrame.from_dict(\
        data=run_details_dict,orient='index').reset_index()
    run_details.columns = ['RunID','AsOfDate','CovarianceDate','Seed']
    details_data = [np.str(halflife),vol_weighting,vol_computation,\
        sens_source,np.str(nb_simulations),fitting_method,\
        copula + '|' + np.str(copula_df),'|'.join(map(str,kernel))]
    details_cols =  ['HalfLife','VolWeighting','VolFunction',\
        'SensitivitiesSource','NumberSimulations','ReturnsNormalized',
        'Copula','SmoothingKernel']
    for i in range(len(details_data)):
        run_details.loc[:,details_cols[i]] = details_data[i]
    
    # VaR and volatility results
    portfolio_results_df = fc.format_df_output(results,results_col)
    if ('MonteCarlo' in VaR_method) or ('MonteCarloSVaR' in VaR_method):
        factors_VaR_df = fc.format_df_output(factor_VaR,factor_VaR_col)
        grouped_VaR_df = fc.format_df_output(grouped_VaR,grouped_VaR_col)
        var_largest_cont = get_largest_contributors_var(factors_VaR_df,portfolio)
    else:
        factors_VaR_df= pd.DataFrame(columns=['Portfolio'])
        grouped_VaR_df = pd.DataFrame(columns=['Portfolio'])
        var_largest_cont = pd.DataFrame(columns=['Portfolio'])
        
    if 'param' in VaR_method:
        factor_vol_df = fc.format_df_output(factor_vol,factor_vol_col)
        factor_vol_df = factor_vol_df.loc[factor_vol_df.Value != 0,:]
        grouped_vol_df = fc.format_df_output(grouped_vol,grouped_vol_col)
        grouped_vol_df = grouped_vol_df.loc[grouped_vol_df.Value != 0,:]
        vol_largest_cont = get_largest_contributors_var(factor_vol_df,portfolio)
    else:
        factor_vol_df = pd.DataFrame(columns=['Portfolio'])
        grouped_vol_df = pd.DataFrame(columns=['Portfolio'])
        vol_largest_cont = pd.DataFrame(columns=['Portfolio'])
    
    
    # Get sensitivities formatted and aggregated sensitivities
    portfolio_sens_df = format_sensitivities_output(\
        port_sens_all_dates,sensitivities_def)
    agg_sensitivities = aggregate_sensitivities(portfolio_sens_df)
    
    # Factor volatilities
    # For Excel file update
    factor_mkt_vols_df = fc.format_df_output(factor_mkt_vols_df)
    # For upload
    existing_vols = data_mt.fetch_files('VaR_output_market_vol',\
        cond={'VaR_output_market_vol':[['VolMethod','=',vol_weighting]]})
    vols = all_vols.reset_index()
    vols.rename(columns={'index':'Month'},inplace=True)
    vols_long = pd.melt(vols,id_vars=['Month'],var_name='Factor',\
                              value_name='Value')
    vols_long = vols_long.loc[pd.notnull(vols_long.Value),:]
    vols_long.loc[:,'VolMethod'] = vol_weighting
    new_vols = fc.compare_df(vols_long,existing_vols,\
                             ['Month','Factor','VolMethod'])
    
    # List of files to be uploaded: need to exclude ID column if it exists
    upload_df = [run_details,portfolio_results_df,factors_VaR_df,factor_vol_df,\
        grouped_vol_df,grouped_VaR_df,var_largest_cont,vol_largest_cont,\
        agg_sensitivities,new_vols]
    upload_names = ['VaR_results_run_details','VaR_results_main',\
        'VaR_results_factors_VaR','VaR_results_factors_vol',\
        'VaR_results_grouped_vol','VaR_results_grouped_VaR',\
        'VaR_results_largest_factors_VaR','VaR_results_largest_factors_vol',\
        'VaR_output_sensitivities_agg','VaR_output_market_vol']
    # Upload results
    if upload:
        try:
            for i in range(len(upload_df)):
                df = upload_df[i]
                upload_cols = df.columns.tolist()
                if 'ID' in upload_cols:
                    upload_cols.remove('ID')
                # For sensitivities_agg table check only upload data if new
                if upload_names[i] == 'VaR_output_sensitivities_agg':
                    existing = data_mt.fetch_files(upload_names[i],\
                        cond={upload_names[i]:[['Date','=',dates]]})
                    compare_cols = upload_cols[:]
                    new_df = fc.compare_df(df,existing,compare_cols)
                    df = new_df
                if len(df) > 0:
                    data_mt.upload_file(upload_names[i],df=df[upload_cols])
        except Exception as e:
            print('\nCould not complete VaR data upload to database:')
            print(e)
    
    # Get list of indices and portfolios values breakdowns
    idx_tab,value_bdown = var_reporting(dates,portfolio)
    
    # Add to list of dataframes (excluding uploaded volatilities)
    upload_df = upload_df[:-1] + [d_vols,portfolio_sens_df,idx_tab,value_bdown]
    upload_tabs = ['RunDetails','MainResults','FactorsVaR','FactorsVol',\
        'GroupedVol','GroupedVaR','LargestFactorsVaR','LargestFactorsVol',\
        'SensitivitiesAgg','MarketVols','Sensitivities','Factors',\
        'PortfolioAmounts']
    
    # Export to Excel using template file
    if update_template:
        if excel_file is None:
            print('No template Excel file passed as input')
        else:
            update_VaR_template(excel_file,excel_filename,upload_df,\
                                upload_tabs)
    
    # If we're not updating template or uploading to database, export to Excel
    if export_excel or (not(upload) and not(update_template)):
        fc.export_to_excel(upload_df,location=os.getcwd(),\
            filename='VaR Results',excel_format='xlsx',tab=upload_tabs)
    
    return
###############################################################################

### Functions

import os
import pandas as pd
import numpy as np
import dateutil
import datetime
from operator import itemgetter
from itertools import chain
import gc
import xlwings as xw
import warnings
######################## Display Options ######################################
warnings.simplefilter(action='ignore')
np.warnings.filterwarnings('ignore')
pd.reset_option("display.width")
pd.set_option('display.width', 135)
#pd.set_option('display.max_columns', None) # default is 20
#pd.set_option('display.max_rows', None) # default is 20
#np.set_printoptions(threshold=np.inf) # default is 100
###############################################################################

######################## Functions folder #####################################
#functions_folder = "P:\\Portfolio VaR\\POINT Tools\\Scenarios\\Production\\"
#functions_folder = "P:\\Remy\\Stress Scenarios\\Production\\"
#os.chdir(functions_folder)
import CSIM_CusipToIndexMapping
reload(CSIM_CusipToIndexMapping)
import PointExtract_HFV_merge
reload(PointExtract_HFV_merge)
import StressCalculator
reload(StressCalculator)
import ms_access_data_mgmt as data_mt
reload(data_mt)
import QRM_valuation_parser
reload(QRM_valuation_parser)
import sd_functions as sdf
reload(sdf)
###############################################################################
def get_closest_date_around(date,dates_list):
    """ Return the date closest to input date in the list dates_list.
    """
    if isinstance(date,basestring):
        d = date_str_to_datetime(date)
    else:
        d = date
    
    if isinstance(dates_list[0],basestring):
        d_list = date_str_to_datetime(dates_list)  
    else:
        d_list = dates_list[:]
    
    t_idx = min(enumerate([np.abs(x-d) for x in d_list]),key=itemgetter(1))[0]
    
    return d_list[t_idx]
###############################################################################
def remove_columns(df,cols):
    """ df: dataframe
    cols: list of columns to be removed (or a string)
    Remove specific columns from a dataframe if they are indeed in it.
    If the columns listed in cols are not in df then nothing happens. Columns
    are checked one by one.
    """
    
    # If cols is a single string convert it to a list
    if isinstance(cols,basestring):
        cols = [cols]
    
    new_df = df.copy()
    df_columns = df.columns.tolist()
    for col in cols:
        if col in df_columns:
            del new_df[col]
    
    return new_df
###############################################################################
# Merge two dataframes: ignore case by uppercasing the key column
def merge_no_case(df1,df2,col1,col2,merge_how):
    """ Same parameters as the pandas function merge used to merge two
    dataframes, but here the columns from each dataframe used for the merge
    are transformed to their uppercase version to avoid case issues, so that
    foo still maps to FOO/FoO/etc.
    If the columns on which we are merging are called the same then we keep the
    columns from df1 in the dataframe returned.
    """
    if isinstance(col1,basestring):
        col1 = [col1]
        col2 = [col2]
    
    # In case df2 does not contain unique rows drop duplicates
    df2.drop_duplicates(subset=col2,keep='first',inplace=True)
    
    nb_cols = len(col1)
    if nb_cols != len(col2):
        print('Different number of columns given for df1 and df2')
        return
    else:
        col1_upper = []
        col2_upper = []
        for i in range(nb_cols):
            col1_up = col1[i] + '_upper'
            col2_up = col2[i] + '_upper'
            df1.loc[:,col1_up] = df1[col1[i]].apply(lambda x:
                np.str(x).upper())
            df2.loc[:,col2_up] = df2[col2[i]].apply(lambda x: 
                np.str(x).upper())
            
            if col1[i] == col2[i]:
                df2 = remove_columns(df2,col2[i])
            
            col1_upper += [col1_up]
            col2_upper += [col2_up]
        
        merged = pd.merge(df1,df2,left_on=col1_upper,
                          right_on=col2_upper,how=merge_how)
        
        for i in range(nb_cols):
            if col1[i] == col2[i]:
                merged = remove_columns(merged,col1_upper[i])
            else:
                merged = remove_columns(merged,[col1_upper[i],col2_upper[i]]) 
        
        return merged
###############################################################################
def format_date(date,str_format=''):
    """ date: date in the form of a string or datetime.datetime
    str_format: if a special format is needed (eg: to avoid slashes if the
    date is going to be in some Excel file's name)
    Takes either a string value containing a date where the year might be at 
    the beginning or the end, where separators might be dashes, slashes, spaces
    or nothing and returns a standard string format. The default is to have the
    date at the end (last 2 or last 4 characters) and the month before the day.
    The input could also be a datetime.datetime variable, the result will be 
    the same.
    The result is a string with the following format: mm/dd/yyyy.
    """
    if str_format == '':
        str_format = "%m/%d/%Y"
    
    if isinstance(date,list) or isinstance(date,np.ndarray):
        if isinstance(date[0],datetime.datetime):
            formatted = [datetime.datetime.strftime(x,str_format)\
                for x in date]
        else:
            formatted = [dateutil.parser.parse(np.str(x)).strftime(str_format)\
                for x in date]
        
    else:
        if isinstance(date,datetime.datetime):
            formatted = datetime.datetime.strftime(date,str_format)
        else:
            formatted = dateutil.parser.parse(np.str(date)).strftime(str_format)
    
    return formatted
###############################################################################
def date_str_to_datetime(str_date):
    """ str_date: date in a string format as returned by the function
    format_date.
    Returns a string or a list (depending on input format) containing the input
    dates in a datetime format that can be sorted.
    """
    str_format = '%m/%d/%Y'
    if isinstance(str_date,basestring):
        output = datetime.datetime.strptime(str_date,str_format)
        
    elif isinstance(str_date,list) or isinstance(str_date,np.ndarray):
        output = [datetime.datetime.strptime(x,str_format) for x in str_date]
    
    else:
        print('Unexpected format of input received, no date formatting')
        return
    
    return output  
###############################################################################
def get_existing_dates(table):
    """ table: name of one of the tables as used in the code
    Gets the unique list of dates for which we have data for the table passed 
    as input (eg: HFV_daily_final, IP_mapped, IP_stress_pnl)
    This function returns the list of unique dates existing in a table (field
    AsOfDate) and returns a list of those formatted as mm/dd/yyyy
    """
    dates = data_mt.fetch_files(table,columns={table:['AsOfDate']})
    unique_sorted_dates = np.sort(np.unique(dates))
    dates_list = [dateutil.parser.parse(np.str(x)).strftime("%m/%d/%Y")\
        for x in unique_sorted_dates if pd.notnull(x)]
    return dates_list
###############################################################################    
def get_closest_date(date,filetype,before_after='before'):
    """ date: date we want to find the data closest to
    filetype: which type of file (eg: FRY14Q, HFV monthly) for which we want to
    find the as of date closest to the input date for which we have data. If
    the exact date exist we return that date.
    before_after: whether we want the result date to be before the 
    input date (older data) or after (newer data, can be used for historical
    portfolios). Should either be equal to 'before' or 'after'. If we want the
    latest different date use 'before_strict' and 'after_strict' for the next
    different date.
    The function returns the as of date closest to the input date for which 
    data for filetype is available. The date is in the form of a string in
    format mm/dd/yyyy.
    """
    ### Format input date
    datetime_date = datetime.datetime.strptime(date,"%m/%d/%Y")
    
    ### Get list of dates for which we have data for that file type
    dates_list = get_existing_dates(filetype)
    datetime_dates_list = [datetime.datetime.strptime(x,"%m/%d/%Y")\
        for x in dates_list]
    
    # Find closest date depending on whether we want older or newer data
    if before_after == 'before':
        try:
            closest_date = format_date(max([x for x in datetime_dates_list if
                                            x <= datetime_date]))
        except:
            closest_date = ''
    
    elif before_after == 'after':
        try:
            closest_date = format_date(min([x for x in datetime_dates_list if
                                            x >= datetime_date]))
        except:
            closest_date = ''
    
    elif before_after == 'before_strict':
        try:
            closest_date = format_date(max([x for x in datetime_dates_list if
                                            x < datetime_date]))
        except:
            closest_date = ''
    
    elif before_after == 'after_strict':
        try:
            closest_date = format_date(min([x for x in datetime_dates_list if
                                            x > datetime_date]))
        except:
            closest_date = ''                                        
    
    else:
        closest_date = ''
    
    return closest_date
###############################################################################
def upload_to_db(filetype,filename='',file_location='',df=None,tab=None):
    """ filetype: type of file in code we want to upload (eg:HFV_daily_prelim,
    POINT_report, IP_not_mapped)
        filename: if uploading an Excel file, name of the file as it appears in
    the folder with extension (eg: HFV_20161130_MonthlyDaily.xlsx)
        file_loc: if uploading an Excel file, location of file to upload in 
    Python format (eg: P:\\Remy\\HFV\\)
        df: if uploading a Python dataframe, the dataframe to upload
        tab: NOT IMPLEMENTED YET
        This function upload to the table of filetype by either taking an Excel
    file using its name and location on a drive, or using the dataframe
    directly if was computed earlier in the code
    """
    try:
        data_mt.upload_file(filetype,filename,file_location,df=df)
        print(np.str(filetype) + ' was parsed\n')
    except Exception as e:
        print('\n' + np.str(filetype) + ': Error, not parsed\n')
        print(e)
    return
###############################################################################
def upload_and_check(filetype,filename='',file_location='',df=None):
    """ filetype: type of file in code we want to upload (eg:HFV_daily_prelim,
    POINT_report, IP_not_mapped)
        filename: if uploading an Excel file, name of the file as it appears in
    the folder with extension (eg: HFV_20161130_MonthlyDaily.xlsx)
        file_loc: if uploading an Excel file, location of file to upload in 
    Python format (eg: P:\\Remy\\HFV\\)
        df: if uploading a Python dataframe, the dataframe to upload
        tab: NOT IMPLEMENTED YET
        This function calls the function upload_to_db and then does a check
    on the uploaded data (dependent on the type of file uploaded)
    """
    # Upload to database
    upload_to_db(filetype,filename,file_location,df=df)
    
    # Do check on data that was uploaded
    if filetype == 'HFV_daily_prelim':
        excel_df = read_excel(filename,folder=file_location)
        date = format_date(excel_df.As_of_Date[0])
        HFV_input_checks(date,filetype=filetype)
    
    return
###############################################################################
def HFV_BPV_to_POINT_KRD(IP_data):
    """ This function fills the portfolio data so that instruments that were
    not modeled in POINT still have some key rate durations. It takes a bond's
    OAD to estimate at which tenors present in POINT its IR BPV coming 
    from the HFV file should be allocated to.
    The input data is a dataframe that contains the POINT key rate durations
    in columns named 'KRD_XXy' where XX is the tenor in years (eg: 0.5, 2, 10),
    and the bonds' duration and IR BPV from the HFV file are in the columns
    'QRM OAD' and 'QRM BPV'.
    """
    original_cols = IP_data.columns.tolist()
    ## Get dictionary of tenors: number in years as key, column name as value
    tenors = {}
    for col in original_cols:
        if col[:4] == 'KRD_':
            tenors.update({np.float(col[4:len(col)-1]): col})

    ## List of instruments that need their KRDs info filled    
    no_KRD = IP_data.KRD_10y.isnull()
    
    ## If a bond's duration is shorter than the shortest tenor, or longer than
    # the longest, then its BPV is set to that tenor
    below_shortest = no_KRD & (IP_data.loc[no_KRD,'QRM OAD'] <= sorted(tenors)[0])
    above_longest = no_KRD & (IP_data.loc[no_KRD,'QRM OAD'] >= sorted(tenors)[-1])
    IP_data.ix[below_shortest,tenors[sorted(tenors)[0]]] = \
        IP_data.ix[below_shortest,'QRM BPV']
    IP_data.ix[above_longest,tenors[sorted(tenors)[-1]]] = \
        IP_data.ix[above_longest,'QRM BPV']
    
    ## If a bond's duration falls between 2 tenors then the BPV is split
    # between the 2 tenors
    for iTenor in range(0,len(tenors)-1):
        lower_bound = sorted(tenors)[iTenor]
        upper_bound = sorted(tenors)[iTenor+1]
        in_range = no_KRD & (lower_bound < IP_data.loc[no_KRD,'QRM OAD'])
        in_range = in_range & (IP_data.loc[in_range,'QRM OAD'] <= upper_bound)
        # Check there are some bonds in that range        
        if len(in_range)>0:
            lower_bound_weight = (upper_bound-IP_data.loc[in_range,'QRM OAD'])/\
                (upper_bound-lower_bound)
            IP_data.ix[in_range,tenors[lower_bound]] = \
                lower_bound_weight * IP_data.ix[in_range,'QRM BPV']
            IP_data.ix[in_range,tenors[upper_bound]] = \
            (1-lower_bound_weight) * IP_data.ix[in_range,'QRM BPV']
    
    ## Fill tenors that did not get filled with 0
    IP_data.loc[no_KRD,tenors.values()] = IP_data.loc[no_KRD,tenors.values()].\
        fillna(value=0)
    return IP_data
###############################################################################
def bond_to_portfolio_sensitivities(bond_data,scenario_sens=None):
    """ bond_data: sensitivities at the bond level. As available in the table
        scenario_sens: table scenario_sensitivities
    Results_Security_Sensitivities, with SpreadIndex_ID column added
    Returns portfolio sensitivities aggregated at the risk factor index.
    """
    ## Check there is data, otherwise return empty dataframe
    if len(bond_data) < 1:
        portfolio_sensitivities = pd.DataFrame(data=None,
            columns=[['RiskFactor','Sensitivity']])
        return portfolio_sensitivities
    
    ## Aggregate CUSIP level sensitivities per risk factor to get
    # portfolio sensitivities
    
    ## Aggregate OASD and Vega per SpreadIndex_ID
    OASD = bond_data.groupby(['SpreadIndex_ID']).agg({'OASD':'sum'})
    OASD.reset_index(inplace=True)
    OASD.rename(index=str,inplace=True,
        columns={'OASD':'Sensitivity','SpreadIndex_ID':'RiskFactor'})
    Vega = bond_data.groupby(['SpreadIndex_ID']).agg({'Vega':'sum'})
    Vega.reset_index(inplace=True)
    Vega.rename(index=str,inplace=True,
        columns={'Vega':'Sensitivity','SpreadIndex_ID':'RiskFactor'})
    Vega.loc[:,'RiskFactor'] = Vega.apply(lambda x: 'Vega_' + x.RiskFactor,\
        axis=1)
    
    # Aggregate OAD per currency
    bond_data.rename(index=str,columns={'KRD_0_5y':'KRD_0.5y'},inplace=True)
    KRD_cols = [x for x in bond_data.columns if x[:4] == 'KRD_']
    OAD_table = bond_data[['Currency'] + KRD_cols].\
        groupby(['Currency']).sum()
    OAD_table.reset_index(inplace=True)
    OAD = pd.melt(OAD_table,id_vars='Currency',var_name='tenor',
                  value_name='Sensitivity')
    OAD.loc[:,'RiskFactor'] = OAD.apply(lambda x:\
                                        x.Currency + '_' + x.tenor,axis=1)
    
    # Aggregate OAC per Currency
    bond_data.rename(index=str,columns={'OAC_0_5y':'OAC_0.5y'},inplace=True)
    OAC_cols = [x for x in bond_data.columns if x[:4] == 'OAC_']
    OAC_table = bond_data[['Currency'] + OAC_cols].\
        groupby(['Currency']).sum()
    OAC_table.reset_index(inplace=True)
    OAC = pd.melt(OAC_table,id_vars='Currency',var_name='tenor',
                  value_name='Sensitivity')
    OAC.loc[:,'RiskFactor'] = OAC.apply(lambda x:\
                                        x.Currency + '_' + x.tenor,axis=1)
                                        
    # Swap Spread sensitivities: same as OAD, but exclude NO_SPREAD bonds
    # and bonds with CSIM index over Treasury
    if scenario_sens is None:
        scenario_sens = data_mt.fetch_files(
            'scenario_sensitivities',
            cond={'scenario_sensitivities':[['RiskType','=','CS']]},
            columns={'scenario_sensitivities':['RiskFactor','ReferenceCurve']})
    else:
        scenario_sens = scenario_sens.loc[scenario_sens.RiskType == 'CS',\
            ['RiskFactor','ReferenceCurve']]
    bond_data = pd.merge(bond_data,scenario_sens,left_on='SpreadIndex_ID',
                       right_on='RiskFactor',how='left')
    bond_data.loc[:,'SSP_ajd'] = bond_data.apply(lambda x: 0 
        if x.SpreadIndex_ID == 'NO_SPREAD'
        or x.ReferenceCurve == 'Treasury' else 1,axis=1)
    SSP_table = bond_data.loc[bond_data.SSP_ajd > 0,\
        ['Currency'] + KRD_cols].groupby(['Currency']).sum()
    SSP_table.reset_index(inplace=True)
    SSP = pd.melt(SSP_table,id_vars='Currency',var_name='tenor',
                  value_name='Sensitivity')
    # Check there is data to avoid issues with aggregation
    if len(SSP) > 0:
        SSP.loc[:,'RiskFactor'] = SSP.apply(lambda x:
                                            x.Currency + '_' + x.tenor,axis=1)
        SSP = SSP.replace(to_replace={'RiskFactor':{'_KRD_':'_SSP_'}},
                          regex=True)
    else:
        SSP = pd.DataFrame(data=None,columns=OAD.columns)
    
    # Aggregate all sensitivities in one dataframe
    portfolio_sensitivities = OASD.append(OAD[['RiskFactor','Sensitivity']]).\
        append(SSP[['RiskFactor','Sensitivity']],ignore_index=False).\
        append(Vega[['RiskFactor','Sensitivity']]).\
        append(OAC[['RiskFactor','Sensitivity']])
    
    # Remove zero entries
    portfolio_sensitivities = portfolio_sensitivities.loc[\
        portfolio_sensitivities.Sensitivity != 0,:]
    
    return portfolio_sensitivities
###############################################################################
def update_security_agg_map(date,bonds,ret_result=False,upload=True):
    """ date: as of date of the portfolio
        bonds: list of securities we need to add/update in the security_agg_map
        ret_result: True or False. Whether to return results of function.
        upload: True or False. Whether to update results to database.
        This function adds the bonds for the as of date passed as inputs into  
    the table security_agg_map. It fetches data from several sources to assign 
    each bond to one of the possible aggregation keys.
    The keys are: AssetClass, which corresponds to the breakdown of the 
    portfolio used in POINT. Intent that separates bonds based on their
    Par Value in AFS and HTM intents (Agg_Ratio is necessary here). BOLI 
    separates BOLI and non-BOLI instruments (some bonds could exist both in
    and outside of BOLI). Total is used as the key for the IP.
    The different inputs are the HFV file (for SEC 1,2,3 parameters), the
    HFV POINT merged files for the POINT only instruments and the QRM report
    parser parameters for instruments with CUSIPs only used for QRM report.
    This function can be run several times as only the bonds passed as inputs
    are added to the Access database.
    Because BOLI instruments were part of the IP until May 2017 (included) IP
    includes BOLI bonds until that date, but starting June 2017 IP excludes
    BOLI. Therefore the BOLI key breaks down non-BOLI and BOLI only bonds until
    May 2017, and starting June 2017 it breaks down BOLI only and IPplusBOLI,
    non-BOLI is simply IP starting June 2017.
    """
    # Check data is passed, otherwise return empty dataframe
    upload_cols = ['Security','Agg_Key','Agg_Value','Agg_Ratio']
    if len(bonds) < 1:
        return pd.DataFrame(data=None,columns=['AsOfDate']+upload_cols)
    
    # Format input date
    fdate = format_date(date)
    
    ##### Get list of instruments already mapped, back out unmapped ones
    # and already existing keys
    mapped_bonds = remove_columns(data_mt.fetch_files('security_agg_map',
        cond={'security_agg_map':[['AsOfDate','=',fdate]]}),\
        ['Version','TimeStamp'])
    unmapped = [x for x in bonds if x not in mapped_bonds.Security.tolist()]
    existing_keys = np.unique(mapped_bonds.Agg_Key)
    
    ### Check whether all aggregation keys already exist: if not it means
    # some new aggregation has to be done even if all bonds are mapped for
    # some of the keys
    all_keys = ['AssetClass','BOLI','Entity','Intent','Total']
    if len(existing_keys) < len(all_keys):
        missing_keys = [x for x in all_keys if x not in existing_keys]
    else:
        missing_keys = []
        
    # Check all legal entities exist in current data
    legal_entities_def = data_mt.fetch_files('legal_entities_def')
    entities_list = np.unique(legal_entities_def.Entity).tolist()
    existing_entities = np.unique(mapped_bonds.loc[\
        mapped_bonds.Agg_Key == 'Entity','Agg_Value'].values).tolist()
    missing_entities = [x for x in entities_list if x not in existing_entities]
    
    
    # If no bond is missing, all keys exist and all entities are mapped, the 
    # function call is unnecessary
    if (len(unmapped) < 1) and (len(missing_keys) < 1) and \
       (len(missing_entities) < 1):
        return pd.DataFrame(data=None,columns=['AsOfDate']+upload_cols)
    
    ## Get indicator of whether as of date is before June 2017
    # If yes all instruments are in the IP, otherwise only non-BOLI are
    before_june2017 = (date_str_to_datetime(fdate) < \
        date_str_to_datetime(format_date('6/30/2017')))
    
    ##### Load Parameters
    # HFV daily prelim
    hfv_prelim = data_mt.fetch_files('HFV_daily_prelim',
        cond = {'HFV_daily_prelim':[['AsOfDate','=',fdate]]})
    # HFV daily final (used for AFS HTM ratio only, no SEC columns)
    hfv_final = data_mt.fetch_files('HFV_daily_final',
        cond = {'HFV_daily_final':[['AsOfDate','=',fdate]]})
    # POINT HFV reports merged
    point_hfv = data_mt.fetch_files('POINT_hfv_merged',
        cond = {'POINT_hfv_merged':[['AsOfDate','=',fdate]]})
    # Instruments out of QRM report not in HFV
    non_intrader = remove_columns(data_mt.\
        fetch_files('non_intrader_bonds'),['Version','TimeStamp'])
    qrm_no_cusip_info = remove_columns(data_mt.fetch_files(\
        'qrm_valuation_param_no_cusip'),['Version','TimeStamp'])
    non_intrader = non_intrader.append(qrm_no_cusip_info)
    # SEC 1,2,3 to AssetClass map
    sec_to_partition = data_mt.fetch_files('sec_to_partition')
    
    
    ##### Find bonds in one of the inputs
    ## HFV Prelim and Final (final only if needed)
    hfv_cols_old = ['Security','SEC 1','SEC 2','SEC 3','Curr Intent',
                       'Par/Curr_Face USD_Equiv','Group','Reg']
    hfv_cols_new = ['Security','SEC1','SEC2','SEC3','Intent','Par','Group',\
        'Reg']
    hfv_prelim = hfv_prelim.loc[:,hfv_cols_old].rename(index=str,columns=\
        dict(zip(hfv_cols_old,hfv_cols_new)))
    # hfv_prelim = hfv_prelim.loc[hfv_prelim.Security.isin(unmapped),:] ###########
    
    # Split Asset Class (AC), intent (AFS_HTM), BOLI and entity mapping
    sec_cols = ['SEC1','SEC2','SEC3']
    hfv_AC = hfv_prelim.loc[:,['Security'] + sec_cols]
    hfv_intent = hfv_prelim.loc[:,['Security','Intent','Par']]
    hfv_boli = hfv_prelim.loc[:,['Security','Group','Par']]
    hfv_entity = hfv_prelim.loc[:,['Security','Reg','Group','Par']]
    
    # If HFV prelim is empty or some bonds are found in the HFV final but not
    # the HFV prelim, use HFV final
    in_final_not_prelim = [x for x in hfv_final.Security.tolist() if \
        x not in hfv_prelim.Security.tolist()]
#    unmapped_in_final = [x for x in unmapped if x not in\ ########################
#        hfv_prelim.Security.tolist()]
#    if len(unmapped_in_final) > 0 or len(missing_keys) > 0:
    if len(in_final_not_prelim) > 0:
        hfv_final = hfv_final.loc[:,hfv_cols_old].rename(index=str,columns=\
            dict(zip(hfv_cols_old,hfv_cols_new)))
        hfv_final = hfv_final.loc[\
            hfv_final.Security.isin(in_final_not_prelim),:]
        hfv_AC = hfv_AC.append(hfv_final.loc[:,['Security'] + sec_cols])
        hfv_intent = hfv_intent.append(hfv_final.loc[:,\
            ['Security','Intent','Par']])
        hfv_boli = hfv_boli.append(hfv_final.loc[:,\
            ['Security','Group','Par']])
        hfv_entity = hfv_entity.append(hfv_final.loc[:,\
            ['Security','Reg','Group','Par']])
     
    ## Asset Class
    hfv_AC = merge_no_case(hfv_AC,sec_to_partition,sec_cols,sec_cols,'left')
    hfv_AC.rename(index=str,columns={'Partition':'AssetClass'},inplace=True)
    hfv_AC.drop_duplicates(subset='Security',keep='first',inplace=True)
    
    ## Intent
    if len(hfv_intent) > 0:
        hfv_AFS_par = hfv_intent.loc[hfv_intent.Intent=='AFS',:].\
            groupby('Security').sum().reset_index().rename(index=str,\
            columns={'Par':'Par_AFS'})
        hfv_intent_agg = hfv_intent.groupby('Security').sum().reset_index()
        hfv_intent_agg = merge_no_case(hfv_intent_agg,hfv_AFS_par,
            'Security','Security','left')
        hfv_intent_agg.loc[:,'AFS'] = hfv_intent_agg.apply(lambda x:\
            (x.Par_AFS/x.Par) if (pd.notnull(x.Par_AFS) and x.Par_AFS > 0) \
            else 0,axis=1)
        hfv_intent_agg.loc[:,'HTM'] = 1 - hfv_intent_agg.loc[:,'AFS']
    else:
        # Create empty dataframe to avoid errors when merging results
        hfv_intent_agg = pd.DataFrame(data=None,\
                                      columns=['Security','AFS','HTM'])
    
    ## BOLI
    if len(hfv_boli) > 0:
        hfv_boli_par = hfv_boli.loc[hfv_boli.Group =='9999BOLI',:].\
            groupby('Security').sum().reset_index().rename(index=str,\
            columns={'Par':'Par_BOLI'})
        hfv_boli_agg = hfv_boli.groupby('Security').sum().reset_index()
        hfv_boli_agg = merge_no_case(hfv_boli_agg,hfv_boli_par,
            'Security','Security','left')
        hfv_boli_agg.loc[:,'BOLI'] = hfv_boli_agg.apply(lambda x:\
            (x.Par_BOLI/x.Par) if (pd.notnull(x.Par_BOLI) and x.Par_BOLI > 0) \
            else 0,axis=1)
        hfv_boli_agg.loc[:,'NotBOLI'] = 1 - hfv_boli_agg.loc[:,'BOLI']
    else:
        # Create empty dataframe to avoid errors when merging results
        hfv_boli_agg = pd.DataFrame(data=None,\
                                      columns=['Security','BOLI','NotBOLI'])
    
    ## Legal entities: TOB, Europe, UK
    # Legal Entity definitions
    legal_entities_def = remove_columns(legal_entities_def,\
        ['Version','TimeStamp'])
    legal_entities_def['Reg'] = legal_entities_def.apply(lambda x: \
        np.str(np.int(x.Reg.split('.')[0])),axis=1)
    entities = np.unique(legal_entities_def.Entity)
    
     # Loop through each entity in legal_entities_def
    hfv_entity_agg = pd.DataFrame(data=None,\
                                  columns=['Security','Agg_Value','Agg_Ratio'])
    
    if len(hfv_entity) > 0:
        # Format Reg column and get each bond's total Par
        hfv_entity['Reg'] = hfv_entity.apply(lambda x: \
            np.str(x.Reg.split('.')[0]),axis=1)
        hfv_entity_par_agg = hfv_entity.groupby('Security').sum().reset_index()
    
        for e in entities:
            # Get entity's criteria
            e_def = legal_entities_def.loc[legal_entities_def.Entity == e,:]
            e_def = remove_columns(e_def,'Entity')
            # Get list of instruments in that entities
            e_bonds = pd.DataFrame(data=None,columns=['Security','Par'])
            for _,row in e_def.iterrows():
                bonds_row = hfv_entity
                for c in e_def.columns:
                    if pd.notnull(row[c]):
                        bonds_row = bonds_row.loc[bonds_row[c] == row[c],:]
                e_bonds = e_bonds.append(bonds_row)
            # Aggregate per bond
            e_bonds = e_bonds.groupby('Security').sum().reset_index()
                
            # Compute ratio of Par value in this entity to total Par value
            e_bonds.rename(index=str,columns={'Par':'Par_e'},inplace=True)
            e_bonds = merge_no_case(e_bonds,hfv_entity_par_agg,\
                'Security','Security','left')
            e_bonds['Agg_Ratio'] = e_bonds.apply(lambda x:\
                x.Par_e / x.Par,axis=1)
            e_bonds['Agg_Value'] = e
            hfv_entity_agg = hfv_entity_agg.append(\
                e_bonds[hfv_entity_agg.columns])
            
    ## HFV and POINT reports merged: get mapping for instruments that are in
    ##  the HFV-POINT merged report only
    AC_unmapped = [x for x in bonds if x not in hfv_AC.Security.tolist()]
    intent_unmapped = [x for x in bonds if \
        x not in hfv_intent_agg.Security.tolist()]
    boli_unmapped = [x for x in bonds if \
        x not in hfv_boli_agg.Security.tolist()]
    
    # Asset Class for all instruments
    point_hfv_AC = point_hfv.loc[point_hfv.Security.isin(AC_unmapped),:]
    point_hfv_AC.rename(index=str,columns={'Custom Portfolio 1':'AssetClass'},\
        inplace=True)
    # POINT only instruments are set with AFS as intent
    point_hfv_intent = point_hfv.loc[point_hfv.Security.isin(intent_unmapped),:]
    point_hfv_intent = point_hfv_intent.loc[point_hfv_intent.comment == \
        'POINT_only',:]
    if len(point_hfv_intent) > 0:
        point_hfv_intent.loc[:,'AFS'] = 1.0
        point_hfv_intent.loc[:,'HTM'] = 0.0
    else:
        point_hfv_intent = pd.DataFrame(data=None,\
                                        columns=hfv_intent_agg.columns)
    # POINT only instruments are set as not BOLI
    point_hfv_boli = point_hfv.loc[point_hfv.Security.isin(boli_unmapped),:]
    point_hfv_boli = point_hfv_boli.loc[point_hfv_boli.comment == \
        'POINT_only',:]
    if len(point_hfv_boli) > 0:
        point_hfv_boli.loc[:,'NotBOLI'] = 1.0
        point_hfv_boli.loc[:,'BOLI'] = 0.0
    else:
        point_hfv_boli = pd.DataFrame(data=None,\
                                        columns=hfv_boli_agg.columns)
    
    ## Entity of POINT only instruments: some swaps are TOB, other swaps are 
    # in the US and Wall Street instruments are booked in Asia
    ## We only need to check the POINT only instruments
    point_only = point_hfv.loc[point_hfv.comment == 'POINT_only',:]
    
    # Get Summit swaps
    summit_swaps = data_mt.fetch_files('summit_extract',\
        columns={'summit_extract':['DEAL_NBR','BOOK']},
        cond={'summit_extract':[['AsOfDate','=',fdate]]})
    if len(summit_swaps) > 0:
        # Add details to each swap based on their book and bond specific hedges
        summit_books_list = remove_columns(data_mt.fetch_files(\
            'summit_books_list'),['Version','TimeStamp'])
        summit_swaps = pd.merge(summit_swaps,summit_books_list[\
            ['Book','Entity']],left_on='BOOK',right_on='Book',how='left').\
            rename(columns={'DEAL_NBR':'DealNumber'})
    else:
        summit_swaps = data_mt.fetch_files('summit_swaps_list')
    
    tob_deal_nbr = summit_swaps.loc[summit_swaps.Entity == 'TOB','DealNumber']
    tob_swaps = ['SUMMIT_' + x for x in tob_deal_nbr.tolist()]
    point_tob = point_only.loc[point_only.Security.isin(tob_swaps),['Security']]
    if len(point_tob) > 0:
        point_tob.loc[:,'Agg_Value'] = 'TOB'
        point_tob.loc[:,'Agg_Ratio'] = 1.0
    
    
    ## Non Intrader instruments
    non_intrader.rename(index=str,columns={'CUSIP':'Security'},inplace=True)
    non_intrader = non_intrader.loc[non_intrader.Security.isin(bonds),:]
    if len(non_intrader) > 0:
        # Treat swaps that hedge a specific bond separately
        bond_specific = non_intrader.loc[\
            pd.notnull(non_intrader.Security_Mapped) & \
            (non_intrader.Security_Mapped != ''),:]
        if len(bond_specific) > 0:
            # Add swaps asset class and entity to the HFV dataframe directly
            for i,row in bond_specific.iterrows():
                swap = row['Security']
                bond = row['Security_Mapped']
                s_ac = hfv_AC.loc[hfv_AC.Security == bond,:].copy()
                s_entity = hfv_entity_agg.loc[\
                    hfv_entity_agg.Security == bond,:].copy()
                
                if len(s_ac) > 0:
                    s_ac.loc[:,'Security'] = swap
                    hfv_AC = hfv_AC.append(s_ac)
                
                if len(s_entity) > 0:
                    s_entity.loc[:,'Security'] = swap
                    hfv_entity_agg = hfv_entity_agg.append(s_entity)
        
        # AFS/HTM and BOLI
        non_intrader.loc[:,'AFS'] = non_intrader.apply(lambda x:\
            1.0 if x.AFS_HTM == 'AFS' else 0,axis=1)
        non_intrader.loc[:,'HTM'] = 1 - non_intrader.loc[:,'AFS']
        non_intrader.loc[:,'NotBOLI'] = 1.0
        non_intrader.loc[:,'BOLI'] = 0.0
        
        # Entity
        non_intrader_entity = non_intrader.loc[\
            pd.notnull(non_intrader.Entity) & (non_intrader.Entity != ''),\
            ['Security','Entity']]
        
        if len(non_intrader_entity) > 0:
            non_intrader_entity.rename(index=str,columns=\
                {'Entity':'Agg_Value'},inplace=True)
            non_intrader_entity.loc[:,'Agg_Ratio'] = 1.0
        else:
            non_intrader_entity = pd.DataFrame(data=None,columns=['Security',
                'Agg_Value','Agg_Ratio'])
        
    else:
        non_intrader = pd.DataFrame(data=None,columns=['Security',
            'AFS','HTM','AssetClass','NotBOLI','BOLI'])
        non_intrader_entity = pd.DataFrame(data=None,columns=['Security',
            'Agg_Value','Agg_Ratio'])
    
    
    ##### Aggregate results from all sources to get mapping information
    intent_cols = ['Security','AFS','HTM']
    AC_cols = ['Security','AssetClass']
    BOLI_cols = ['Security','NotBOLI','BOLI']
    intent = non_intrader.loc[:,intent_cols].\
        append(hfv_intent_agg.loc[:,intent_cols]).\
        append(point_hfv_intent.loc[:,intent_cols])
    asset_class = non_intrader.loc[pd.notnull(non_intrader.AssetClass),AC_cols].\
        append(hfv_AC.loc[:,AC_cols]).append(point_hfv_AC.loc[:,AC_cols])
    boli = non_intrader.loc[:,BOLI_cols].\
        append(hfv_boli_agg.loc[:,BOLI_cols]).\
        append(point_hfv_boli.loc[:,BOLI_cols])
    entities = hfv_entity_agg.append(point_tob).append(non_intrader_entity)
    
    ##### Format results for upload
    # Intent: Split between AFS and HTM
    AFS = intent.loc[intent.AFS > 0,:].rename(index=str,\
                                              columns={'AFS':'Agg_Ratio'})
    if len(AFS > 0):
        AFS.loc[:,'Agg_Key'] = 'Intent'
        AFS.loc[:,'Agg_Value'] = 'AFS'
    else:
        AFS = pd.DataFrame(data=None,columns=upload_cols)
        
    HTM = intent.loc[intent.HTM > 0,:].rename(index=str,\
                                              columns={'HTM':'Agg_Ratio'})
    if len(HTM > 0):
        HTM.loc[:,'Agg_Key'] = 'Intent'
        HTM.loc[:,'Agg_Value'] = 'HTM'
    else:
        HTM = pd.DataFrame(data=None,columns=upload_cols)
    # Asset Class
    asset_class.rename(index=str,columns={'AssetClass':'Agg_Value'},\
                       inplace=True)
    if len(asset_class > 0):
        asset_class.loc[:,'Agg_Key'] = 'AssetClass'
        asset_class.loc[:,'Agg_Ratio'] = 1.0
    else:
        asset_class = pd.DataFrame(data=None,columns=upload_cols)
    # BOLI
    BOLI_boli = boli.loc[boli.BOLI > 0,:].rename(index=str,\
                                              columns={'BOLI':'Agg_Ratio'})
    if len(BOLI_boli > 0):
        BOLI_boli.loc[:,'Agg_Key'] = 'BOLI'
        BOLI_boli.loc[:,'Agg_Value'] = 'BOLI'
    else:
        BOLI_boli = pd.DataFrame(data=None,columns=upload_cols)
        
    NotBOLI = boli.loc[boli.NotBOLI > 0,:].rename(index=str,\
                                              columns={'NotBOLI':'Agg_Ratio'})
    if len(NotBOLI > 0):
        NotBOLI.loc[:,'Agg_Key'] = 'BOLI'
        NotBOLI.loc[:,'Agg_Value'] = 'NotBOLI'
    else:
        NotBOLI = pd.DataFrame(data=None,columns=upload_cols)
    # Entities are formatted, just need the key name
    entities.loc[:,'Agg_Key'] = 'Entity'
    
    # All unmapped bonds in the list
    all_bonds = pd.DataFrame({'Security':bonds,'Agg_Ratio':1.0})
                              
    # If before June 2017 set IP as all the instruments
    if before_june2017:
        all_bonds.loc[:,'Agg_Key'],all_bonds.loc[:,'Agg_Value'] = 'Total','IP'
    else:
        # All bonds correspond to IP plus BOLI instruments
        all_bonds.loc[:,'Agg_Key'] = 'BOLI'
        all_bonds.loc[:,'Agg_Value'] = 'IPplusBOLI'
        # IP is equal to non-BOLI instruments
        NotBOLI.loc[:,'Agg_Key'],NotBOLI.loc[:,'Agg_Value'] = 'Total','IP'
        
    
    # Aggregate results in a single dataframe
    all_keys = asset_class.loc[:,upload_cols].\
        append(AFS.loc[:,upload_cols]).append(HTM.loc[:,upload_cols]).\
        append(BOLI_boli.loc[:,upload_cols]).\
        append(NotBOLI.loc[:,upload_cols]).\
        append(all_bonds.loc[:,upload_cols]).\
        append(entities.loc[:,upload_cols])
    
#    # Split results between missing and existing keys if necessary
#    if len(existing_keys) > 0:# and len(missing_keys) > 0:
#        # Only upload missing bonds for keys with data already
#        upload_existing = all_keys.loc[all_keys.Agg_Key.isin(existing_keys),:]
#        upload_existing = upload_existing.loc[\
#            upload_existing.Security.isin(unmapped)]
#        # For new keys add all results
#        upload_missing = all_keys.loc[all_keys.Agg_Key.isin(missing_keys),:]
#        upload_missing = upload_missing.loc[upload_missing.Security.isin(\
#            bonds),:]
#        upload_df = upload_existing.append(upload_missing)
#    else:
#        upload_df = all_keys.loc[all_keys.Security.isin(unmapped),:]
    
    # Only upload new mapping rows to database
    id_columns = mapped_bonds.columns.tolist()
    id_columns.remove('Agg_Ratio')
    id_columns.remove('AsOfDate')
    upload_df = compare_df(all_keys,mapped_bonds,id_columns)
    
    ##### Upload results to database
    if len(upload_df) > 0:
        upload_df.loc[:,'AsOfDate'] = fdate
    
        if upload:
            data_mt.upload_file('security_agg_map',\
                df = upload_df)
    else:
        upload_df = pd.DataFrame(data=None,\
            columns=['AsOfDate'] + upload_cols)
    
    ##### Output results
    if ret_result:
        return upload_df
    return
###############################################################################
def get_bond_level_sensitivities_POINT(fdate,IP_mapped,
                                       POINT_HFV_merged_mapped=None):
    """ fdate, POINT_HFV_merged_mapped: same as in the function 
    get_sensitivity_vector()
        IP_mapped: mapping from bond to SpreadIndex_ID
    This function is called by get_sensitivity_vector(). It computes and
    returns bond level sensitivities for a given date with POINT as the data 
    source.
    """
    # We need the HFV and POINT files merged as a source of data
    try:
        if POINT_HFV_merged_mapped is None:
            merged_reports = HFV_POINT_merge(fdate,upload=False,
                                             ret_result=True)
            # Add portfolio mapping to merged POINT and HFV reports
            merged_reports = merge_no_case(
                merged_reports,IP_mapped,'Security','Security','left')
        else:
            merged_reports = POINT_HFV_merged_mapped
    except:
        print('Error occurred in function get_sensitivity_vector')
        return ''
    
    # Aggregate results at bond level (instead of ticket)
    cols_agg_sum = ['QRM BPV','QRM Spread BPV','OASD Exposure',
        'KRDExposure 0_5yr','KRDExposure 02yr','KRDExposure 05yr',
        'KRDExposure 10yr','KRDExposure 20yr','KRDExposure 30yr',
        'QRM $ Convexity','OAC Exposure','Vega Exposure','Vega']
    cols_agg_first = ['Currency','POINT_Currency','QRM WAL','QRM OAD',
                      'SpreadIndex_ID']
    aggDict = dict.fromkeys(cols_agg_sum,'sum')
    firstDict = dict.fromkeys(cols_agg_first,'first')
    aggDict.update(firstDict)
    IP_data = merged_reports.groupby(['AsOfDate','Security']).agg(aggDict)
    IP_data.loc[IP_data.SpreadIndex_ID.isnull(),\
        'SpreadIndex_ID'] = 'NO_SPREAD'
    IP_data.reset_index(inplace=True)
    
    # Set currency for POINT only instruments
    IP_data.loc[IP_data.Currency.isnull(),'Currency'] = IP_data.loc[
        IP_data.Currency.isnull(),'POINT_Currency']
        
    ## Rename columns, then update exposures for non POINT instruments
    col_names = ['KRDExposure 0_5yr','KRDExposure 02yr','KRDExposure 05yr',
        'KRDExposure 10yr','KRDExposure 20yr','KRDExposure 30yr',
        'OASD Exposure','QRM $ Convexity','OAC Exposure','Vega']
    new_col_names = ['KRD_0.5y','KRD_02y','KRD_05y','KRD_10y','KRD_20y',
                     'KRD_30y','OASD','QRM OAC','OAC','QRM Vega']
    rename_dict = pd.Series(new_col_names,index=col_names).to_dict()
    IP_data.rename(index=str,columns=rename_dict,inplace=True)
    IP_data.rename(index=str,columns={'Vega Exposure':'Vega'},inplace=True)
    
    # Scale BPV and Spread BPV coming from HFV to $/1bp (originally to 
    # -100bps), then get IR BPV through the function, then copy spread BPV
    # from HFV
    IP_data[['QRM BPV','QRM Spread BPV']] =\
        -IP_data[['QRM BPV','QRM Spread BPV']]/100.
    IP_data = HFV_BPV_to_POINT_KRD(IP_data)
    IP_data.loc[IP_data.OASD.isnull(),'OASD'] = IP_data.loc[\
        IP_data.OASD.isnull(),'QRM Spread BPV']
    IP_data.loc[IP_data.OAC.isnull(),'OAC'] = IP_data.loc[\
        IP_data.OAC.isnull(),'QRM OAC']
    IP_data.loc[IP_data.Vega.isnull(),'Vega'] = IP_data.loc[\
        IP_data.Vega.isnull(),'QRM Vega']
        
    # Set OAC per tenor equal to total OAC divided by number of tenors
    OAC_cols = ['OAC_0_5y','OAC_02y','OAC_05y','OAC_10y','OAC_20y','OAC_30y']
    nb_OAC_cols = np.float(len(OAC_cols))
    for c in OAC_cols:
        IP_data.loc[:,c] = IP_data.loc[:,'OAC'] / nb_OAC_cols
    
    # Rename 0.5y tenor to KRD_0_5y and get upload/output dataframe
    IP_data.rename(index=str,columns={'KRD_0.5y':'KRD_0_5y'},inplace=True)
    security_sens_cols = ['AsOfDate','Security','Currency','KRD_0_5y',
            'KRD_02y','KRD_05y','KRD_10y','KRD_20y','KRD_30y','OASD','Vega'] +\
            OAC_cols
    security_sens = IP_data.loc[:,security_sens_cols]
    
    return security_sens
###############################################################################
def get_bond_level_sensitivities_QRM(fdate):
    """ fdate: same as in the function get_sensitivity_vector()
    This function is called by get_sensitivity_vector(). It computes and
    returns bond level sensitivities for a given date with QRM as the data 
    source.
    """
    ### Load QRM data
    # All sensitivities
    KRS = data_mt.fetch_files('qrm_outputs_krs',
        cond={'qrm_outputs_krs':[['AsOfDate','=',fdate]]})
    # Details for each bond (includes currency and QRM spread BPV)
    bond_details = data_mt.fetch_files(
        'qrm_outputs_IP_details',
        cond={'qrm_outputs_IP_details':[['AsOfDate','=',fdate]]})
    
    ### Only keep bonds in HFV file as well as Summit Swaps and Wall Street
    ## Load HFV file and QRM's no CUSIP bonds for list of bonds
    HFV = data_mt.fetch_files('HFV_daily_prelim',
       cond = {'HFV_daily_prelim':[['AsOfDate','=',fdate]]})
    ## Get non-intrader instruments
    qrm_no_cusip_info = data_mt.fetch_files(\
        'qrm_valuation_param_no_cusip')
    # Summit: split between receiving and paying legs
    summit_extract = data_mt.fetch_files('summit_extract',\
        columns={'summit_extract':['DEAL_NBR']},
        cond={'summit_extract':[['AsOfDate','=',fdate]]})
    summit_bonds = list(chain.from_iterable((x + '_P', x+'_R') for x in\
        summit_extract.DEAL_NBR.tolist()))
    # Wall Street
    # Get Wall Street report date (one month behind) and extract
    WSS_dates = np.unique(data_mt.fetch_files('WSS_extract',\
        columns={'WSS_extract':['AsOfDate']}))
    WSS_dates = [format_date(x) for x in WSS_dates if \
        np.abs((np.datetime64(pd.to_datetime(fdate))-x)/np.timedelta64(1,'D')-\
        30.) < 5]
    if len(WSS_dates) > 0:
        WSS_date = WSS_dates[0]
    else:
        WSS_date = '1/1/1900'
    WSS_extract = data_mt.fetch_files('WSS_extract',\
        columns={'WSS_extract':['DEAL_NUMBER']},
        cond={'WSS_extract':[['AsOfDate','=',WSS_date]]})
    # All non-intrader instruments for that date
    non_intrader = qrm_no_cusip_info.CUSIP.tolist() + \
        summit_bonds + WSS_extract.DEAL_NUMBER.tolist()
    
    # Get total list of bonds to keep and restrict inputs to it
    bonds_keep = HFV.Security.tolist() + non_intrader
    KRS = KRS.loc[KRS.CUSIP.isin(bonds_keep),:]
    bond_details = bond_details.loc[bond_details.CUSIP.isin(bonds_keep),:]
        
    ### Shock used in QRM computation for key rate sensitivities, in bps
    key_shock = 25.
    ### List of tenor columns to map to
    tenors_cols_yr = ['KRD_0_5y','KRD_02y','KRD_05y','KRD_10y',
                      'KRD_20y','KRD_30y']
    tenors_cols_yr_oac = ['OAC' + x[3:] for x in tenors_cols_yr]
    
    ### Rearrange key rate sensitivities from QRM to be per 1bp
    # Keep CUSIP and tenor sensitivities columns
    KRS_cols = KRS.columns.tolist()
    KRS_cols_tenors = [x[:-1] for x in KRS_cols if x[-1] == '+']
    OAC_tenors = []
    # Compute sensitivity per 1 basis point for each tenor using positive 
    # and negative shocks (25 bps shocks): duration and convexity
    for t in KRS_cols_tenors:
        tenor = t.split(' ')
        OAC_tenor = tenor[0] + ' OAC'
        OAC_tenors += [OAC_tenor]
        pos_shock_col = t + '+'
        neg_shock_col = t + '-'
        KRS.loc[:,t] = (KRS.loc[:,pos_shock_col]-KRS.loc[:,neg_shock_col])/\
            (2. * key_shock)
        KRS.loc[:,OAC_tenor] = (KRS.loc[:,pos_shock_col] + \
            KRS.loc[:,neg_shock_col]) / (np.power(key_shock/100.,2))
    # Keep sensitivities per tenor per 1 basis point shock
    QRM_sensitivities = KRS.loc[:,['CUSIP'] + KRS_cols_tenors + OAC_tenors]
    
    
    ### Remap tenors from QRM to our own list
    # Get list of final tenors in years, with map from year in number to
    # column name
    tenors_yr_dict = {}
    for x in tenors_cols_yr:
        tenors_yr_dict.update({np.float(x[4:-1].replace('_','.')):x})
    tenors_yr = np.sort(tenors_yr_dict.keys())
    
    # Get source tenors in months
    tenors_QRM = [x[:-5] for x in KRS_cols_tenors]
    
    # Get final tenors to which each QRM tenor is mapped to: each QRM tenor 
    # is split between the 2 closest final tenors
    QRM_to_final = {}
    for tenor in tenors_QRM:
        t = np.int(tenor)
        # If current tenor is below min or above max of final tenor then it
        # maps to that tenor
        if t <= min(tenors_yr * 12.):
            QRM_to_final.update({tenor:[[tenors_yr_dict[min(tenors_yr)],1.]]})
        elif t >= max(tenors_yr * 12.):
            QRM_to_final.update({tenor:[[tenors_yr_dict[max(tenors_yr)],1.]]})
        else:
            # QRM tenor is between the min and max of the final tenors,
            # we find which 2 tenors if falls between
            for i in range(len(tenors_yr)):
                t_months_i =  tenors_yr[i] * 12.
                t_months_i1 =  tenors_yr[i+1] * 12.
                t_yr_i = tenors_yr_dict[tenors_yr[i]]
                t_yr_i1 = tenors_yr_dict[tenors_yr[i+1]]
                if (t >= t_months_i) and (t < t_months_i1) :
                    ratio_i = (t_months_i1 - t) / (t_months_i1-t_months_i)
                    if ratio_i < 1:
                        QRM_to_final.update({tenor:[
                            [t_yr_i,ratio_i],[t_yr_i1,1-ratio_i]]})
                    else:
                        QRM_to_final.update({tenor:[[t_yr_i,ratio_i]]})
                    break
    
    # Create columns of final tenors and fill them
    for t_yr in tenors_cols_yr:
        QRM_sensitivities.loc[:,t_yr] = 0
    
    for t_yr in tenors_cols_yr_oac:
        QRM_sensitivities.loc[:,t_yr] = 0
    
    for t_QRM in KRS_cols_tenors:
        tenor = t_QRM[:-5]
        t_QRM_OAC = t_QRM.split(' ')[0] + ' OAC'
        target_cols = QRM_to_final[tenor]
        for target_t in target_cols:
            # Duration
            QRM_sensitivities.loc[:,target_t[0]] += \
                QRM_sensitivities.loc[:,t_QRM] * target_t[1]
            # Convexity
            target_t_oac = 'OAC' + target_t[0][3:]
            QRM_sensitivities.loc[:,target_t_oac] += \
                QRM_sensitivities.loc[:,t_QRM_OAC] * target_t[1]
    
    ### Keep final columns and add currency, spread bpv of each bond
    security_sens = QRM_sensitivities.loc[:,['CUSIP'] + tenors_cols_yr + \
        tenors_cols_yr_oac]
    security_sens = security_sens.merge(\
        bond_details.loc[:,['CUSIP','Currency','Spread BPV','Vega']],how='left')
    # Manually add currency of CTE swaps and WSS bonds
    security_sens.loc[security_sens.CUSIP == 'CTE_Swaps','Currency'] = 'USD'
    security_sens.loc[:,'Currency'] = security_sens.apply(lambda x: \
        x.CUSIP[-3:] if x.CUSIP[:4] == 'WSS_' else x.Currency,axis=1)
    # Set value of CTE swaps and WSS bonds spread BPV and Vega to 0
    security_sens.loc[pd.isnull(security_sens['Spread BPV']),'Spread BPV'] = 0
    security_sens.loc[pd.isnull(security_sens['Vega']),'Vega'] = 0
    
    # Rename some columns
    security_sens.rename(index=str,columns={'Spread BPV':'OASD',\
                                        'CUSIP':'Security'},inplace=True)
    
    # Modify OASD to be per +1bp shock
    security_sens.loc[:,'OASD'] = security_sens.loc[:,'OASD']/(-100.)
    
    return security_sens
###############################################################################
def get_portfolio_security_details(portfolio,date=None,bond_agg_map=None,
    portfolio_def=None):
    """ portfolio: name of portfolio as it appears in the Portfolio columns
    of the table Input_Portfolios_Definition
        date: as of date of the portfolio
        bond_agg_map: security_agg_map that gives the ratio of each bond in
    each possible portfolio breakdown
        portfolio_def: table portfolios_definition
        Either the date or the bond_agg_map need to be passed as input to this
    function. If both only the date is used.
        The function gets the ratio of each bond that makes up the portfolio
    passed as input using the security_agg_map and portfolio definition.
    """
    ### Format input date
    if date is not None:
        fdate = format_date(date)
        bond_agg_map = data_mt.fetch_files('security_agg_map',
            cond={'security_agg_map':[['AsOfDate','=',fdate]]})
    else:
        if bond_agg_map is None:
            print('Error in function get_portfolio_security_details:' +\
                'No bond_agg_map or date passed as input')
    
    # Get portfolio criterias from definition
    if portfolio_def is None:
        portfolio_def = data_mt.fetch_files(\
            'portfolios_definition')
    
    p_def = portfolio_def.loc[portfolio_def.Portfolio == portfolio,:]
    criterias = []
    for c in p_def.columns:
        if c.startswith('Criteria'):
            if p_def[c].values[0] is not None:
                criterias += [p_def[c].values[0]]
                
    ### Restrict list of bonds and compute ratios based on criterias
    # Using the first criteria
    p_map = bond_agg_map.copy()
    p_bonds = (p_map.Agg_Key == criterias[0].split('|')[0]) & \
        (p_map.Agg_Value == criterias[0].split('|')[1])
    p_map = p_map.loc[p_bonds,:]
    # Restricting further if there are other criterias
    for c in criterias[1:]:
        c_bonds = (bond_agg_map.Agg_Key == c.split('|')[0]) & \
            (bond_agg_map.Agg_Value == c.split('|')[1])
        c_map = bond_agg_map.loc[c_bonds,['Security','Agg_Ratio']].copy()
        c_map.rename(index=str,columns={'Agg_Ratio':'C_Ratio'},inplace=True)
        p_map = p_map.loc[p_map.Security.isin(c_map.Security.tolist()),:]
        p_map = pd.merge(p_map,c_map,on='Security',how='left')
        p_map.loc[:,'Agg_Ratio'] = p_map.loc[:,'Agg_Ratio'] * \
            p_map.loc[:,'C_Ratio']
        p_map = remove_columns(p_map,['C_Ratio'])
    
    return p_map.loc[:,['Security','Agg_Ratio']]
###############################################################################
def get_sensitivity_vector(date,level='portfolio',portfolio='IP',source='QRM',
                           POINT_HFV_merged_mapped=None,upload=False,
                           compute_subportfolios=True):
    """ date: as of date of the portfolio
        level: security or portfolio. Whether we want to get CUSIP level 
        sensitivities or a portfolio's aggregated sensitivities
        portfolio: which portfolio we want to get sensitivities for (whether 
    level is equal to portfolio or security)
        source: QRM or POINT. Whether sensitivities are sourced from POINT (as 
    much as possible) or QRM.
        POINT_HFV_merged_mapped: POINT report and HFV file merged, with CSIM 
    SpreadIndexID. It is optional, and will be passed by the function calling
    the stress calculator (when the security sensitivities are supposed to be 
    computed for the first time)
        upload: whether to upload results to Access database (if the 
    sensitivities for that portfolio and date cannot be found in the first 
    place)
        compute_subportfolios: whether to call function to compute subportfolio
    sensitivities used for VaR. This allows us not to call the subportfolio
    function when this function is called from the subportfolio function.
        Get the sensitivities for a given portfolio for some date. If the  
    results are already in the database we get those. Otherwise we fetch the 
    HFV and POINT (or QRM) report for a specific date, merge them, add the 
    CSIM mapping, and format output.
    """
    ### Format input date
    fdate = format_date(date)
    
    ### List of possible portfolios and their definitions
    portfolio_def = data_mt.fetch_files('portfolios_definition')
    possible_port = portfolio_def.Portfolio.tolist()
    if portfolio not in possible_port:
        print('Portfolio passed as input is not valid: ' + np.str(portfolio))
        return
    
    ### Try to get portfolio sensitivities from database. See if it exists by
    ### checking the length of the result. If nothing we compute results
    conditions =  [['AsOfDate','=',fdate],['Source','=',source]]
    if level == 'portfolio':
        table = 'portfolio_sensitivities'
        portfolio_conditions = [['Portfolio','=',portfolio]]
    elif level == 'security':
        table = 'security_sensitivities'
        portfolio_conditions = []
    
    sensitivities = data_mt.fetch_files(
        table,cond={table:conditions+portfolio_conditions})
    # Check whether portfolio sensitivities exist if getting securities level
    # so we can compute and upload them
    if level == 'security':
        port_sens = data_mt.fetch_files(
            'portfolio_sensitivities',\
            cond={'portfolio_sensitivities':conditions + \
            [['Portfolio','=',portfolio]]})
    
    # If we tried to get portfolio sensitivities but they don't exist yet, we 
    # need to check whether bond level sensitivities already exist
    if (level == 'portfolio') & (len(sensitivities) < 1):
        security_sens = data_mt.fetch_files(
            'security_sensitivities',cond={'security_sensitivities':\
            [['AsOfDate','=',fdate],['Source','=',source]]})
    else:
        security_sens = sensitivities
    
    ### If nothing was returned, or bond level sensitivities exist but not
    # portfolio level ones, then compute sensitivities with chosen source 
    if len(sensitivities) < 1 or (level == 'security' and len(port_sens) < 1):
        # Get portfolio spread mapping, used no matter what source is used
        try:
            # Get instruments mapping to credit spread indices
            IP_mapped = data_mt.fetch_files(
                'IP_mapped',cond={'IP_mapped':[['AsOfDate','=',fdate]]},
                columns={'IP_mapped':['Security','SpreadIndex_ID']})
        except:
            print('Could not get portfolio mapping for date: ' + fdate)
            return
        # Check whether security level sensitivities need to be computed
        if len(security_sens) < 1:
            ### Get bond level sensitivities depending on the source
            ## POINT
            if source == 'POINT':
                security_sens = get_bond_level_sensitivities_POINT(fdate,\
                    IP_mapped,POINT_HFV_merged_mapped)
                
                if len(security_sens) < 1:
                    if level == 'portfolio':
                        print('Could not get portfolio sensitivity for ' +\
                            'portfolio: ' + portfolio + ' for ' + fdate)
                    else:
                        print('Could not get security sensitivities for ' + \
                            'date: ' + fdate)
                    return
            ## QRM
            elif source == 'QRM':
                security_sens = get_bond_level_sensitivities_QRM(fdate)
                
                # Add map of non-intrader bonds in QRM to the mapped dataframe
                non_intrader = remove_columns(data_mt.\
                    fetch_files('non_intrader_bonds'),['Version','TimeStamp'])
                qrm_no_cusip_info = remove_columns(data_mt.\
                    fetch_files('qrm_valuation_param_no_cusip'),\
                    ['Version','TimeStamp'])
                non_intrader = non_intrader.append(qrm_no_cusip_info)
                non_intrader.rename(index=str,
                    columns={'CUSIP':'Security'},inplace=True)
                IP_mapped = IP_mapped.append(non_intrader.loc[:,
                    ['Security','SpreadIndex_ID']])
            
            ### Upload bond level sensitivities            
            # Add date and portfolio columns
            security_sens.loc[:,'AsOfDate'] = fdate
            security_sens.loc[:,'Source'] = source
            
            # CUSIP level sensitivities are computed, we upload them
            if upload:
                data_mt.upload_file('security_sensitivities',
                                                df = security_sens)

        ### Call mapping function to map new bonds
        update_security_agg_map(fdate,security_sens.Security.tolist(),\
            ret_result=False,upload=True)
        bond_agg_map = data_mt.fetch_files('security_agg_map',
            cond={'security_agg_map':[['AsOfDate','=',fdate]]})
        
        ### Compute portfolio level sensitivities: IP and other portfolios,
        ### only updating those without sensitivities yet
        ## Get list of portfolios without sensitivities (with date and source)
        # List of portfolios with sensitivities already
        # The full list of portfolios is computed if we upload results
        if upload:
            portfolio_list = np.unique(data_mt.fetch_files(
                'portfolio_sensitivities',
                cond={'portfolio_sensitivities':conditions},
                columns={'portfolio_sensitivities':['Portfolio']}))
            # Back out portfolios we need to compute sensitivities for:
            # subportfolios are computed separately
            needed_port = [x for x in possible_port if \
                (x not in portfolio_list) and pd.notnull(x) and ('|' not in x)]
        else:
            needed_port = [portfolio]
        
        ## Add spread index mapping to bond sensitivities
        bond_data = pd.merge(security_sens,IP_mapped,left_on='Security',
                             right_on='Security',how='left')
        # Set SpreadIndex_ID to NO_SPREAD for missing instruments (those 
        # are swaps and Wall Street short dated government bonds)
        bond_data.loc[pd.isnull(bond_data.SpreadIndex_ID),\
            'SpreadIndex_ID'] = 'NO_SPREAD'
        
        ## Loop through portfolios
        # Get list of sensitivities columns to keep
        sens_cols = [x for x in bond_data.columns if x[:4] == 'KRD_' or \
             x[:4] == 'OAC_'] + ['OASD','Vega']
        for p in needed_port:	
            # Get list of bonds with their ratio in this portfolio
            port_bond_data = get_portfolio_security_details(p,\
                bond_agg_map=bond_agg_map)
            
            # Check there are bonds in the selected portfolio
            if len(port_bond_data) > 1:
                # Use ratio to scale sensitivities
                port_bond_data = merge_no_case(port_bond_data,bond_data,
                    'Security','Security','left')
                port_bond_data.fillna(value=np.nan,inplace=True)
                port_bond_data.loc[:,sens_cols] = port_bond_data.loc[:,\
                    sens_cols].multiply(port_bond_data.loc[:,'Agg_Ratio'],
                    axis='index')
                
                port_sensitivities = bond_to_portfolio_sensitivities(\
                    port_bond_data)
                # If there is no sensitivity, add a dummy row
                if len(port_sensitivities) < 1:
                    port_sensitivities = pd.DataFrame(index=[0],\
                        data={'RiskFactor':port_bond_data['SpreadIndex_ID'][0],\
                        'Sensitivity':0})
                
                # Add date and portfolio columns
                port_sensitivities.loc[:,'AsOfDate'] = fdate
                port_sensitivities.loc[:,'Portfolio'] = p
                port_sensitivities.loc[:,'Source'] = source
                
                if upload:
                    data_mt.upload_file('portfolio_sensitivities',
                        df = port_sensitivities)
        
        # Return bond or portfolio sensitivities depending on function input
        if level == 'security':
            sensitivities = security_sens
        elif level == 'portfolio':
            # If data was uploaded several portfolios were computed
            if upload:
                sensitivities = data_mt.fetch_files(\
                    'portfolio_sensitivities',
                    cond={table:conditions+portfolio_conditions})
            else:
                sensitivities = port_sensitivities
        
        # Compute subportfolio sensitivities if necessary
        if upload and compute_subportfolios:
            get_subportfolio_sensitivities(fdate,source)
                                            
    sensitivities = remove_columns(sensitivities,['Version','TimeStamp'])
    # Remove bonds mapped to NO_SPREAD and with No Match from sensitivities
    # in the portfolio since they do not bring risk
    if level == 'portfolio':
        sensitivities = sensitivities.loc[\
            -sensitivities.RiskFactor.isin(['No Match','NO_SPREAD']),:]
    
    if level == 'security':
        sensitivities.rename(index=str,inplace=True,
            columns={'KRD_0_5y':'KRD_0.5y','OAC_0_5y':'OAC_0.5y'})
    
    
    ## If getting sensitivities at the security level we filter the positions
    ## now based on the selected portfolio
    if level == 'security':
        sensitivities.fillna(value=np.nan,inplace=True)
        # Aggregate sensitivities per Security in case some bond appears
        # more than once
        sensitivities = sensitivities.groupby(\
            ['Source','AsOfDate','Security','Currency']).sum()
        sensitivities.reset_index(inplace=True)
        # Get list of bonds, with ratio, in chosen portfolio
        bonds = get_portfolio_security_details(portfolio,date=fdate)
        sensitivities = sensitivities.loc[sensitivities.Security.isin(\
            bonds.Security).tolist(),:]
        
        # Use ratio to scale sensitivities
        sensitivities = merge_no_case(sensitivities,bonds.loc[:,\
            ['Security','Agg_Ratio']],'Security','Security','left')
#        sens_cols = ['KRD_0.5y','KRD_02y','KRD_05y','KRD_10y','KRD_20y',\
#            'KRD_30y','OASD']
        sens_cols = [x for x in sensitivities.columns if x[:4] == 'KRD_' or \
             x[:4] == 'OAC_'] + ['OASD','Vega']
        sensitivities.loc[:,sens_cols] = sensitivities.loc[:,\
            sens_cols].multiply(sensitivities.loc[:,'Agg_Ratio'],\
            axis='index')
        sensitivities = remove_columns(sensitivities,['Agg_Ratio'])
    
    return sensitivities
###############################################################################
def HFV_POINT_merge(date,upload=False,ret_result=True,HFV_version=''):
    """
    date: as of date of the data
    upload: True or False. Selects whether the output of the function should
    be uploaded to the database. Default is False
    ret_result: True or False. Selects whether the function should return
    the dataframe of the merged HFV file with the POINT report. Default is True
    HFV_version: which version of the HFV file to use. This will be saved in 
    the database. The options are Final or Prelim (for the daily file), or
    Monthly if we are trying to run the function on the Monthly file (will
    have poor results). Default is Final, if nothing is passed and this is not 
    available the function fails.
    This function merges the HFV file and the POINT report. It uploads the
    data to the database if requested (default) to table POINT_hfv_merged and
    returns the dataframe if requested (default value)
    """
    # Format date
    fdate = format_date(date)

    # Check version of HFV passed as input is valid
    HFV_version_to_file = {'Final':'HFV_daily_final',
        'Prelim':'HFV_daily_prelim','Monthly':'HFV_monthly'}
    if HFV_version in HFV_version_to_file.keys():
        HFV_filename = HFV_version_to_file[HFV_version]
    elif HFV_version == '':
        # No version specified, try for final daily, final prelim otherwise
        HFV_final_dates = get_existing_dates('HFV_daily_final')
        if fdate in HFV_final_dates:
            HFV_filename = 'HFV_daily_final'
            HFV_version = 'Final'
        else:
            # No final version found, look for prelim version
            HFV_prelim_dates = get_existing_dates('HFV_daily_prelim')
            if fdate in HFV_prelim_dates:
                HFV_filename = 'HFV_daily_prelim'
                HFV_version = 'Prelim'
            else:
                print('No daily HFV file found, no HFV-POINT report merging')
    else:
        print 'Version of HFV not valid, no HFV-POINT report merging'
        return
    
    # Get HFV file
    HFV = data_mt.fetch_files(HFV_filename,
        cond = {HFV_filename:[['AsOfDate','=',fdate]]})
    
    # Check POINT report exists for that date, if not return HFV file only
    POINT_report_dates = get_existing_dates('POINT_report')
    if fdate not in POINT_report_dates:
        print('No POINT report for date ' + fdate + ', HFV file returned')
        return HFV
        
    # Fetch POINT report to merge
    POINT_report = data_mt.fetch_files('POINT_report',
        cond = {'POINT_report':[['AsOfDate','=',fdate]]})
    
    # Get other inputs needed for merging
    special_IDs_map,HFV_duplicates_map = data_mt.fetch_files(
        ['special_IDs_map','HFV_duplicates_map'])
    
    # Run merging function       
    merged_reports = PointExtract_HFV_merge.PointExtract_HFV_merge(
        HFV,POINT_report,special_IDs_map,HFV_duplicates_map,HFV_version)
    
    # Upload result and return it if inputs are True
    if upload:
        data_mt.upload_file('POINT_hfv_merged',df = merged_reports)
        
    if ret_result:
        return merged_reports
    return
###############################################################################
def upload_qrm_output_to_db(date,filename='',file_location=''):
    """ date: as of date of the valuation run
    filename: name of the file as it appears in the folder with extension
    (eg: 20170331_QRM_KRS.xlsm)
    file_loc: if uploading an Excel file, location of file to upload in Python
    format (eg: P:\\Remy\\HFV\\)
    This function takes the valuation file sent by the ALM team (and modified
    to have all the necessary data) and formats it to get it ready to be
    uploaded in the database.
    """
    # Format date
    fdate = format_date(date)
    
    ### Get data from OLAP report directly or Excel (if file preformatted)
    IP_details, KRS = QRM_valuation_parser.fetch_qrm_valuation_data(\
        filename,file_location)
#    qrm_no_cusip_info = data_mt.fetch_files(\
#        'qrm_valuation_param_no_cusip')
#    IP_details, KRS = QRM_valuation_parser.parse_qrm_valuation_excel_file(\
#        filename,file_location,qrm_no_cusip_info)
    
    ### Get list of instruments that are transformed in numbers and replace
    ### numbers with original CUSIPs
    # Get HFV file and use number_to_cusip and  function
    hfv_prelim = data_mt.fetch_files('HFV_daily_prelim',
        cond={'HFV_daily_prelim':[['AsOfDate','=',fdate]]})
    cusip_as_number = QRM_valuation_parser.number_to_cusip(\
        hfv_prelim.loc[:,'Security'].tolist())
    IP_details = QRM_valuation_parser.fix_cusip_number(IP_details,
        cusip_as_number,'CUSIP')
    KRS = QRM_valuation_parser.fix_cusip_number(KRS,cusip_as_number,'CUSIP')
    IP_details, KRS = QRM_valuation_parser.adjust_UST_from_QRM(\
        hfv_prelim,IP_details,KRS)
    
    ### Print list of instruments in HFV not in QRM
    hfv_bonds = np.unique(hfv_prelim.Security)
    qrm_bonds = np.unique(IP_details.CUSIP)
    hfv_not_in_qrm = [x for x in hfv_bonds if x not in qrm_bonds]
    if len(hfv_not_in_qrm) > 0:
        print('\n\tInstruments in HFV file not found in QRM extract: \n')
        print hfv_prelim.loc[hfv_prelim.Security.isin(hfv_not_in_qrm),\
            ['Security','SEC 1','SEC 2','SEC 3','Group',\
            'Par/Curr_Face USD_Equiv','Curr Intent','Fair_Value(USD_Equiv)',\
            'Purchase Date','Maturity']]
    
    ### Add as of date to dataframes
    IP_details.loc[:,'AsOfDate'] = fdate
    KRS.loc[:,'AsOfDate'] = fdate
    
    ### Upload to database
    # IP details
    try:
        data_mt.upload_file('qrm_outputs_IP_details',df=IP_details)
        print('qrm_outputs_IP_details was parsed\n')
    except Exception as e:
        print('\nqrm_outputs_IP_details : Error, not parsed\n')
        print(e)
    
    # Key rate shocks
    try:
        data_mt.upload_file('qrm_outputs_krs',df=KRS)
        print('qrm_outputs_krs was parsed\n')
    except:
        print('qrm_outputs_krs : Error, not parsed\n')
    
    return
###############################################################################
def upload_bbg_to_db(date,filename='',file_location=''):
    """ date: as of date for production run. Market data up to that point is 
    uploaded.
        filename: name of the file as it appears in the folder with extension
    (eg: BBG_Inputs.xlsx)
        file_loc: if uploading an Excel file, location of file to upload in 
    Python format (eg: P:\\Remy\\HFV\\)
        This function uploads to the table of the Bloomberg market data and the
    Bloomberg bond information table by getting data from the Excel file.
    """
    # Format date
    fdate = format_date(date)
    
    ### Get Excel file
    os.chdir(file_location)
    pd_wb = pd.ExcelFile(filename)
    na_values = ['#N/A Invalid Security','#N/A N/A',
        '#N/A Field Not Applicable','#N/A Requesting Data...']
    
    ### Load market data first
    mkt_data_tab = 'MarketData'
    mkt_data = data_mt.excel_tab_to_df(pd_wb,filename,mkt_data_tab)
   
    # Delete first row as it is a dummy one used for data downloading only
    mkt_data = mkt_data.iloc[1:,:]
    
    # Clean invalid values and fill missing values
    for col in mkt_data.columns:
        mkt_data.loc[:,col] = mkt_data.loc[:,col].apply(lambda x:
            np.nan if (x in na_values) else x)
    mkt_data.fillna(method='ffill',inplace=True)
    
    # Keep data up to the date provided, and for dates in that same month
    last_date = dateutil.parser.parse(fdate)
    mkt_data_dates = [format_date(x) for x in mkt_data.Date if 
        (x.month == last_date.month) and (x.year == last_date.year)]
    mkt_data = mkt_data.loc[mkt_data.Date.isin(mkt_data_dates),:]
    
    # Rename dates column to match database name
    mkt_data.rename(index=str,columns={'Date':'AsOfDate'},inplace=True)
    
    # Upload to database
    try:
        data_mt.upload_file('market_data_rates',df=mkt_data)
        print('market_data_rates was parsed\n')
    except Exception as e:
        print('\nmarket_data_rates : Error, not parsed\n')
        print(e)
        
        
    ### Load bond data
    bond_data_tab = 'BondData'
    bond_data = data_mt.excel_tab_to_df(pd_wb,filename,bond_data_tab)
    
    # Delete first row as it is a dummy one
    bond_data = bond_data.loc[bond_data.Security != 'DummyCUSIP',:]
    
    # Clean invalid values and fill missing values
    for col in bond_data.columns:
        bond_data.loc[:,col] = bond_data.loc[:,col].apply(lambda x:
            '' if (x in na_values or pd.isnull(x)) else x)
    
    # Upload to database
    try:
        data_mt.upload_file('bbg_parameters',df=bond_data)
        print('bbg_parameters was parsed\n')
    except Exception as e:
        print('\nbbg_parameters : Error, not parsed\n')
        print(e)
    
    return
###############################################################################
def upload_CS_market_data(filename='',file_location='',date=None):
    """ filename: name of the file as it appears in the folder with extension
    (eg: CS Market Data 2018 01.xlsx)
        file_loc: location of Excel file to upload in Python format (eg:
    P:\\Remy\\HFV\\)
        date: as of date for production run. Market data up to that point is 
    uploaded if a date is provided.
        This function uploads Credit Spreads market data in the table 
    market_data_spreads_csim.
    """
    ### Get Excel file
    data = read_excel(filename,folder=file_location)
    data['AsOfDate'] = pd.to_datetime(data['AsOfDate'])
    
    ### Replace index name if needed
    if 'CMBS_SS_L_AAA_10Y' in data.columns.tolist():
        data.rename(columns={'CMBS_SS_L_AAA_10Y':'CMBS_SS_L__AAA_10Y'},\
                    inplace=True)
    ### Only keep new data
    existing_dates = data_mt.fetch_files('market_data_spreads_csim',
        columns = {'market_data_spreads_csim':['AsOfDate']})
    existing_last_date = max(existing_dates['AsOfDate'])
    
    # If a date is provided keep data up to the date, otherwise keep everything
    if date is None:
        date = max(data['AsOfDate'])
        
    keep_data = (data['AsOfDate'] > existing_last_date) & \
                (data['AsOfDate'] <= pd.to_datetime(format_date(date)))
    data = data.loc[keep_data]
    
    # Upload to database
    try:
        if len(data) > 0:
            data_mt.upload_file('market_data_spreads_csim',df=data)
            print('market_data_spreads_csim was parsed\n')
    except Exception as e:
        print('\nmarket_data_spreads_csim : Error, not parsed\n')
        print(e)
    return
###############################################################################
def upload_non_intrader_to_db(date,filetype='',filename='',file_location=''):
    """ date: as of date for the data being uploaded
        filetype: summit_extract or WSS_extract
        filename: name of the file as it appears in the folder with extension
    (eg: WSS_Instruments_20171229.xlsm,bos_summit_ircont_12292017.xlsx)
        file_location: if uploading an Excel file, location of file to upload  
    in Python format (eg: P:\\Portfolio VaR\\POINT Tools\\)
        This function uploads to the table of the Summit and Wall Street
    instruments extracts after adding the data's as of date to the dataframe.
    """
    # Format date
    fdate = format_date(date)
    
    ### Get Excel file
    os.chdir(file_location)
    pd_wb = pd.ExcelFile(filename)
    tabs = pd_wb.sheet_names
    
    # If Wall Street file check if there are several tabs
    if (filetype == 'WSS_extract') and (len(tabs) > 1):
        tab = 'query_output'
    else:
        tab = tabs[0]
        
    ### Load data and add as of date
    df = data_mt.excel_tab_to_df(pd_wb,filename,tab)
    df.loc[:,'AsOfDate'] = fdate
    
    ### Upload to database
    try:
        data_mt.upload_file(filetype,df=df)
        print(filetype + ' was parsed\n')
    except Exception as e:
        print('\n' + filetype + ' : Error, not parsed\n')
        print(e)
    
    return
###############################################################################
def mapping_changes(date,comp_date=''):
    """ date: as of date of the portfolio we are looking at
    comp_date: different as of date we want to do a comparison with. This is
    an optional input, if it is not provided we find the latest date before the 
    other input date
    This function takes 2 dates and looks at the portfolio mapping at that
    date, comparing it with the mapping from the other date if provided, or
    with the latest available older date. If changes have occurred between the
    2 dates the changes are printed.
    """
    ### Format input dates
    fdate = format_date(date)
    
    # Check whether we need to get the other date
    if comp_date == '':
        mapped_dates = get_existing_dates('IP_mapped')
        if fdate not in mapped_dates:
            print('Could not find date ' + date + ' in the database')
            return
        else:
            # Fetch last date before the date passed as input
            date_index = mapped_dates.index(fdate)
            fcomp_date = mapped_dates[date_index-1]
    else:
        fcomp_date = format_date(comp_date)
        
    # Fetch the dataframes containing the mapped portfolio for both dates
    IP_mapped = data_mt.fetch_files('IP_mapped',
        cond = {'IP_mapped':[['AsOfDate','=',fdate]]})
    IP_mapped_comp = data_mt.fetch_files('IP_mapped',
        cond = {'IP_mapped':[['AsOfDate','=',fcomp_date]]})
    IP_mapped = remove_columns(IP_mapped,['Version','TimeStamp'])
    
    # Merge both dates of mapped portfolios
    IP_mapped_comp = IP_mapped_comp[['Security','SpreadIndex_ID']].rename(
        index=str,columns={'SpreadIndex_ID':'SpreadIndex_ID_comp'})
    IP_mapped = merge_no_case(IP_mapped,IP_mapped_comp,
                              'Security','Security','left')
    
    # Compare indices at CUSIP level, taking into account the fact that new
    # securities and securities not here anymore will not match
    IP_mapped['Comp'] = IP_mapped.apply(lambda x: 'Same' if
        (x.SpreadIndex_ID == x.SpreadIndex_ID_comp) else ('New' if 
        pd.isnull(x.SpreadIndex_ID_comp) else ('Old' if 
        pd.isnull(x.SpreadIndex_ID) else 'Diff')),axis=1)
    
    # Return entire comparison
    #IP_mapped_diff = IP_mapped.query("Comp == 'Diff'")
    return IP_mapped
###############################################################################
def mapping_IP(date,subset='',upload=False,ret_result=True):
    """ date: date for which to map the investment portfolio's bonds
    subset: short list of bonds on which to run the mapping (used in the
    function mapping_IP_rerun_failed)
    upload: True or False. Selects whether the output of the function should
    be uploaded to the database. Default is False
    ret_result: True or False. Selects whether the function should return
    the dataframe of the mapped portfolio. Default is True
    The function fetches the HFV file, by default we try to get the final
    version but we take the preliminary version of the daily one if it is not
    available for that date (normal case for production)
    """
    ### Format input dates
    fdate = format_date(date)
    
    ### Check which version of HFV file to use
    HFV_final_dates = get_existing_dates('HFV_daily_final')
    HFV_prelim_dates = get_existing_dates('HFV_daily_prelim')
    
    if fdate in HFV_final_dates:
        HFV_version = 'HFV_daily_final'
    elif fdate in HFV_prelim_dates:
        HFV_version = 'HFV_daily_prelim'
    else:
        print('No HFV file found for date ' + fdate)
        return
    
    ### Load HFV file
    HFV_daily = data_mt.fetch_files(HFV_version,
                cond = {HFV_version:[['AsOfDate','=',fdate]]})
    
    # Restrict list of CUSIPs is subset is not empty
    if subset != '':
        HFV_daily = HFV_daily.loc[HFV_daily.Security.isin(subset),:]
    
    ### Get inputs
    # Collateral type: only one source for this information so we make sure we
    # have the information for every bond
    data_CCARteam = data_mt.fetch_files('data_CCARteam')
    has_collateral_info = (pd.notnull(data_CCARteam.CollateralType_Intex)) &\
        (data_CCARteam.CollateralType_Intex != '')
    collateral_type = data_CCARteam.loc[has_collateral_info,:].copy()
    
    ### Get date of most recently available FRY14Q, CCAR and HFV monthly 
    # reports as of the running date. We also get the date of the next 
    # available reports (future data) for reruns of historical scenarios
    for report in ['FRY14Q','HFV_monthly','data_CCARteam']:
        report_df = None
        latest_date = get_closest_date(fdate,report,before_after='before')
        
        # Download the latest date's data if that date exists
        if latest_date == fdate:
            report_df = data_mt.fetch_files(report,
                cond = {report:[['AsOfDate','=',latest_date]]})
        else:
            # Get most recent, and closest dates in the future, if they exist
            future_date = get_closest_date(fdate,report,before_after='after')
            if latest_date != '':
                report_df = data_mt.fetch_files(report,
                    cond = {report:[['AsOfDate','=',latest_date]]})
            if future_date != '':
                future_df = data_mt.fetch_files(report,
                    cond = {report:[['AsOfDate','=',future_date]]})
                if report_df is not None:
                    report_df.append(future_df)
                else:
                    report_df = future_df
        
        # Assign fetched data to correct report variable
        if report == 'FRY14Q':
            FRY14Q = report_df.copy()
        elif report == 'HFV_monthly':
            HFV_monthly = report_df.copy()
        elif report == 'data_CCARteam':
            data_CCARteam = report_df.copy()
            
    # Add to the CCAR team's data to have the collateral type
    data_CCARteam = data_CCARteam.append(collateral_type)
    data_CCARteam.drop_duplicates(subset='Security',keep='first',inplace=True)
    
    ### Load the rest of the data inputs
    ratings_map, FFELP_list, AC_detail_map, mapping_attributes, mapping_rules,\
        country_code_map, override_parameters, bbg_parameters = \
        data_mt.fetch_files(['ratings_map','FFELP_list',
        'AC_detail_map','mapping_attributes','mapping_rules',
        'country_code_map','override_parameters','bbg_parameters'],
        cond = {'override_parameters':[['AsOfDate','=',fdate]]})
    
    ### Call mapping function
    IP_mapped = CSIM_CusipToIndexMapping.CSIM_CusipToIndexMapping(
        ratings_map,FFELP_list,AC_detail_map,mapping_attributes,mapping_rules,
        country_code_map,HFV_monthly,HFV_daily,bbg_parameters,
        FRY14Q,data_CCARteam,override_parameters)
    
    
    ### If some instruments are not mapped look into older mapped portfolios
    # to override the instruments with that date's mapping. Dataframe 
    # ampped_override contains all instruments not mapped initially, and
    # either contains the override index, or no match, so that even unmapped
    # instruments are kept
    if len(IP_mapped.query("SpreadIndex_ID == 'No Match'")) > 0:
        mapped_override, IP_not_mapped = CSIM_CusipToIndexMapping.\
            CSIM_CusipToIndexMappingOverride(\
            IP_mapped.query("SpreadIndex_ID == 'No Match'").copy())
        IP_mapped = IP_mapped.query("SpreadIndex_ID != 'No Match'").\
            append(mapped_override)
    else:
        IP_not_mapped = ''
    
    ### Upload and return results if input variables are True
    if upload:
        data_mt.upload_file('IP_mapped',df=IP_mapped)
        if len(IP_not_mapped) > 0:
            print IP_not_mapped
            data_mt.upload_file('IP_not_mapped',df=IP_not_mapped)
           
    if ret_result:
        return IP_mapped
    return
###############################################################################
def mapping_IP_rerun_failed(date,upload=False,ret_result=True):
    """ date: date for which to remap the investment portfolio's failed bonds
    upload: True or False. Selects whether the output of the function should
    be uploaded to the database, and whether to delete the current entries for
    the bonds we are rerunning. Default is False
    ret_result: True or False. Selects whether the function should return
    the dataframe of the remapped bonds (including those that failed to find
    an index). Default is True
    The function runs the mapping_IP function for the bonds that found no
    matching index the first time around (because of wrong or missing inputs
    usually). It then deletes the entries for those in the results tables
    (both the final one and the no match tables) and replaces them with the 
    output of that table if the input variable upload is true.
    """
    ### Format input dates
    fdate = format_date(date)
    
    ### Get instruments still not mapped for that date
    # Includes bonds with override from other date
    not_mapped_all = data_mt.fetch_files('IP_not_mapped',
                cond = {'IP_not_mapped':[['AsOfDate','=',fdate]]})
    # Only bonds with no index
    not_mapped = not_mapped_all.loc[\
        not_mapped_all.SpreadIndex_ID == 'No Match',:]
    bonds_unmapped = not_mapped.Security.tolist()
    
    # If no bond is unmapped we exit the function
    if len(not_mapped_all) < 1:
        print('No unmapped bond for date: ' + fdate)
        return
    
    # Main mapping: only rows of bonds not mapped
    not_mapped_main = data_mt.fetch_files('IP_mapped',
                cond = {'IP_mapped':[['AsOfDate','=',fdate],
                                     ['Security','=',bonds_unmapped]]})
    
    # If the length of the 2 dataframes is different there is an issue
    if len(not_mapped_all) != len(not_mapped_main):
        print('No remapping occurred: the number of unmapped bonds in ' +\
            'IP_mapped does not match the number in IP_not_mapped')
        return
    
    # Run main mapping function on subset of unmapped bonds
    rerun_mapping = mapping_IP(fdate,subset=bonds_unmapped,upload=False,
                               ret_result=True)
    
    # Compare new mapping df: any instrument not mapped before with an index
    # now is deleted from the database to be updated
    new_mapped = rerun_mapping.loc[rerun_mapping.SpreadIndex_ID != 'No Match',
                                   'Security'].tolist()
    
    if (len(new_mapped) > 0) and upload:
        new_mapped_upload = rerun_mapping.loc[\
            rerun_mapping.SpreadIndex_ID != 'No Match',:]
        for security in new_mapped:
            # Delete data from IP_mapped and IP_not_mapped
            not_mapped_row = not_mapped.query("Security == '"+security+"'")
            main_mapped_row = not_mapped_main.query("Security == '"+security+"'")
            
            not_mapped_ts = format_date(np.str(not_mapped_row.TimeStamp.values[0]),
                                        str_format='%m/%d/%Y %I:%M:%S %p')
            mapped_ts = format_date(np.str(main_mapped_row.TimeStamp.values[0]),
                                        str_format='%m/%d/%Y %I:%M:%S %p')
            
            data_mt.delete_data('IP_not_mapped',cond = [
                ['AsOfDate',fdate],['Security',security],
                ['Version',not_mapped_row.Version.values[0]],
                ['TimeStamp',not_mapped_ts]])
            data_mt.delete_data('IP_mapped',cond = [
                ['AsOfDate',fdate],['Security',security],
                ['Version',main_mapped_row.Version.values[0]],
                ['TimeStamp',mapped_ts]])
        
        # Upload newly mapped instruments
        upload_to_db('IP_mapped',df = new_mapped_upload)
    
    # Print list of instruments still not mapped
    print rerun_mapping.loc[rerun_mapping.SpreadIndex_ID == 'No Match',:]
    
    if ret_result:
        return rerun_mapping
    return
###############################################################################
def get_QRM_full_reval(date,portfolio='IP'):
    """ date: portfolio as of date
        portfolio: portfolio of interest
        This function gets the change in market value for parallel rates shocks
    as computed by full revaluation in QRM, for the set of bonds in the 
    portfolio passed as input.
    """
    ### Format input date
    fdate = format_date(date)
    
    ### Load necessary inputs
    # QRM full reval data (part of KRS table)
    KRS = data_mt.fetch_files('qrm_outputs_krs',
        cond={'qrm_outputs_krs':[['AsOfDate','=',fdate]]})
    # Map from bond to portfolio
    port_bond_data = get_portfolio_security_details(portfolio,date=fdate)
    
    # Keep full reval columns only and rename them
    rename_cols = {}
    for c in KRS.columns:
        if 'UP ' in c:
            rename_cols.update({c:'+' + c[3:]})
        if 'DOWN ' in c:
            rename_cols.update({c:'-' + c[5:]})
    shock_cols = rename_cols.values()
    rename_cols.update({'CUSIP':'Security'})
    KRS.rename(columns=rename_cols,inplace=True)
    KRS = KRS.loc[:,rename_cols.values()]
    
    # Replace NAs with 0
    KRS.fillna(value=0,inplace=True)
    
    # Keep only bonds in scope and adjust amounts
    KRS = KRS.loc[KRS.Security.isin(port_bond_data.Security.tolist()),:]
    KRS = pd.merge(KRS,port_bond_data,on='Security',how='left')
    KRS.loc[:,shock_cols] = KRS.loc[:,shock_cols].multiply(\
            KRS.loc[:,'Agg_Ratio'],axis='index')
    
    # Remove unnecessary column
    KRS = remove_columns(KRS,['Agg_Ratio'])
    
    return KRS
###############################################################################
def stress_calc(date,flooring=True,portfolio='IP',sens_source='QRM',
                scenario_subset=None,upload=False,ret_result=True):
    """ date: date for which to compute stress PnL
        flooring: whether to floor shocks using the current date's market data.
    Default is True.
        portfolio: name of portfolio on which to compute stress PnL, eg: IP,
    IP_AFS, BOLI, IP_ABS_Auto, ...
        sens_source: whether sensitivities should be from POINT or QRM
        scenario_subset: subset of scenarios to run the calculator on. Either
    a list of scenario names (matching the database entries) or a string 
    containing the single scenario name. If None no subsetting done.
        upload: True or False. Selects whether the output of the function 
    should be uploaded to the database. Default is False
        ret_result: True or False. Selects whether the function should return
    the dataframes containing stress results. Default is True
        The function computes the stress PnL for the date passed as input for 
    the entire set of scenarios defined in the scenario database.
    The HFV file is aggregated at the CUSIP level, summing quantitative fields
    (columns defined in cols_agg_sum) and taking the first occurrence of
    qualitative fields (columns defined in cols_agg_first).
    The HFV file used is the final daily version if it is available, otherwise
    we revert to the preliminary daily version
    """
    ### Format input date
    fdate = format_date(date)
    
    ### Get HFV and POINT merged files
    merged_reports = HFV_POINT_merge(fdate,upload=False,ret_result=True)
    
    ### Fetch market data
    market_data_rates = data_mt.fetch_files('market_data_rates',
        cond = {'market_data_rates':[['AsOfDate','=',fdate]]})
    
    ### Get instruments mapping to credit spread indices
    IP_mapped = data_mt.fetch_files(
        'IP_mapped',cond={'IP_mapped':[['AsOfDate','=',fdate]]},
        columns={'IP_mapped':['Security','SpreadIndex_ID']})
        
    ### Get scenario shocks and sensitivities
    scenario_shocks,scenario_sens = data_mt.fetch_files([
        'scenario_shocks','scenario_sensitivities'])
    
    ### Get sovereign bonds and their shocks for credit spread risk metric
    sov_bonds = data_mt.fetch_files(
        'IP_mapped',cond={'IP_mapped':[['AsOfDate','=',fdate],
        ['SEC1_FRB_Category','=','Sovereign Bond']]})
    sov_bonds_csrm_shocks = data_mt.fetch_files(\
        'cs_risk_metric_sov_shocks')
    
    
    ### Add portfolio mapping to merged POINT and HFV reports
    merged_reports = merge_no_case(\
                         merged_reports,IP_mapped,'Security','Security','left')
    
    ### Get sensitivities at security level and add index mapping 
    IP_stress_data = get_sensitivity_vector(fdate,level='security',
        portfolio=portfolio,source=sens_source,
        POINT_HFV_merged_mapped=merged_reports,upload=True)
    IP_stress_data = merge_no_case(IP_stress_data,
        merged_reports.loc[:,['Security','SpreadIndex_ID']],
        'Security','Security','left')
    IP_stress_data.loc[IP_stress_data.SpreadIndex_ID.isnull(),\
        'SpreadIndex_ID'] = 'NO_SPREAD'
    # Add full reval data
    if sens_source == 'QRM':
        IP_full_reval = get_QRM_full_reval(date,portfolio=portfolio)
        # Only keep full reval data if populated
        if np.abs(IP_full_reval['+400'].sum()) < 1:
            IP_full_reval = None
    else:
        IP_full_reval = None
    

    ### Run subset of all scenarios
    if scenario_subset is not None:
        if isinstance(scenario_subset,basestring):
            scenario_subset = [scenario_subset]
        scenario_shocks = scenario_shocks.loc[scenario_shocks.Scenario.isin(
            scenario_subset),:]
    
    ### Call calculator function: outputs CUSIP level stress results
    IP_stress_pnl = StressCalculator.StressCalculator(\
        IP_stress_data,scenario_shocks,scenario_sens,sov_bonds,\
        sov_bonds_csrm_shocks,market_data_rates,flooring,IP_full_reval)
    
    
    ##### Format results for upload: CSRM and all scenarios after
    ### Get Asset Class for each bond
    bond_map = data_mt.fetch_files('security_agg_map',
        cond={'security_agg_map':[['AsOfDate','=',fdate],
                                  ['Agg_Key','=','AssetClass']]})
    bond_map.rename(index=str,columns={'Agg_Value':'AssetClass'},inplace=True)
    
    ##  Adjust the asset class ratios of each bond for the specific portfolio
    p_bonds = get_portfolio_security_details(portfolio,date=fdate)
    p_bonds.rename(index=str,columns={'Agg_Ratio':'AggP_Ratio'},\
        inplace=True)
    bond_map = bond_map.loc[bond_map.Security.isin(p_bonds.Security.\
        tolist()),:]
    bond_map = pd.merge(bond_map,p_bonds,on='Security',how='left')
    bond_map.loc[:,'Agg_Ratio'] = bond_map.apply(lambda x:\
        x.Agg_Ratio * x.AggP_Ratio,axis=1)
    
    if sens_source == 'POINT':
        # Add Asset Class for each bond
        merged_reports = merge_no_case(merged_reports,bond_map.loc[:,\
            ['Security','AssetClass','Agg_Ratio']],'Security','Security',\
            'left')
        
        # Use Agg_Ratio to scale reporting metrics needed
        for c in ['Fair_Value(USD_Equiv)','Market Value']:
            merged_reports.loc[:,c] = merged_reports.loc[:,c] * \
                merged_reports.loc[:,'Agg_Ratio']
        
        # Keep columns needed for reporting (Market Value and Spread BPV)
        cols_old = ['Security','Fair_Value(USD_Equiv)','Market Value',\
                    'AssetClass']
        cols_new = ['Security','HFV_FV','POINT_FV','Partition']
        cols_dict = dict(zip(cols_old,cols_new))
        IP_data = merged_reports.loc[:,cols_old].copy()
        IP_data.rename(index=str,columns=cols_dict,inplace=True)
        
        # Aggregate results
        IP_data_agg = IP_data.groupby(['Security','Partition']).sum()
        IP_data_agg.reset_index(inplace=True)
        IP_data_agg.loc[:,'MarketValue'] = IP_data_agg.apply(lambda x:
            x.POINT_FV if pd.notnull(x.POINT_FV) else x.HFV_FV,axis=1)
        IP_data_agg = merge_no_case(IP_data_agg,IP_stress_data.loc[:,\
            ['Security','OASD']],'Security','Security','left')
        IP_data_agg.rename(index=str,columns={'OASD':'OASD_Exposure'},\
            inplace=True)
        
        # Keep rows with non null market value
        IP_data_agg = IP_data_agg.loc[pd.notnull(IP_data_agg.MarketValue),:]
        
    elif sens_source == 'QRM':
        # Get Spread BPV and Market Value from QRM details
        QRM_details = data_mt.fetch_files('qrm_outputs_IP_details',
            cond={'qrm_outputs_IP_details':[['AsOfDate','=',fdate]]})
        # Keep only instruments with an asset class
        IP_data = merge_no_case(bond_map,QRM_details,'Security','CUSIP','left')
        IP_data.loc[:,'Market Value'] = IP_data.loc[:,'Market Value'] * \
            IP_data.loc[:,'Agg_Ratio']
        
        # Delete instruments without an asset class: extra QRM information not
        # part of the Investment Portfolio (or should be excluded, like BOLI)
        IP_data = IP_data.loc[pd.notnull(IP_data.AssetClass),:]
        cols_old = ['Security','Market Value','AssetClass']
        cols_new = ['Security','MarketValue','Partition']
        cols_dict = dict(zip(cols_old,cols_new))
        IP_data.rename(index=str,columns=cols_dict,inplace=True)
        IP_data_agg = IP_data.groupby(['Security','Partition']).sum()
        IP_data_agg.reset_index(inplace=True)
        IP_data_agg = merge_no_case(IP_data_agg,IP_stress_data.loc[:,\
            ['Security','OASD']],'Security','Security','left')
        IP_data_agg.rename(index=str,columns={'OASD':'OASD_Exposure'},
                           inplace=True)
    
    ### Keep CSRM stress PnL only, add partitions, prepare df for upload
    agg_map = get_asset_class_maps(fdate)
    agg_map = agg_map.loc[agg_map.Security.isin(IP_stress_pnl.Security.tolist()),:]
    # Keep CSRM data only and add security details
    CSRM = IP_stress_pnl.loc[IP_stress_pnl.Scenario == 'CreditSpreadRiskMetric',
                             ['Security','CreditSpreads']].copy()
    CSRM = merge_no_case(IP_data_agg[['Security','MarketValue',\
        'OASD_Exposure']],CSRM,'Security','Security','left')
    # Loop through asset class breakdowns
    CSRM_agg_all = pd.DataFrame(\
        columns=['Partition','CSRM','OASD_Exposure','MarketValue'])
    for l in np.unique(agg_map.LevelName):
        l_map = agg_map.loc[agg_map.LevelName == l,['Security','AssetClass']]
        CSRM_l = merge_no_case(CSRM,l_map,'Security','Security','left')
        CSRM_agg = CSRM_l.groupby('AssetClass').sum().reset_index().rename(\
            columns={'CreditSpreads':'CSRM','AssetClass':'Partition'})
        CSRM_agg.loc[:,'Partition'] = l + '|' + CSRM_agg.loc[:,'Partition']
        CSRM_agg_all = CSRM_agg_all.append(CSRM_agg,ignore_index=True)
    
    # Add row for total portfolio
    CSRM_total = pd.DataFrame(CSRM.sum()).T.rename(columns={'CreditSpreads':'CSRM'})
    CSRM_total.loc[:,'Partition'] = 'Total'
    CSRM_agg_all = CSRM_agg_all.append(CSRM_total[CSRM_agg_all.columns])
    # Add run details
    CSRM_agg_all.loc[:,'AsOfDate'] = fdate
    CSRM_agg_all.loc[:,'SensitivitiesSource'] = sens_source
    CSRM_agg_all.loc[:,'Portfolio'] = portfolio
    
    
    ### All Stress Scenarios: Take Cusip level results and aggregate them
    # Rates component: one number per scenario per date
    agg_results_rates = IP_stress_pnl[['Rates','Scenario']].\
        groupby('Scenario').sum()
    agg_results_rates.reset_index(inplace=True)
    agg_results_rates.rename(index=str,columns={'Rates':'StressPnL'},\
                             inplace=True)
    agg_results_rates.loc[:,'RiskFactor'] = 'Rates'
    # Swap spreads component: one number per scenario per date
    agg_results_swap_spreads = IP_stress_pnl[['SwapSpreads','Scenario']].\
        groupby('Scenario').sum()
    agg_results_swap_spreads.reset_index(inplace=True)
    agg_results_swap_spreads.rename(index=str,\
        columns={'SwapSpreads':'StressPnL'},inplace=True)
    agg_results_swap_spreads.loc[:,'RiskFactor'] = 'SwapSpreads'
    # Credit Spreads: aggregate results a Spread Index level
    agg_results_spreads = IP_stress_pnl[['Scenario','SpreadIndex_ID',\
        'CreditSpreads']].groupby(['Scenario','SpreadIndex_ID']).sum()
    agg_results_spreads.reset_index(inplace=True)
    agg_results_spreads.rename(index=str,columns={'CreditSpreads':'StressPnL',\
        'SpreadIndex_ID':'RiskFactor'},inplace=True)
        
    ### Save first order rates results to estimate impact of "full reval"
    agg_results_rates_FO = IP_stress_pnl[['Rates_FirstOrder','Scenario']].\
        groupby('Scenario').sum()
    agg_results_rates_FO.reset_index(inplace=True)
    agg_results_rates_FO.rename(index=str,\
        columns={'Rates_FirstOrder':'StressPnL'},inplace=True)
    agg_results_rates_FO.loc[:,'RiskFactor'] = 'Rates_FO'
    agg_results_spreads = agg_results_spreads.append(agg_results_rates_FO)
    
    # Get all results in one dataframe, adding flooring variable
    upload_df = agg_results_spreads.append(agg_results_rates)
    upload_df = upload_df.append(agg_results_swap_spreads)
    upload_df.loc[:,'AsOfDate'] = fdate
    upload_df.loc[:,'SensitivitiesSource'] = sens_source
    upload_df.loc[:,'Portfolio'] = portfolio
    
    if flooring:
        upload_df.loc[:,'Floor'] = 'Floored'
    else:
        upload_df.loc[:,'Floor'] = 'NoFloor'
    
    
    ### Upload and return results if input variables are True        
    if upload:
        data_mt.upload_file('IP_stress_pnl',df=upload_df)
        data_mt.upload_file('IP_csrm',df=CSRM_agg_all)
           
    if ret_result:
        return upload_df, CSRM_agg
    return
###############################################################################
def date_source_pairs(sens_source,df_dates_sources):
    """ sens_source: QRM, POINT, QRM_POINT or POINT_QRM. Which sources of data
    must be kept.
    df_dates_sources: dataframe containing a column with the list of dates
    available in our dataframe (stress PnL) with the corresponding sources for
    each date (2 sources means 2 rows for the same date)
    This returns a dictionary containing the dates to keep as keys and the 
    corresponding data source based on the input sens_source.
    """
    date_source = {}
    main_source = sens_source.split('_')[0]
    for d in df_dates_sources.AsOfDate:
        nb_sources = len(df_dates_sources.loc[\
            df_dates_sources.AsOfDate == d,:])
        if nb_sources < 2:
            unique_source = df_dates_sources.loc[\
                df_dates_sources.AsOfDate == d,'SensitivitiesSource'].values[0]
            if unique_source in sens_source:
                date_source.update({d:unique_source})
        else:
            date_source.update({d:main_source})
    
    return date_source
###############################################################################
def output_reporting_data(filename='',filepath='',portfolio='IP',
                          sens_source='QRM_POINT'):
    """filename: name of Excel file to export to (improvement would be to 
    udpate and already existing file)
        filepath: folder in which to export the data, or in which the file 
    filename is located. If nothing is provided we export in the working 
    directory
        portfolio: name of portfolio we are fetching data for
        sens_source: QRM, POINT, QRM_POINT or POINT_QRM. When only one source
    is in the input then only data from that source is fetched. When the 2 
    sources are in the input then we take the source indicated first when both
    are available for the same date.
    This function exports the scenario PnL for all dates available at the CSIM
    index level in one tab, and the CSRM data for all dates available in a 
    different tab
    """
    
    ### All Stress scenarios results
    ip_stress = data_mt.fetch_files('IP_stress_pnl',
        cond={'IP_stress_pnl':[['Portfolio','=',portfolio]]})
    ip_stress = remove_columns(ip_stress,['Version','TimeStamp'])
    
    ## Keep data based on sensitivities source passed as input
    # Get list of dates, and each source available for that date
    stress_dates = ip_stress.groupby(['AsOfDate','SensitivitiesSource']).size()
    stress_dates = stress_dates.reset_index()
    ## Get source-date pairs needed based on sens_source value
    date_source = date_source_pairs(sens_source,stress_dates)
    # Keep stress data based on date-source pairs
    ip_stress_out = pd.DataFrame(data=None,columns=ip_stress.columns)
    for d in date_source.keys():
        ip_stress_out = ip_stress_out.append(\
            ip_stress.loc[(ip_stress.AsOfDate == d) & \
            (ip_stress.SensitivitiesSource == date_source[d]),:])
    
    
    ### Scenario sensitivities: add risk factor to risk type (IR, CS, swap spread)
    scenario_sens = data_mt.fetch_files('scenario_sensitivities')
    ip_stress_out = pd.merge(ip_stress_out,scenario_sens[['RiskFactor','RiskType']],
        left_on='RiskFactor',right_on='RiskFactor',how='left')
    
    # Everything not CS is marked as rates
    ip_stress_out.loc[pd.isnull(ip_stress_out.RiskType),'RiskType'] = 'IR'
    scenarios_agg = ip_stress_out.groupby(\
        ['AsOfDate','Floor','Scenario','RiskType']).sum()
    scenarios_agg.reset_index(inplace=True)
    
    
    ### Get Credit Spread Risk Metric separately
    csrm = data_mt.fetch_files('IP_csrm',
        cond={'IP_csrm':[['Portfolio','=',portfolio]]})
    
    ## Keep data based on sensitivities source passed as input
    # Get list of dates, and each source available for that date
    csrm_dates = csrm.groupby(['AsOfDate','SensitivitiesSource']).size()
    csrm_dates = csrm_dates.reset_index()
    ## Get source-date pairs needed based on sens_source value
    date_source = date_source_pairs(sens_source,csrm_dates)
    # Keep stress data based on date-source pairs
    csrm_out = pd.DataFrame(data=None,columns=csrm.columns)
    for d in date_source.keys():
        csrm_out = csrm_out.append(csrm.loc[(csrm.AsOfDate == d) & \
            (csrm.SensitivitiesSource == date_source[d]),:])
    
    csrm_out = remove_columns(csrm_out,\
        ['Version','TimeStamp','SensitivitiesSource'])
    
    
    ### Export results to new Excel file
    if filepath != '':
        os.chdir(filepath)
    
    if filename == '':
        portfolio = portfolio.replace('|','_')
        now_str = datetime.datetime.now().strftime('%Y-%m-%d %Hh%Mm%S')
        writer = pd.ExcelWriter(now_str + ' ' + portfolio + \
            ' Scenarios and CSRM.xlsx')
        scenarios_agg.to_excel(writer,"Scenarios")
        csrm_out.to_excel(writer,"CSRM")
        writer.save()
    return
###############################################################################
def new_instruments_list(date,export_excel=True,filepath=''):
    """ date: date of month for which we want to find new instruments in
    portfolio
    export_excel: True or False. Whether to export results in Excel file or not
    filepath: folder in which to write Excel file if export_excel is True
    Finds list of instruments that have been purchased in a given month and
    exports that list in Excel file (if selected) with the SEC categories for
    each. Result dataframe is also printed.
    """
    ### Format input date
    fdate = format_date(date)
    
    # No version specified, try for final daily, final prelim otherwise
    HFV_final_dates = get_existing_dates('HFV_daily_final')
    if fdate in HFV_final_dates:
        HFV_filename = 'HFV_daily_final'
    else:
        # No final version found, look for prelim version
        HFV_prelim_dates = get_existing_dates('HFV_daily_prelim')
        if fdate in HFV_prelim_dates:
            HFV_filename = 'HFV_daily_prelim'
        else:
            print('No daily HFV file found for date ' + fdate)
    
    prev_date = get_closest_date(fdate,HFV_filename,before_after='before_strict')
    if prev_date == '':
        if HFV_filename == 'HFV_daily_final':
            HFV_filename_prev = 'HFV_daily_prelim'
            prev_date = get_closest_date(fdate,HFV_filename_prev,
                                         before_after='before_strict')
        elif HFV_filename == 'HFV_daily_prelim':
            HFV_filename_prev = 'HFV_daily_final'
            prev_date = get_closest_date(fdate,HFV_filename_prev,
                                         before_after='before_strict')
        
        if prev_date == '':
            print('No previous month file found, no new instruments list')
    else:
        HFV_filename_prev = HFV_filename
    
    HFV_current = data_mt.fetch_files(HFV_filename,
        cond={HFV_filename:[['AsOfDate','=',fdate]]})
    HFV_previous = data_mt.fetch_files(HFV_filename_prev,
        cond={HFV_filename_prev:[['AsOfDate','=',prev_date]]})
                                    
    
    bonds_current = pd.unique(HFV_current.Security)
    bonds_previous = pd.unique(HFV_previous.Security)
    
    new_bonds_list = set(bonds_current)-set(bonds_previous)
    new_bonds = HFV_current.loc[HFV_current.Security.isin(new_bonds_list),
                                ['Security','SEC 1','SEC 2','SEC 3']]
    new_bonds.drop_duplicates(subset='Security',keep='first',inplace=True)
    new_bonds.sort_values(by=['SEC 1','Security'],inplace=True)
    
    if filepath != '':
        os.chdir(filepath)
    
    print(new_bonds)
    
    if export_excel:
        fdate = format_date(date,str_format="%m-%d-%Y")
        now_str = datetime.datetime.now().strftime('%Y-%m-%d %Hh%Mm%S')
        writer = pd.ExcelWriter(now_str +' - '+fdate + ' New Instruments.xlsx')
        new_bonds.to_excel(writer,"NewBonds")
        writer.save()
    return
###############################################################################
def export_to_excel(df,filename,location=os.getcwd(),timestamp=True,
                    excel_format='xlsx',tab=''):
    """ df: single dataframe or a list of dataframes to paste. If several
    dataframes are passed the output format cannot be csv. If a list is passed
    as input the variable tab has to be of the same length if anything is 
    passed as input.
    timestamp: whether to include the time and date in the file's name
    Write dataframes df passed as input in Excel file specified by the 
    location, filename and tab passed.
    tab is an optional input. If nothing is passed the tabs will be called 
    Sheet1, Sheet2, etc.
    """
    if isinstance(df,list):
        if excel_format == 'csv' and len(df)>1:
            excel_format = 'xlsx'
            print('Format of output file changed to xlsx instead of csv')
        
        if tab != '':
            if len(tab) < len(df):
                for i in range(len(tab),len(df)):
                    tab.append('Sheet' + np.str(i+1))
    else:
        df = [df]
        if tab == '':
            tab = ['Sheet1']
        if isinstance(tab,basestring):
            tab = [tab]
    
    # In file name remove special characters
    for c in '<>:\"/\\|?*':
        filename = filename.replace(c,'_')
    
    os.chdir(location)
    
    if timestamp:
        now_str = datetime.datetime.now().strftime('%Y-%m-%d %Hh%Mm%S')
        output_filename = now_str + ' - ' + filename + '.' + excel_format
    else:
        output_filename = filename + '.' + excel_format
    
    if excel_format == 'xlsx':
        writer = pd.ExcelWriter(output_filename,date_format='mm/dd/yyyy')
        for i in range(len(df)):
            current_df = df[i]
            current_df.to_excel(writer,sheet_name=tab[i])
        writer.save()
        
    elif excel_format == 'csv':
        df[0].to_csv(output_filename)
    return
###############################################################################
def pnl_vs_sensitivities_changes(date,comp_date='',portfolio='IP',
                                 sens_source='QRM'):
    """ date: as of date of comparison
        comp_date: date for which to run the comparison. If nothing is passed 
    then we automatically fetch the previous date.
        portfolio: portfolio on which to fetch data
        sens_source: source of sensitivities to use (POINT or QRM)
    This function creates some of the results needed for ongoing monitoring.
    It compares the changes in rates and credit spreads PnL of each scenario
    with the changes in sensitivities for the same dates.
    """
    ### Format input date
    fdate = format_date(date)
    
    # Check whether we need to get the other date
    if comp_date == '':
        mapped_dates = get_existing_dates('IP_stress_pnl')
        if fdate not in mapped_dates:
            print('Could not find date ' + date + ' in the database')
            return
        else:
            # Fetch last date before the date passed as input
            date_index = mapped_dates.index(fdate)
            fcomp_date = mapped_dates[date_index-1]
    else:
        fcomp_date = format_date(comp_date)
    
    # Get stress results and sensitivities
    pnl = data_mt.fetch_files('IP_stress_pnl',
        cond={'IP_stress_pnl':[['AsOfDate','=',fdate],
                               ['SensitivitiesSource','=',sens_source],
                               ['Portfolio','=',portfolio]]})
    pnl_comp = data_mt.fetch_files('IP_stress_pnl',
        cond={'IP_stress_pnl':[['AsOfDate','=',fcomp_date],
                               ['SensitivitiesSource','=',sens_source],
                               ['Portfolio','=',portfolio]]})
#    sens = get_sensitivity_vector(fdate,level='portfolio',portfolio=portfolio,\
#        source=sens_source)
    sens = remove_columns(data_mt.fetch_files('portfolio_sensitivities',\
        cond={'portfolio_sensitivities':[['AsOfDate','=',fdate],\
        ['Source','=',sens_source],['Portfolio','=',portfolio]]}),\
        ['Version','TimeStamp'])
    sens = sens.loc[:,['RiskFactor','Sensitivity']]
#    sens_comp = get_sensitivity_vector(fcomp_date,level='portfolio',
#        portfolio=portfolio,source=sens_source)
    sens_comp = remove_columns(data_mt.fetch_files('portfolio_sensitivities',\
        cond={'portfolio_sensitivities':[['AsOfDate','=',fcomp_date],\
        ['Source','=',sens_source],['Portfolio','=',portfolio]]}),\
        ['Version','TimeStamp'])
    sens_comp = sens_comp.loc[:,['RiskFactor','Sensitivity']]
    
    # Get sensitivities definition
    sensitivities_def = data_mt.fetch_files(
        'scenario_sensitivities')
    sensitivities_def.loc[:,'RiskType'].replace(
        to_replace='SwapSpread',value='Rates',inplace=True)
    sens_type = sensitivities_def.loc[:,['RiskFactor','RiskType']]
    
    # Add CS or Rates type to each sensitivity
    pnl.loc[:,'RiskType'] = pnl['RiskFactor'].apply(lambda x:
        'Rates' if (x=='Rates' or x=='SwapSpreads') else 'CS')
    pnl_comp.loc[:,'RiskType'] = pnl_comp['RiskFactor'].apply(lambda x:
        'Rates' if (x=='Rates' or x=='SwapSpreads') else 'CS')
    sens = pd.merge(sens,sens_type,left_on='RiskFactor',
                   right_on='RiskFactor',how='left')
    sens_comp = pd.merge(sens_comp,sens_type,left_on='RiskFactor',
                   right_on='RiskFactor',how='left')
    
    # Aggregate at risk type level
    pnl_agg = pnl.groupby(['Floor','Scenario','RiskType']).sum()
    pnl_comp_agg = pnl_comp.groupby(['Floor','Scenario','RiskType']).sum()
    sens_agg = sens.groupby('RiskType').sum()
    sens_comp_agg = sens_comp.groupby('RiskType').sum()
    
    # Do comparison and compute changes
    pnl_comp_agg.rename(index=str,columns={'StressPnL':'StressPnL_prev'},
                        inplace=True)
    sens_comp_agg.rename(index=str,columns={'Sensitivity':'Sensitivity_prev'},
                        inplace=True)
    pnl_changes = pd.merge(pnl_agg,pnl_comp_agg,right_index=True,
                           left_index=True,how='left')
    pnl_changes.loc[:,'PnL_change'] = pnl_changes.apply(lambda x:
        x.StressPnL/x.StressPnL_prev-1 if np.abs(x.StressPnL_prev)>0
        else 0,axis=1)
    
    sens_change = pd.merge(sens_agg,sens_comp_agg,right_index=True,
                           left_index=True,how='left')
    sens_change.loc[:,'sens_change'] = sens_change.apply(lambda x:
        x.Sensitivity/x.Sensitivity_prev-1 if np.abs(x.Sensitivity_prev)>0
        else 0,axis=1)
    
    pnl_changes.reset_index(inplace=True)
    comparison = pd.merge(pnl_changes,sens_change,left_on='RiskType',
                           right_index=True,how='left')
    comparison.loc[:,'ChangeDifference'] = comparison.apply(lambda x:
        x.sens_change - x.PnL_change,axis=1)
    
    return comparison     
###############################################################################
def ongoing_monitoring_CSRM(date,sens_source='QRM',portfolio='IP',
                            prev_date=None):  
    """ date: portfolio as of date
        sens_source: source of sensitivities used
        portfolio: portfolio of interest
        prev_date: if a comparison date other than the previous month
        This function computes the ongoing monitoring specific to the credit
    spread risk metric by computing the metric and month-on-month changes
    at the index level. It also returns the reported Credit Spread Risk Metric
    tables for both dates.
    """
    # Format input date
    fdate = format_date(date)
    
    # Get sensitivities and previous month's sensitivities at index level
    try:
        sensitivities = data_mt.fetch_files('portfolio_sensitivities',\
                cond={'portfolio_sensitivities':[['AsOfDate','=',fdate],
                ['Source','=',sens_source]]})
        # Get previous month file
        if prev_date is None:
            prev_date = get_closest_date(fdate,'portfolio_sensitivities',\
                before_after='before_strict')
        sensitivities_prev = data_mt.fetch_files('portfolio_sensitivities',\
                cond={'portfolio_sensitivities':[['AsOfDate','=',prev_date],
                ['Source','=',sens_source]]})
    except Exception as e:
        print('\nCould not get sensitivities for input checks for date ' +\
              fdate + ' for ' + sens_source)
        print('Error: ')
        print(e)
        return
    
    # Keep data for portfolio of interest only
    sensitivities = sensitivities.query("Portfolio == '" + portfolio +"'")
    sensitivities_prev = sensitivities_prev.query(\
        "Portfolio == '" + portfolio +"'")
    
    # Only keep credit spread sensitivities
    sensitivities_def = data_mt.fetch_files('scenario_sensitivities')
    CS_indices = sensitivities_def.loc[sensitivities_def.RiskType == 'CS',\
                                       'RiskFactor'].tolist()
    sensitivities = sensitivities.loc[\
        sensitivities.RiskFactor.isin(CS_indices),:]
    sensitivities_prev = sensitivities_prev.loc[\
        sensitivities_prev.RiskFactor.isin(CS_indices),:]
    
    # Get Credit Spread Risk Metric shocks
    scenario_shocks = data_mt.fetch_files('scenario_shocks',cond={
        'scenario_shocks':[['Scenario','=','CreditSpreadRiskMetric']]})
    scenario_shocks = scenario_shocks.loc[\
        scenario_shocks.RiskFactor.isin(CS_indices),:]
        
    # Merge two sensitivities dataframes
    sensitivities.rename(columns={'Sensitivity':'Current_OASD'},inplace=True)
    sensitivities_prev.rename(columns={'Sensitivity':'Previous_OASD'},inplace=True)
    sensitivities = pd.merge(scenario_shocks[['RiskFactor','Shock']],\
                             sensitivities[['RiskFactor','Current_OASD']],\
                             on='RiskFactor',how='outer')
    comp = pd.merge(sensitivities[['RiskFactor','Current_OASD','Shock']],
                    sensitivities_prev[['RiskFactor','Previous_OASD']],\
                    on='RiskFactor',how='outer')
    comp.loc[pd.isnull(comp.Current_OASD),'Current_OASD'] = 0
    comp.loc[pd.isnull(comp.Previous_OASD),'Previous_OASD'] = 0
             
    # Compute CSRM for each index and date, then get differences
    comp.loc[:,'Current_CSRM'] = comp.apply(lambda x:\
            x.Current_OASD * x.Shock,axis=1)
    comp.loc[:,'Previous_CSRM'] = comp.apply(lambda x:\
            x.Previous_OASD * x.Shock,axis=1)
    comp.loc[:,'Change_CSRM'] = comp.loc[:,'Current_CSRM'] - \
            comp.loc[:,'Previous_CSRM']
    comp.loc[:,'Change_OASD'] = comp.loc[:,'Current_OASD'] - \
            comp.loc[:,'Previous_OASD']
            
    ### Actual CSRM reported
    csrm = remove_columns(data_mt.fetch_files('IP_csrm',cond={'IP_csrm':[\
        ['Portfolio','=',portfolio],['AsOfDate','=',[fdate,prev_date]],\
        ['SensitivitiesSource','=',sens_source]]}),['Version','TimeStamp'])
    
    return comp,csrm
###############################################################################
def scenarios_ongoing_monitoring(date,export_excel=True,filepath='',
                                 sens_source='QRM',portfolio='IP'):
    """ date: as of date for which to create ongoing monitoring data.
    export_excel: True or False. Whether to export the results to an Excel file
    or not. If not results are printed (for mapping changes only the bonds
    with a different index).
        filepath: where to export the Excel results. If it is empty then 
    results are exported in the current working directory.
        sens_source: POINT or QRM, source of sensitivties to use
        portfolio: portfolio of interest
        This calls the functions mapping_changes,  pnl_vs_sensitivities_changes 
    and ongoing_monitoring_CSRM, then it exports the results in a single Excel 
    file (different tabs).
    """
    pnl_sens_comp = pnl_vs_sensitivities_changes(date,sens_source=sens_source,\
        portfolio=portfolio)
    bond_mapping_comp = mapping_changes(date)
    csrm_comp,csrm = ongoing_monitoring_CSRM(date,sens_source=sens_source,\
        portfolio=portfolio)
    
    # Add portfolio as of date
    pnl_cols = pnl_sens_comp.columns.tolist()
    pnl_sens_comp.loc[:,'AsOfDate'] = format_date(date)
    pnl_sens_comp = pnl_sens_comp.loc[:,['AsOfDate'] + pnl_cols]
    
    csrm_comp_cols = csrm_comp.columns.tolist()
    csrm_comp.loc[:,'AsOfDate'] = format_date(date)
    csrm_comp = csrm_comp.loc[:,['AsOfDate'] + csrm_comp_cols]
    
    if filepath != '':
        location = filepath
    else:
        location = os.getcwd()
    
    if export_excel:
        filename = 'Scenarios Ongoing Monitoring ' +\
            format_date(date,str_format='%Y%m%d')
        export_to_excel([pnl_sens_comp,bond_mapping_comp,csrm_comp,csrm],\
            location=location,filename=filename,\
            tab=['PnlSensitivityComp','MappingChanges','CSRM_Comparison',
                 'CSRM_Reported'])
    else:
        print(pnl_sens_comp)
        print(bond_mapping_comp.query("Comp <> 'Same'"))
        print(csrm_comp)
    return
###############################################################################
def read_excel(filename,folder='',tab=''):
    """ filename: name of Excel file with extension
        folder: if in a different folder than the current one
        tab: name of tab to read data from if needed
        Reads an Excel file and returns the result in a dataframe.
    """
    # Get filetype from extension
    filetype = filename.split('.')[-1]
    
    # Get current directory and change folder if necessary
    original_folder = os.getcwd()
    if folder!= '':
        os.chdir(folder)
    
    # Read csv file directly
    if filetype == 'csv':
        df = pd.read_csv(filename)
    # Otherwise load workbook and parse chosen tab (if no tab is passed as
    # input we parse the first one)
    else:
        wb = pd.ExcelFile(filename)
        if tab == '':
            tab = wb.sheet_names[0]
        df = wb.parse(tab)
        
    os.chdir(original_folder)
    
    return df
###############################################################################
def compare_df(df,existing_df,id_columns):
    """ df: dataframe containing new data that has just been computed
        existing_df: dataframe similar to df that contains data computed in
    the past and already in the database
        id_columns: columns used to identify a row in the dataframes
        This function compares 2 dataframes to find which rows in df are not
    in existing_df by using the columns listed in columns to identify a row in
    each dataframe. Only the rows in df that do not exist in existing_df are
    returned.
    """
    # Reformat date columns in existing df to string if necessary
    if len(existing_df) > 0:
        for c in ['Date','AsOfDate']:
            if c in existing_df.columns.tolist():
                if not isinstance(existing_df[c][0],basestring):
                    existing_df.loc[:,c] = existing_df.apply(lambda x:\
                                   format_date(x[c]),axis=1)
    
    # Remove column value if in list of columns
    if 'Value' in id_columns:
        id_columns.remove('Value')
    
    # Replace null entries with empty strings
    for c in id_columns:
        df.loc[pd.isnull(df[c]),c] = ''
        existing_df.loc[pd.isnull(existing_df[c]),c] = ''
    
    # Create an ID column to uniquely identify a given row in each dataframe
    if len(df) > 0 and len(existing_df) > 0:
        df.loc[:,'ID'] = df.apply(lambda x: '|'.join(x[id_columns]),axis=1)
        existing_df.loc[:,'ID'] = existing_df.apply(lambda x:\
            '|'.join(x[id_columns]),axis=1)
            
        # Keep only rows in df not in existing_df
        new_df = df.loc[~df.ID.isin(existing_df.ID),:]
        del new_df['ID']
    else:
        new_df = df
        
    return new_df
###############################################################################
def values_assetclass_currency_breakdown(date):
    """ date: date for which to get values
        This function gets the fair values, book values and par values broken
    down per asset class and currency for each portfolio. It fills the table
    value_breakdown_ac_ccy in database VaR. It also fills the table
    risk_metrics_years in database VaR containing duration (rates and credit 
    spreads) and convexity in years.
    """
    ### Format inputs
    fdate = format_date(date)
    
    ### Download inputs
    port_breakdown_param = data_mt.fetch_files(\
            'VaR_portfolio_breakdown')
    non_intrader = remove_columns(data_mt.\
        fetch_files('non_intrader_bonds'),['Version','TimeStamp'])
    qrm_no_cusip_info = remove_columns(data_mt.\
        fetch_files('qrm_valuation_param_no_cusip'),\
        ['Version','TimeStamp'])
    bond_agg_map = data_mt.fetch_files('security_agg_map',
        cond={'security_agg_map':[['AsOfDate','=',fdate]]})
    sd_map = remove_columns(data_mt.fetch_files(\
        'sd_bond_mapping'),['Version','TimeStamp'])
    qrm_bond_details = data_mt.fetch_files(
        'qrm_outputs_IP_details',
        cond={'qrm_outputs_IP_details':[['AsOfDate','=',fdate]]})

    ### Get existing data for this date
    # Dollar values (Par, FV, ...)
    existing = remove_columns(data_mt.fetch_files(\
        'value_breakdown_ac_ccy',\
        cond = {'value_breakdown_ac_ccy':[['AsOfDate','=',fdate]]}),\
        ['Version','TimeStamp'])
    existing_cols = existing.columns.tolist() 
    
    ### Portfolios needed for VaR
    portfolio = np.unique(port_breakdown_param.Portfolio)
    
    ### Load HFV data: final if available, prelim otherwise
    hfv_cols = ['Security','Currency','Par/Curr_Face USD_Equiv',\
        'Fair_Value(USD_Equiv)','Adj/Mgmt Book_Value (USD_Equiv)',\
        'QRM Dirty Market Value','QRM BPV','QRM Spread BPV','QRM $ Convexity']
    cols_rename = ['Security','Currency','Par','FairValue','BookValue',\
        'FairValueDirty','BPV','SpreadBPV','Convexity']
    hfv = data_mt.fetch_files('HFV_daily_final',
        cond = {'HFV_daily_final':[['AsOfDate','=',fdate]]},
        columns= {'HFV_daily_final':hfv_cols})
    hfv_type = 'final'
    if len(hfv) < 1:
        hfv = data_mt.fetch_files('HFV_daily_prelim',
            cond = {'HFV_daily_prelim':[['AsOfDate','=',fdate]]},
            columns= {'HFV_daily_prelim':hfv_cols})
        hfv_type = 'prelim'
    hfv.columns = cols_rename
    hfv_agg = hfv.groupby(['Security','Currency']).sum().reset_index()
    
    # Get list of new instruments that have no sensitivities in HFV
    no_data_hfv = np.unique(hfv.loc[pd.isnull(hfv.SpreadBPV),'Security'])
    
    ### Non Intrader bonds: get data from QRM output, or POINT output
    non_intrader = non_intrader.append(qrm_no_cusip_info)
    non_intrader.rename(index=str,columns={'CUSIP':'Security'},inplace=True)
    qrm_cols = ['CUSIP','Face Amount','Book Value','Market Value','Currency',\
                'BPV','Spread BPV','Convexity','Accrued Interest']
    qrm_cols_rename = ['Security','Par','BookValue','FairValueDirty','Currency',\
                       'BPV','SpreadBPV','Convexity','AccruedInterest']
    
    # Get list of non intrader instruments and add new instruments to it
    qrm_details_bonds = np.concatenate(
            [np.unique(non_intrader.Security.tolist()),no_data_hfv])
    
    # If no book value (swap) set Par value to zero. Add dirty fair value
    if len(qrm_bond_details) > 0:
        qrm_bond_details = qrm_bond_details.loc[:,qrm_cols]
        qrm_bond_details.columns = qrm_cols_rename
        
        # Add Fair Value from QRM to HFV if using HFV prelim for dirty FV
        if hfv_type == 'prelim':
            hfv_agg = remove_columns(hfv_agg,'FairValueDirty')
            hfv_agg = pd.merge(hfv_agg,qrm_bond_details[\
                ['Security','FairValueDirty']],on='Security',how='left')
            
        qrm_bond_details = qrm_bond_details.loc[\
            qrm_bond_details.Security.isin(qrm_details_bonds),:]
        qrm_bond_details.loc[:,'Par'] = qrm_bond_details.apply(lambda x:\
            x.Par if np.abs(x.BookValue) > 1. else 0.,axis=1)
        qrm_bond_details.loc[:,'AccruedInterest'] = qrm_bond_details.apply(\
            lambda x: x.AccruedInterest if pd.notnull(x.AccruedInterest) \
            else 0.,axis=1)
        qrm_bond_details.loc[:,'FV_metrics'] = \
            qrm_bond_details.loc[:,'FairValueDirty']
        qrm_bond_details.loc[:,'FairValue'] = qrm_bond_details.loc[:,\
            'FairValueDirty'] - qrm_bond_details.loc[:,'AccruedInterest']
    else:
        # Get POINT output if no data from QRM details
        point_only_cols = ['Security','Position Amount (Base)','Market Value',\
            'Custom Portfolio 1']
        point_only_cols_rename = ['Security','Par','FairValue','AssetClass']
        point_only = data_mt.fetch_files('POINT_hfv_merged',\
            cond = {'POINT_hfv_merged':[['AsOfDate','=',fdate],\
            ['comment','=','POINT_only']]}).loc[:,point_only_cols]
        point_only.columns = point_only_cols_rename
        
        ## Get data from market structure to compute risk metrics in years
        point_report_cols = ['Security','OAD Exposure','OAC Exposure',\
            'OASD Exposure']
        point_report_cols_rename = ['Security','BPV','Convexity','SpreadBPV']
        POINT_report = data_mt.fetch_files('POINT_report',
            cond = {'POINT_report':[['AsOfDate','=',fdate]]}).\
            loc[:,point_report_cols]
        POINT_report = POINT_report.loc[POINT_report.Security.isin(\
            point_only.Security.tolist()),:]
        POINT_report.columns = point_report_cols_rename
        POINT_report = pd.merge(POINT_report,point_only,on='Security')
        
        # Adjust sensitivities to be on the same scale as QRM/HFV
        POINT_report.loc[:,'BPV'] = -100*POINT_report.loc[:,'BPV']
        POINT_report.loc[:,'SpreadBPV'] = -100*POINT_report.loc[:,'SpreadBPV']
        POINT_report.loc[:,'Convexity'] = 100*POINT_report.loc[:,'Convexity']
        
        # Treat swaps differently (potentially)
        xccy_swaps =  [x for x in POINT_report.Security.tolist() if \
            ('SUMMIT' in x) and (('_A' in x) or ('_B' in x))]
        single_ccy_swaps = [x for x in POINT_report.Security.tolist() if \
            ('SUMMIT' in x) and (x not in xccy_swaps)]
        POINT_report.loc[:,'FV_metrics'] = POINT_report.apply(lambda x:\
            x.FairValue if x.Security in xccy_swaps else \
            x.FairValue if x.Security in single_ccy_swaps else\
            x.FairValue,axis=1)
        
        ## Set Par amount of Summit bonds to zero
        summit_bonds = [x for x in point_only.Security.tolist() if \
            x[:7] == 'SUMMIT_']
        point_only.loc[point_only.Security.isin(summit_bonds),'Par'] = 0.
        point_only.loc[:,'FairValueDirty'] = point_only.loc[:,'FairValue']
        point_only.loc[:,'BookValue'] = point_only.apply(lambda x:\
            0. if np.abs(x.Par) < 1  else x.Par,axis=1)
        point_only.loc[:,'Currency'] = point_only.apply(lambda x:\
            'USD' if x.AssetClass == 'SPG' else \
            ('AUD' if x.Security[:2] == 'AU' else \
            ('SGD' if x.Security[:2] == 'SG'  else \
            ('KRW' if x.Security[:2] == 'KR'  else 'USD'))),axis=1)
        qrm_bond_details = point_only.loc[:,qrm_bond_details.columns.tolist()]
    
    ### For bonds with no sensitivities in HFV add data from outside source
    if (len(no_data_hfv) > 0) and (len(qrm_bond_details) > 0):
        hfv_replace_data = qrm_bond_details.loc[qrm_bond_details.Security.\
            isin(no_data_hfv),['Security','BPV','SpreadBPV','Convexity']]
        hfv_replace_data.columns = \
            ['Security','BPV_qrm','SpreadBPV_qrm','Convexity_qrm']
        hfv_agg = pd.merge(hfv_agg,hfv_replace_data,on='Security',how='left')
        hfv_agg.loc[:,'BPV'] = hfv_agg.apply(lambda x: \
            x.BPV_qrm if pd.isnull(x.BPV) else x.BPV,axis=1)
        hfv_agg.loc[:,'SpreadBPV'] = hfv_agg.apply(lambda x: \
            x.SpreadBPV_qrm if pd.isnull(x.SpreadBPV) else x.SpreadBPV,axis=1)
        hfv_agg.loc[:,'Convexity'] = hfv_agg.apply(lambda x: \
            x.Convexity_qrm if pd.isnull(x.Convexity) else x.Convexity,axis=1)
        hfv_agg = remove_columns(hfv_agg,\
                                 ['BPV_qrm','SpreadBPV_qrm','Convexity_qrm'])
        qrm_bond_details = qrm_bond_details.loc[\
            ~qrm_bond_details.Security.isin(no_data_hfv),:]
    
    
    ### Determine list of breakdowns
#    ac_bdown = np.unique(bond_agg_map.loc[bond_agg_map.Agg_Key == 'AssetClass',\
#        'Agg_Value'])
    sd_bdown_level1 = np.unique(sd_map.loc[sd_map.LevelName == 'Level1',\
        'AssetClass'])
    # Only keep level 2 if different from level 1
    sd_bdown_level2 = [x for x in \
        np.unique(sd_map.loc[sd_map.LevelName == 'Level2','AssetClass']) if \
        x not in sd_bdown_level1]
    
    hfv_agg.loc[:,'FV_metrics'] = hfv_agg.loc[:,'FairValueDirty']
    cols_rename += ['FV_metrics']    
    hfv_agg = hfv_agg.append(qrm_bond_details,ignore_index=True)
    
    # Only keep mapped bonds in aggregate dataframe
    bonds_list = np.unique(hfv_agg.Security)
    bond_agg_map = bond_agg_map.loc[bond_agg_map.Security.isin(bonds_list),:]
    sd_map = sd_map.loc[sd_map.Security.isin(bonds_list),:]
    
    ## Aggregate mapping of asset classes
    bond_agg_map_ac = bond_agg_map.loc[bond_agg_map.Agg_Key == 'AssetClass',:]
    bond_agg_map_ac = bond_agg_map_ac.rename(\
        columns={'Agg_Key':'LevelName','Agg_Value':'AssetClass'}).\
        loc[:,['Security','LevelName','AssetClass','Agg_Ratio']]
    sd_map.loc[:,'Agg_Ratio'] = 1.0
    agg_map = bond_agg_map_ac.append(sd_map,ignore_index=True)
    
    
    ### Loop through portfolios
    all_port = pd.DataFrame(columns=['Portfolio','Currency',\
        'FairValue','BookValue','Par','FairValueDirty'])
    for p in portfolio:
        # Bonds in this portfolio, with ratios of each bond
        p_bonds = get_portfolio_security_details(p,date=fdate,\
            bond_agg_map=bond_agg_map)
        
        # Restrict to bonds in this portfolio
        p_hfv = pd.merge(hfv_agg,p_bonds,on='Security',how='left')
        p_hfv = p_hfv.loc[pd.notnull(p_hfv.Agg_Ratio),:]
        p_hfv[cols_rename[2:]] = p_hfv[cols_rename[2:]].mul(\
            p_hfv.Agg_Ratio,axis=0)
        p_hfv = remove_columns(p_hfv,'Agg_Ratio')
        p_total_ccy = p_hfv.groupby('Currency').sum().reset_index()
        p_total_ccy.loc[:,'Portfolio'] = p
        p_tot = pd.DataFrame(p_hfv.loc[:,cols_rename[2:]].sum()).T
        p_tot.loc[:,'Portfolio'] = p
        p_tot.loc[:,'Currency'] = 'Total'
        p_total_ccy = p_total_ccy.append(p_tot,ignore_index=True)
        all_port = all_port.append(p_total_ccy,ignore_index=True)
        
        # Loop through breakdowns
        for l in ['AssetClass','Level1','Level2']:
            # Add asset class map
            l_agg_map = agg_map.loc[agg_map.LevelName == l,:]
            # For Level2 only keep asset classes not identical to level1
            if l == 'Level2':
                l_agg_map = l_agg_map.loc[l_agg_map.AssetClass.isin(\
                    sd_bdown_level2),:]
            
            p_l_hfv = pd.merge(p_hfv,l_agg_map,on='Security',how='left')
            p_l_hfv = p_l_hfv.loc[pd.notnull(p_l_hfv.AssetClass),:]
            if len(p_l_hfv) < 1:
                continue
            p_l_hfv[cols_rename[2:]] = p_l_hfv[cols_rename[2:]].mul(\
                p_l_hfv.Agg_Ratio,axis=0)
            p_l_hfv = remove_columns(p_l_hfv,['Agg_Ratio','LevelName'])
            
            # Compute total exposures, then per currency
            p_total = p_l_hfv.groupby('AssetClass').sum().reset_index()
            p_total.loc[:,'Currency'] = 'Total'
            p_ccy = p_l_hfv.groupby(['AssetClass','Currency']).sum().\
                reset_index()
            p_ccy = p_ccy.append(p_total)
            
            # Save results with other portfolios and breakdowns
            p_ccy.loc[:,'Portfolio'] = p + '|' + l + '|' + p_ccy['AssetClass']
            all_port = all_port.append(p_ccy)
    
    # Compute durations and convexity
    all_port.loc[:,'DurationYears'] = 100 * all_port.BPV / all_port.FV_metrics
    all_port.loc[:,'SpreadDurationYears'] = \
                100 * all_port.SpreadBPV / all_port.FV_metrics
    all_port.loc[:,'ConvexityYears'] = \
                100 * all_port.Convexity / all_port.FV_metrics
   
    # Format output and split dollar metrics from metrics in years
    all_port.loc[:,'AsOfDate'] = fdate
    all_port = remove_columns(all_port,['AssetClass','BPV','Convexity',\
        'FV_metrics','SpreadBPV','AccruedInterest'])
    df_long = pd.melt(all_port,id_vars=['AsOfDate','Portfolio','Currency'],\
        var_name='Metric',value_name='Value')
#    dollar_metrics = ['FairValue','FairValueDirty','Par','BookValue']
#    year_metrics = ['DurationYears','SpreadDurationYears','ConvexityYears']
    
    # Upload new results to database
    new_data = compare_df(df_long,existing,existing_cols)
    
    if len(new_data) > 0:
        data_mt.upload_file('value_breakdown_ac_ccy',df=new_data)
        
    return
###############################################################################
def compute_subportfolio_sensitivities(date,bond_sensitivities,IP_mapped,
    non_intrader,qrm_no_cusip_info,VaR_portfolio_breakdown,bond_agg_map,
    sd_map,portfolio_def,scenario_sens):
    """ date: portfolio date
        bond_sensitivities: sensitivities of our main portfolio at the bond
    level as returned by function get_sensitivity_vector for chosen date
        IP_mapped: table IP_mapped for chosen date
        non_intrader: table non_intrader_bonds
        qrm_no_cusip_info: table qrm_valuation_param_no_cusip
        VaR_portfolio_breakdown: table VaR_portfolio_breakdown
        bond_agg_map: table security_agg_map for chosen date
        sd_map: table sd_bond_mapping
        portfolio_def: table portfolios_definition
        scenario_sens: table scenario_sensitivities
        This function computes the sensitivities at the portfolio level of
    subsets of a given portfolio as defined in bond_breakdowns for a specific
    date. It also returns the possible portfolio breakdowns for each portfolio.
    """
    # Compute sensitivities for all portfolios
    portfolios = VaR_portfolio_breakdown.loc[:,'Portfolio'].tolist()
    
    ### Get mapping of bonds to Credit Spread index
    non_intrader = non_intrader.append(qrm_no_cusip_info)
    non_intrader.rename(index=str,columns={'CUSIP':'Security'},inplace=True)
    IP_mapped = IP_mapped.append(non_intrader.loc[:,\
        ['Security','SpreadIndex_ID']])
    
    # Add mapping to bond sensitivities, setting missing ones as no spread
    bond_sensitivities = merge_no_case(bond_sensitivities,IP_mapped,\
        'Security','Security','left')
    bond_sensitivities.loc[pd.isnull(bond_sensitivities.SpreadIndex_ID),\
        'SpreadIndex_ID'] = 'NO_SPREAD'
    
    
    ### Only keep mapped bonds in sensitivities dataframe
    bonds_list = np.unique(bond_sensitivities.Security)
    bond_agg_map = bond_agg_map.loc[bond_agg_map.Security.isin(bonds_list),:]
    sd_map = sd_map.loc[sd_map.Security.isin(bonds_list),:]
    
    
    ### Determine list of breakdowns
    ac_bdown = np.unique(bond_agg_map.loc[bond_agg_map.Agg_Key == 'AssetClass',\
        'Agg_Value'])
    sd_bdown_level1 = np.unique(sd_map.loc[sd_map.LevelName == 'Level1',\
        'AssetClass'])
    # Only keep level 2 if different from level 1
    sd_bdown_level2 = [x for x in \
        np.unique(sd_map.loc[sd_map.LevelName == 'Level2','AssetClass']) if \
        x not in sd_bdown_level1]
    
    ## Aggregate mapping of asset classes
    bond_agg_map_ac = bond_agg_map.loc[bond_agg_map.Agg_Key == 'AssetClass',:]
    bond_agg_map_ac = bond_agg_map_ac.rename(\
        columns={'Agg_Key':'LevelName','Agg_Value':'AssetClass'}).\
        loc[:,['Security','LevelName','AssetClass','Agg_Ratio']]
    sd_map.loc[:,'Agg_Ratio'] = 1.0
    agg_map = bond_agg_map_ac.append(sd_map,ignore_index=True)
    

    ### Rename 0.5y tenors and get list of sensitivity columns
    bond_sensitivities.rename(index=str,inplace=True,
            columns={'KRD_0_5y':'KRD_0.5y','OAC_0_5y':'OAC_0.5y'})
    sens_cols = [x for x in bond_sensitivities.columns if x[:4] == 'KRD_' or \
	 x[:4] == 'OAC_'] + ['OASD','Vega']
    
    ### Dataframe to save all results
    all_port = pd.DataFrame(columns=['Portfolio','RiskFactor','Sensitivity'])
    
    # Loop through portfolios
    for p in portfolios:
        port_bond_data = get_portfolio_security_details(p,\
            bond_agg_map=bond_agg_map,portfolio_def=portfolio_def)
        p_sens = merge_no_case(port_bond_data,bond_sensitivities,\
            'Security','Security','left')
        p_sens.fillna(value=0.,inplace=True)
        p_sens.loc[:,sens_cols] = p_sens.loc[:,sens_cols].multiply(\
            p_sens.loc[:,'Agg_Ratio'],axis='index')
            
        # Loop through breakdowns
        for l in ['AssetClass','Level1','Level2']:
            if l == 'AssetClass':
                asset_classes = ac_bdown
            elif l == 'Level1':
                asset_classes = sd_bdown_level1
            elif l == 'Level2':
                asset_classes = sd_bdown_level2
            
            l_agg_map = agg_map.loc[agg_map.LevelName == l,:]
            
            # Loop through asset classes in this breakdown
            for ac in asset_classes:
                # Get list of bonds in this asset class only, with ratio
                ac_agg_map = l_agg_map.loc[l_agg_map.AssetClass == ac,:]
                ac_sens = remove_columns(p_sens.loc[p_sens.Security.isin(\
                    ac_agg_map.Security.tolist()),:],'Agg_Ratio')
                ac_sens = pd.merge(ac_sens,ac_agg_map[['Security','Agg_Ratio']],
                                   on='Security',how='left')
        
                # Make sure some bonds in that portfolio and asset class exist
                if len(ac_sens) < 1:
                    continue
                
                ac_sens.loc[:,sens_cols] = ac_sens.loc[:,sens_cols].multiply(\
                    ac_sens.loc[:,'Agg_Ratio'],axis='index')
                subp_sens = bond_to_portfolio_sensitivities(ac_sens,\
                    scenario_sens)
                if len(subp_sens) > 0:
                    subp_sens.loc[:,'Portfolio'] = '|'.join([p,l,ac])
                    all_port = all_port.append(subp_sens,ignore_index=True)
    
    # Add as of date
    all_port.loc[:,'AsOfDate'] = date
    
    ### List portfolio breakdowns for this date
    port_breakdowns = pd.DataFrame(np.unique(all_port.Portfolio),\
        columns=['Component'])
    port_breakdowns.loc[:,'Portfolio'] = port_breakdowns.apply(lambda x:\
        x.Component.split('|')[0],axis=1)
    port_breakdowns.loc[:,'Breakdown'] = port_breakdowns.apply(lambda x:\
        x.Component.split('|')[1],axis=1)
    for p in portfolios:
        port_breakdowns = port_breakdowns.append(pd.DataFrame(\
            [p,'Total',p],index=['Portfolio','Breakdown','Component']).T,
            ignore_index=True)
    
    return all_port, port_breakdowns
###############################################################################
def get_subportfolio_sensitivities(dates,sens_source):
    """ dates: list of portfolio dates
        sens_source: source of sensitivities. QRM or POINT
        This function gets the sensitivities at the portfolio level of
    subsets of the VaR portfolios as defined in bond_breakdowns for a 
    specific date as well as the possible portfolio breakdowns for each 
    portfolio.
    """
    # Format input dates
    if isinstance(dates,basestring):
        dates = [dates]
    
    # Get list of portfolios to break down
    VaR_portfolio_breakdown = data_mt.fetch_files(\
            'VaR_portfolio_breakdown')
    portfolios = np.unique(VaR_portfolio_breakdown.Portfolio)
    
    for d in dates:
        fdate = format_date(d)
        
        # Get existing data
        existing_sens = data_mt.fetch_files('portfolio_sensitivities',\
            cond={'portfolio_sensitivities':[['AsOfDate','=',fdate],
            ['Source','=',sens_source]]})
        
        existing_port_bdown = data_mt.fetch_files('VaR_port_bdown_details',\
            cond={'VaR_port_bdown_details':[['AsOfDate','=',fdate],
            ['Source','=',sens_source]]})
        
        # Get list of existing portfolios and keep subportfolios
        existing_port = np.unique(existing_sens.Portfolio)
        sub_port = [x for x in existing_port if '|' in x]
        main_port = [x for x in existing_port if x not in sub_port]
        
        # Check main portfolios sensitivities exist, otherwise compute them
        missing_main = [x for x in portfolios if x not in main_port]
        if len(missing_main) > 0:
            existing_sens = get_sensitivity_vector(fdate,level='portfolio',\
                portfolio=missing_main[0],source=sens_source,upload=True,
                compute_subportfolios=False)
            existing_sens = data_mt.fetch_files('portfolio_sensitivities',\
                cond={'portfolio_sensitivities':[['AsOfDate','=',fdate],
                ['Source','=',sens_source]]})
        
        # Get list of main portfolios and breakdowns existing
        breakdowns = pd.DataFrame(columns=['Portfolio'],data=portfolios)
        for b in ['AssetClass','Level1','Level2']:
            breakdowns.loc[:,b] = 0
        for s in sub_port:
            main_port,b = s.split('|')[:2]
            breakdowns.loc[breakdowns.Portfolio == main_port,b] += 1
        
        # If any breakdown is missing we compute sensitivities
        missing_bdown = (breakdowns.iloc[:,1:] < 1.).any().any()
        
        if missing_bdown:
            # Call SD mapping function as its output will be used here
            sdf.map_bonds(d)
            
            # Get necessary inputs
            non_intrader = data_mt.fetch_files('non_intrader_bonds')
            qrm_no_cusip_info = data_mt.fetch_files(\
                'qrm_valuation_param_no_cusip')
            sd_map = data_mt.fetch_files('sd_bond_mapping')
            portfolio_def = data_mt.fetch_files('portfolios_definition')
            scenario_sens = data_mt.fetch_files('scenario_sensitivities')
            bond_sensitivities = data_mt.fetch_files('security_sensitivities',\
                cond={'security_sensitivities':[['AsOfDate','=',fdate],\
                ['Source','=',sens_source]]})
            IP_mapped = data_mt.fetch_files('IP_mapped',\
                cond={'IP_mapped':[['AsOfDate','=',fdate]]},\
                columns={'IP_mapped':['Security','SpreadIndex_ID']})
            bond_agg_map = data_mt.fetch_files('security_agg_map',\
                cond={'security_agg_map':[['AsOfDate','=',fdate]]})
        
            # Call calculating function
            sub_port_sens, port_bdown = compute_subportfolio_sensitivities(\
                fdate,bond_sensitivities,IP_mapped,non_intrader,\
                qrm_no_cusip_info,VaR_portfolio_breakdown,bond_agg_map,sd_map,\
                portfolio_def,scenario_sens)
            
            # Compare new results with existing database dataframe
            sub_port_sens.loc[:,'Source'] = sens_source
            sub_port_sens.loc[:,'AsOfDate'] = fdate
            port_bdown.loc[:,'Source'] = sens_source
            port_bdown.loc[:,'AsOfDate'] = fdate
            
            existing_sens.loc[:,'AsOfDate'] = fdate
            existing_port_bdown.loc[:,'AsOfDate'] = fdate
            
            new_sens = compare_df(sub_port_sens,existing_sens,\
                ['Source','Portfolio','AsOfDate','RiskFactor'])
            new_bdown = compare_df(port_bdown,existing_port_bdown,\
                ['Source','Portfolio','AsOfDate','Component','Breakdown'])
            
            # Upload results
            if len(new_sens) > 0:
                data_mt.upload_file('portfolio_sensitivities',df=new_sens)
            if len(new_bdown) > 0:
                data_mt.upload_file('VaR_port_bdown_details',df=new_bdown)
    
    return
###############################################################################
def compute_parallel_results_aggregation(date,portfolios=None):
    """ date: portfolio as of date of interest
        portfolios: list of portfolios of interest
        This function gets the results of the parallel scenario shocks computed
    in QRM and saved with the Key Rate Sensitivities. It aggregates the results
    for the Investment Portfolio according to the Asset Class and Levels 1 and 
    2 and saves the results in table qrm_parallel_results.
    """
    ### Format input data
    fdate = format_date(date)
    if portfolios is not None:
        if isinstance(portfolios,basestring):
            portfolios = [portfolios]
    else:
        exclude_portfolios = ['NotBOLI','IPplusBOLI']
        portfolio_def = data_mt.fetch_files('portfolios_definition')
        portfolios = [x for x in portfolio_def.Portfolio if \
            ('|' not in x) and (x not in exclude_portfolios)]
    
    ### Fetch QRM data for this date
    QRM_details = data_mt.fetch_files('qrm_outputs_IP_details',
            cond={'qrm_outputs_IP_details':[['AsOfDate','=',fdate]]})
    shocked_pnl = data_mt.fetch_files('qrm_outputs_krs',
        cond={'qrm_outputs_krs':[['AsOfDate','=',fdate]]})
    qrm_data = pd.merge(QRM_details,shocked_pnl,on='CUSIP',how='left')
    qrm_cols = ['Market Value','UP 200']
    qrm_cols_rename = ['FairValue','UP_200']
    qrm_data = qrm_data.rename(columns=dict(zip(qrm_cols + ['CUSIP'],\
        qrm_cols_rename + ['Security']))).loc[:,\
        qrm_cols_rename + ['Security','Currency']]
    
    ### Get asset class maps
    agg_map = get_asset_class_maps(fdate)
    
    ### Loop through portfolios
    all_port = pd.DataFrame(columns=['Portfolio','Currency'] + qrm_cols_rename)
    bond_agg_map = data_mt.fetch_files('security_agg_map',\
        cond={'security_agg_map':[['AsOfDate','=',fdate]]})
    for p in portfolios:
        try:
            # Bonds in this portfolio, with ratios of each bond
            p_bonds = get_portfolio_security_details(p,bond_agg_map=bond_agg_map,\
                date=fdate)
            
            # Restrict to bonds in this portfolio
            p_qrm = pd.merge(qrm_data,p_bonds,on='Security',how='left')
            p_qrm = p_qrm.loc[pd.notnull(p_qrm.Agg_Ratio),:]
            p_qrm[qrm_cols_rename] = p_qrm[qrm_cols_rename].mul(\
                p_qrm.Agg_Ratio,axis=0)
            p_qrm = remove_columns(p_qrm,'Agg_Ratio')
            
            # Aggregate per currency
            p_total_ccy = p_qrm.groupby('Currency').sum().reset_index()
            p_total_ccy.loc[:,'Portfolio'] = p
            p_tot = pd.DataFrame(p_qrm.loc[:,qrm_cols_rename].sum()).T
            p_tot.loc[:,'Portfolio'] = p
            p_tot.loc[:,'Currency'] = 'Total'
            p_tot.loc[:,'AssetClass'] = 'Total'
            p_total_ccy = p_total_ccy.append(p_tot,ignore_index=True)
            all_port = all_port.append(p_total_ccy,ignore_index=True)
            
            # Loop through breakdowns
            for l in ['AssetClass','Level1','Level2']:
                l_agg_map = agg_map.loc[agg_map.LevelName == l,:]
                p_l_qrm = pd.merge(p_qrm,l_agg_map,on='Security',how='left')
                p_l_qrm = p_l_qrm.loc[pd.notnull(p_l_qrm.AssetClass),:]
                if len(p_l_qrm) < 1:
                    continue
                p_l_qrm[qrm_cols_rename] = p_l_qrm[qrm_cols_rename].mul(\
                    p_l_qrm.Agg_Ratio,axis=0)
                p_l_qrm = remove_columns(p_l_qrm,['Agg_Ratio','LevelName'])
                
                # Compute total exposures, then per currency
                p_total = p_l_qrm.groupby('AssetClass').sum().reset_index()
                p_total.loc[:,'Currency'] = 'Total'
                p_ccy = p_l_qrm.groupby(['AssetClass','Currency']).sum().\
                    reset_index()
                p_ccy = p_ccy.append(p_total)
                
                # Save results with other portfolios and breakdowns
                p_ccy.loc[:,'Portfolio'] = p + '|' + l + '|' + p_ccy['AssetClass']
                all_port = all_port.append(p_ccy)
        except:
            print('\nIssue in function compute_parallel_results_aggregation'+ \
                  ' with portfolio ' + p + '\n')
    
    
    # Add size relative to total IP
    total_IP_UP200 = all_port.loc[(all_port.Portfolio == 'IP') & \
        (all_port.Currency == 'Total'),'UP_200'].values[0]
    all_port.loc[:,'UP_200RelativeToIP'] = \
                all_port.loc[:,'UP_200'] / total_IP_UP200
    
    # Transform to long format
    all_port.loc[:,'AsOfDate'] = fdate
    all_port = remove_columns(all_port,'AssetClass')
    df_long = pd.melt(all_port,id_vars=['AsOfDate','Portfolio','Currency'],\
        var_name='Metric',value_name='MetricValue')
    
    ### Upload new results only to database
    # Get existing and compare new results
    existing_df = remove_columns(data_mt.fetch_files('qrm_parallel_results',
            cond={'qrm_parallel_results':[['AsOfDate','=',fdate]]}),
            ['Version','TimeStamp'])
    compare_cols = [x for x in existing_df.columns.tolist() if \
                    x != 'MetricValue']
    new_df = compare_df(df_long,existing_df,compare_cols)
    # Upload
    if len(new_df) > 0:
        data_mt.upload_file('qrm_parallel_results',df=new_df)
    return
###############################################################################
def get_asset_class_maps(date):
    """ date: portfolio as of date
        This function gets the map from instrument to asset class for a 
    specific date, aggregating AssetClass level with Levels 1 and 2.
    """
    # Format input date
    fdate = format_date(date)
    
    ## AssetClass map
    bond_agg_map = data_mt.fetch_files('security_agg_map',
        cond={'security_agg_map':[['AsOfDate','=',fdate]]})
    bond_agg_map_ac = bond_agg_map.loc[bond_agg_map.Agg_Key == 'AssetClass',:]
    bond_agg_map_ac = bond_agg_map_ac.rename(\
        columns={'Agg_Key':'LevelName','Agg_Value':'AssetClass'}).\
        loc[:,['Security','LevelName','AssetClass','Agg_Ratio']]
    
    ## Slicing and dicing map
    # Make sure bonds are mapped to SD asset classes first
    sdf.map_bonds(fdate)
    sd_map = remove_columns(data_mt.fetch_files(\
        'sd_bond_mapping'),['Version','TimeStamp'])
    sd_map.loc[:,'Agg_Ratio'] = 1.0
              
    ## Aggregate both maps
    agg_map = bond_agg_map_ac.append(sd_map,ignore_index=True)
    
    return agg_map
###############################################################################
def HFV_input_checks(date,filetype='HFV_daily_prelim'):
    """ date: as of date of interest
        filetype: HFV_daily_prelim or HFV_daily_final. Which HFV file to check
    for a specific date
    """
    # Format input date
    fdate = format_date(date)
    
    # Get file and previous month's file for comparison
    try:
        hfv = data_mt.fetch_files(filetype,\
            cond={filetype:[['AsOfDate','=',fdate]]})
        # Get previous month file
        prev_date = get_closest_date(fdate,filetype,\
            before_after='before_strict')
        hfv_prev = data_mt.fetch_files(filetype,\
            cond={filetype:[['AsOfDate','=',prev_date]]})
    except Exception as e:
        print('\nCould not get HFV file for input checks for date ' + fdate)
        print(e)
        return
        
    ### Check list of SEC 1, 2 and 3 fields already exist in mapping tables
    # SEC 1,2,3 to AssetClass map and SD Level1 rules
    sec_to_partition = data_mt.fetch_files('sec_to_partition')
    sd_rules = data_mt.fetch_files('sd_input_ac_mapping')
    
    # Get list of Level 1 SD rules, partition and HFV SEC combinations
    sd_rules = sd_rules.loc[sd_rules.LevelName == 'Level1',:]
    sd_SEC = np.unique(sd_rules[['Criteria1','Criteria2','Criteria3']].apply(\
        lambda x: '|'.join(map(lambda y: y.split('|')[1].upper(),x)),axis=1))
    partition_SEC = np.unique(sec_to_partition[['SEC1','SEC2','SEC3']].apply(\
        lambda x: '|'.join(map(lambda y: y.upper(),x)),axis=1))
    hfv_SEC = np.unique(hfv[['SEC 1','SEC 2','SEC 3']].apply(lambda x:\
        '|'.join(map(lambda y: y.upper(),x)),axis=1))
    
    # Get list of new SEC combination and print them if any exist
    new_sd_SEC = [x for x in hfv_SEC if x not in sd_SEC]
    new_partition_SEC = [x for x in hfv_SEC if x not in partition_SEC]
    
    if len(new_sd_SEC) > 0:
        print('\nNew SEC fields combinations not in SD map:')
        print(new_sd_SEC)
    
    if len(new_partition_SEC) > 0:
        print('\nNew SEC fields combinations not in partition map:')
        print(new_partition_SEC)
    
    ### Check if there are any bonds with negative par or fair values
    metrics_orig = ['Par/Curr_Face USD_Equiv','Fair_Value(USD_Equiv)']
    hfv_metrics = hfv.loc[:,['Security'] + metrics_orig]
    hfv_metrics.columns = ['Security','Par','FV']
    hfv_metrics_agg = hfv_metrics.groupby('Security').sum().reset_index()
    hfv_negative = hfv_metrics_agg.loc[(hfv_metrics_agg.Par <= 0) | 
                                       (hfv_metrics_agg.FV <= 0),:]
    
    # Print list of negative bonds if any
    if len(hfv_negative) > 0:
        print('Bonds with negative Par or Fair Value (in $mm): \n')
        print(hfv_negative)
    
    ### Compare CUSIP and ticket counts with previous month
    # Rename columns we keep
    cols = ['Security','SEC 1','Ticket','Fair_Value(USD_Equiv)']
    cols_rename = ['Security','SEC1','Ticket','FV']
    hfv = hfv.loc[:,cols].rename(index=str,columns=dict(zip(cols,cols_rename)))
    hfv_prev = hfv_prev.loc[:,cols].rename(index=str,\
                           columns=dict(zip(cols,cols_rename)))
    
    # Get ticket and CUSIP counts, as well as total Fair Value per SEC 1
    agg_f = {'Ticket':'count','FV':'sum'}
    # Current month
    hfv_unique = hfv.drop_duplicates('Security').groupby('SEC1').agg(\
                                    {'Security':'count'})
    hfv_count = hfv.groupby('SEC1').agg(agg_f)
    hfv_agg = pd.merge(hfv_unique,hfv_count,left_index=True,right_index=True)
    hfv_agg = hfv_agg.append(pd.DataFrame(hfv_agg.sum(),columns=['Total']).T)
    # Previous month
    hfv_prev_unique = hfv_prev.drop_duplicates('Security').groupby('SEC1').\
        agg({'Security':'count'})
    hfv_prev_count = hfv_prev.groupby('SEC1').agg(agg_f)
    hfv_prev_agg = pd.merge(hfv_prev_unique,hfv_prev_count,\
        left_index=True,right_index=True)
    hfv_prev_agg = hfv_prev_agg.append(\
        pd.DataFrame(hfv_prev_agg.sum(),columns=['Total']).T)
    # Get changes
    hfv_changes = hfv_agg - hfv_prev_agg
    
    # Format dataframes for better legibility and print
    hfv_summary = {'Previous Month: ' + prev_date:hfv_prev_agg,\
        'Current Month: ' + fdate:hfv_agg,'Changes':hfv_changes}
    print('\nBond and ticket counts, Fair Values in $mm per SEC 1 category')
    for k,v in sorted(hfv_summary.items(),reverse=True):
        df = v.copy()
        df[['Security','Ticket']] = df[['Security','Ticket']].astype(int)
        df['FV'] = (df['FV'] / np.power(10,6)).map('{:,.0f}'.format)
        print('\n' + k)
        print(df)
    
    return
###############################################################################
def to_excel_date_nb(date,out='string'):
    """ date: a single date or a list of dates in string format to be converted
    into an Excel date number.
        out: format in which to output result. string for a string, int for
    an integer (day only), float (includes hour)
    """
    # Base date for Excel
    base = datetime.datetime(1899,12,30)
    
    if isinstance(date,pd.DataFrame):
        date = pd.Series(date.iloc[:,0])
    
    # Excel date number is the difference between a date and that base in days
    if isinstance(date,basestring):
        diff = pd.to_datetime(date) - base
        date_excel = diff.days
        if out == 'string':
            return np.str(np.int(date_excel))
        elif out == 'int':
            return np.int(date_excel)
        elif out == 'float':
            return np.float(date_excel)
    
    elif isinstance(date,pd.Series):
        if isinstance(date[0],basestring):
            diff = pd.to_datetime(date) - base
        else:
            diff = date - base
        
        if out == 'string':
            date_excel = diff.map(lambda x: np.str(np.int(x.days)))
        elif out == 'int':
            date_excel = diff.map(lambda x: np.int(x.days))
        elif out == 'float':
            date_excel = diff.map(lambda x: np.float(x.days))
        
        return date_excel
###############################################################################
def format_df_output(results,columns=None,id_cols=None):
    """ results: dictionary containing final results, or dataframe to which
    we only need to add the column ID
        columns: column names for the dataframe to be returned
        id_cols: list of columns to use to create an ID if not all of them
        This function formats into dataframes the dictionaries containing the
    results so we can export them. Results are formatted into dataframes into
    which we add an ID column.
    """
    # Get list of columns
    if columns is not None:
        cols = columns
    else:
        cols = results.columns.tolist()
    
    # Get date column name
    if 'Date' in cols:
        col_date = 'Date'
    elif 'AsOfDate' in cols:
        col_date = 'AsOfDate'
    else:
        print('No date column, no output formatting')
        return
    
    if isinstance(results,pd.DataFrame):
        df = remove_columns(results.reset_index(),'index')
        columns = df.columns.tolist()
        # Make sure column Date is at the beginning and Value is at the end
        columns.remove(col_date)
        columns = [col_date] + columns
        if 'Value' in columns:
            columns.remove('Value')
            columns = columns + ['Value']     
    else:
        df = pd.DataFrame(results,columns=columns)
    
    # Change format of columns Order, VaR Level and Level to string
    for c in columns:
        if c in ['Order','VaR Level','Level']:
            df.loc[:,c] = df.loc[:,c].apply(lambda x: str(np.int(x)))
    
    # Add ID column
    if len(df) > 0:
        if id_cols is not None:
            id_cols.remove(col_date)
            df['ID'] = to_excel_date_nb(df[[col_date]]) + \
                df[id_cols].astype(str).sum(axis=1)
        else:
            id_agg_cols = columns[:]
            if 'RunID' in columns:
                id_agg_cols.remove('RunID')
                
            df['ID'] = to_excel_date_nb(df[[col_date]]) + \
                df[id_agg_cols[1:-1]].astype(str).sum(axis=1)
    else:
        df['ID'] = ''
    
    return df[['ID'] + columns]
###############################################################################
def col_df(df):
    """ df: pandas dataframe with a single column
        This function drops duplicate rows from a single column dataframe and
    resets index to a range of integers starting at 0.
    """
    return df.drop_duplicates().reset_index(drop=True)
###############################################################################
def breakdown_tab():
    """ This function gets the breakdown data necessary for some Excel file
    template (list of asset classes in AssetClass, Level1, Level2 breakdowns)
    """
    # Get Asset Classes in AssetClass breakdown
    sec_to_partition = data_mt.fetch_files('sec_to_partition')
    asset_classes = col_df(sec_to_partition.loc[:,['Partition']].\
        sort_values(by='Partition')).rename(columns={'Partition':'AssetClass'})
                          
    # Get list of asset classes in level 1 and 2
    mapping_rules = data_mt.fetch_files('sd_input_ac_mapping')
    level1 = col_df(mapping_rules.loc[mapping_rules.LevelName == 'Level1',\
        ['AssetClass']].sort_values(by='AssetClass'))
    level2_ac = np.unique(mapping_rules.loc[mapping_rules.LevelName == 'Level2',\
        'AssetClass']).tolist()
    
    ### Get level 2 and order it using level 1
    # Start with level 2 asset classes
    level1_2 = mapping_rules.loc[mapping_rules.LevelName == 'Level2',\
                                 ['AssetClass','Criteria1']]
    level1_2.loc[:,'Level1'] = level1_2.apply(lambda x: \
        x.Criteria1.split('|')[1],axis=1)
    level1_2.loc[:,'Level1_2_Breakdown'] = 'Level2'
    # Add level 1 to dataframe
    level1_app = level1.copy()
    level1_app.loc[:,'Level1_2_Breakdown'] = 'Level1'
    level1_app.loc[:,'Level1'] = level1_app.loc[:,'AssetClass']
    level1_2 = level1_2.append(level1_app)
    level1_2.sort_values(by=['Level1','Level1_2_Breakdown'],inplace=True)
    level1_2.drop_duplicates(subset='AssetClass',keep='first',inplace=True)
    # Keep level 2 only
    level2 = level1_2.loc[:,['Level1_2_Breakdown','Level1','AssetClass']].copy()
    level2.columns = ['Level2_Breakdown','Level2Parent','Level2']
    level2 = level2.loc[level2.Level2.isin(level2_ac),:]
    level2.reset_index(drop=True,inplace=True)
    # Finish formatting of level1_2 and level 1
    level1_2 = remove_columns(level1_2.rename(columns=\
        {'AssetClass':'Level1_2'}),['Criteria1','Level1']).reset_index(drop=True)
    level1.rename(columns={'AssetClass':'Level1'},inplace=True)
    
    # Get list of Credit Spread Indices
    sensitivities_def = data_mt.fetch_files('scenario_sensitivities')
    cs_indices = col_df(sensitivities_def.loc[sensitivities_def.RiskType=='CS',\
        ['RiskFactor']].sort_values(by='RiskFactor')).reset_index(drop=True).\
        rename(columns={'RiskFactor':'CreditSpreadIndices'})
        
    ### Create final dataframe to contain all columns
    out_df = pd.DataFrame()
    out_df = pd.concat([out_df,asset_classes,level1,level2,level1_2,cs_indices],\
                       axis=1)
    out_df.index = range(1,len(out_df)+1)
    
    return out_df
###############################################################################
def update_xl_tab(wb,tab,df):
    """ wb: Excel workbook (uses xlwing)
        tab: tab name in the workbook that is being updated
        df: dataframe pasted in the Excel tab
        This function updates a tab in an Excel file using the dataframe and
    Excel details passed as input.
    """
    # Clear excel file's tab contents
    try:
        ws = xw.Sheet(tab,wkb=wb)
    except:
        ws = wb.sheets[tab]
    ws.clear_contents()
        
    # Paste data by blocks
    block_size = 20000
    nb_blocks = len(df) / block_size + 1
    for j in range(nb_blocks):
        gc.collect()
        if j < 1:
            try:
                ws_range = xw.Range(ws,'A1')
            except:
                ws_range = ws.range('A1')
            ws_range.value = df[:block_size]
        else:
            paste_row = block_size * j + 2
            try:
                ws_range = xw.Range(ws,'A' + np.str(paste_row))
            except:
                ws_range = ws.range('A' + np.str(paste_row))
            df_start = block_size * j
            df_end = block_size * (j + 1)
            ws_range.options(header=False).value = df[df_start:df_end]
    
    return
###############################################################################
def update_CSRM_parallel_file(filename,folder='',override=False,new_name=None):
    """ filename: name of existing Excel file we are updating
        folder: location of the Excel file to update. By default it corresponds
    to the current folder
        override: whether to override data in existing file passed as input
        new_name: if not overriding existing file, this is the name given to
    the new file
        This function updates the tabs of the Excel file containing Credit
    Spread Risk Metric and parallel rates +200 results.
    """
    current_folder = os.getcwd() + '\\'
    # If no folder is passed as input we use the current folder
    if folder == '':
        folder = os.getcwd() + '\\'
     
    
    os.chdir(folder)
    try:
        wb = xw.Workbook(folder + filename)
    except:
        wb = xw.Book(folder + filename)
    
    ### List of dataframes and tabs for which we get data and update
    # Dataframe containing metrics results
    df_metrics = ['qrm_parallel_results','IP_csrm','IP_stress_pnl']
    
    # List of columns forming ID tab for metrics dataframe
    id_cols = {
        'qrm_parallel_results':['AsOfDate','Portfolio','Currency','Metric'],
        'IP_csrm':['AsOfDate','Portfolio','SensitivitiesSource','Partition'],
        'IP_stress_pnl':['AsOfDate','Portfolio','SensitivitiesSource','Floor',
                         'Scenario','RiskFactor']}
    
    # Full list of dataframes pasted in Excel, with map from df name to tab
    df_tab_map = {'qrm_parallel_results':'IR_200_data',
                  'IP_csrm':'CSRM_data',
                  'IP_stress_pnl':'CSRM_indices_data',
                  'Breakdown':'Breakdown'}
    
    # Save list of portfolios and dates for which we have results
    portfolios = []
    dates = []
    
    # Get breakdown data
    breakdown = breakdown_tab()
    
    # Get the dataframes in df_list and add the ID column
    for df_name in df_tab_map.keys():
        gc.collect()
        # Get table and destination tab names
        tab = df_tab_map[df_name]
        
        # Differentiate between metrics dataframes and others
        if df_name in df_metrics:
            id_columns = id_cols[df_name]
            
            if df_name == 'IP_stress_pnl':
                conditions = {'IP_stress_pnl':\
                              [['Scenario','=','CreditSpreadRiskMetric']]}
                df_i = data_mt.fetch_files(df_name,cond=conditions)
            else:
                df_i = data_mt.fetch_files(df_name)
            
            # Add list of porfolios in this dataframe to main list
            portfolios = np.unique(portfolios + \
                [x for x in df_i.Portfolio if '|' not in x]).tolist()
            dates = dates + [format_date(x) for x in np.unique(df_i.AsOfDate)]
            
            # Add ID column
            df = format_df_output(remove_columns(df_i,['Version','TimeStamp']),\
                                  id_cols=id_columns)
            df.set_index('ID',inplace=True)
        else:
            if df_name == 'Breakdown':
                df = breakdown
        
        update_xl_tab(wb,tab,df)
    
    ### Update tab Details with portfolio and dates list
    # Remove some portfolios from list of interest
    exclude_portfolios = ['NotBOLI']
    portfolios_df = pd.DataFrame(columns=['Portfolios'],\
        data=[x for x in portfolios if x not in exclude_portfolios])
    # Keep unique list of dates, and order them
    dates = np.unique(dates).tolist()
    dates_df = pd.DataFrame(data=dates,columns=['Dates'])
    dates_df.loc[:,'DateNb'] = dates_df.apply(lambda x:\
                date_str_to_datetime(x.Dates),axis=1)
    dates_df = remove_columns(dates_df.sort_values(by='DateNb'),['DateNb'])
    dates_df.index = range(len(dates_df))
    
    # Aggregate and save to Excel
    details_df = pd.DataFrame()
    details_df = pd.concat([details_df,portfolios_df,dates_df],axis=1)
    details_df.index = range(1,len(details_df)+1)
    update_xl_tab(wb,'Details',details_df)
    
    # Save output file
    if override:
        wb.save()
    else:
        if new_name is None:
            now_str = datetime.datetime.now().strftime('%Y-%m-%d %Hh%Mm%S') 
            new_name = now_str + ' - ' + filename
        wb.save(folder + new_name)
        
    wb.close()
    
    # Go back to working folder
    os.chdir(current_folder)
    return
###############################################################################

### MDX queriesimport os
import sys
import pandas as pd
import numpy as np
import win32com.client
from operator import itemgetter
######################## Display Options ######################################
#pd.reset_option("display.width")
#pd.set_option('display.width', 135)
#pd.set_option('display.max_columns', None) # default is 20
#pd.set_option('display.max_rows', None) # default is 20
#np.set_printoptions(threshold=np.inf) # default is 100
######################## Folders ##############################################
# Location of Python files
#production_folder = 'P:\\Remy\\Stress Scenarios\\Production\\'
#os.chdir(production_folder)
###############################################################################
import functions_calls as fc
#reload(fc)
import ms_access_data_mgmt
reload(ms_access_data_mgmt)
###############################################################################
def rename_cube_df(df,rename_dict):
    """ df: a dataframe for which we want to rename the columns
    rename_dict: dictionary containing the map from the columns in df to the
        ones we want in the end.
    It is possible that df contains fewer columns that keys in rename_dict. In
    this case rename_dict is adjusted: df will have fewer columns because
    the output from the MDX query does not contain identifier columns. This 
    can happen when the query returns a single entry, only the columns 
    requested are returned. If several entries match the condition then the
    query will output the identifiers (potentially several columns), thus
    adding columns. Here it is assumed that if the dataframe to rename has
    fewer columns it is because it contains zero identifier column.
    """
    # Check whether the dictionary needs to be modified
    offset = len(rename_dict.keys()) - len(df.columns)
    if offset < 0:
        print('rename_dict shorter, columns were not renamed')
        return df
    elif offset == 0:
        new_dict = rename_dict
    elif offset > 0:
        new_dict = {}
        for i in range(len(df.columns)):
            new_dict.update({i:rename_dict[i+offset]})
        
    renamed_df = df.rename(index=str,columns=new_dict)
    return renamed_df
###############################################################################
def run_mdx_query(cube,conn,columns,rows,condition=None,rename_dict=None):
    """cube: string containing name of cube to query
    conn: ADODB connection to the cube
    columns: formatted string detailing columns piece in the query. eg: 
        [ValScen].[Scenario].Members, or 
        [Measures].[Face Amount],[Measures].[Book Value]
    rows: formated string detailing rows piece in the query. eg:
        [CUSIP].[CUSIP].Members
        [ValScen].[Scenario].Members
    condition: if a WHERE clause is included this is the string containing
    that piece of the query. eg:
        [Measures].[Value],[CF TYPE].[Total Value]
    rename_dict: a dictionary used to rename the columns from the output
        dataframe. If the length of the dictionary does not match the number
        of columns of the dataframe it is adjusted (too many entries in 
        dataframe). Done by function rename_cube_df()
    Runs a MDX query on a cube, based on the other inputs. The results are
    formatted into a dataframe.
    """
    ### Create query
    qry = 'SELECT NON EMPTY {' + columns + '} ON COLUMNS, NON EMPTY {' + rows + '} ON ROWS ' + \
        'FROM [' + cube + ']'
    
    # Add where clause if one is given as input
    if condition is not None:
        qry += ' WHERE (' + condition + ')'
    
    ### Run query on cube 
    # Create record set in which results will be fetched
    try:
        rs = win32com.client.Dispatch(r'ADODB.Recordset')
        rs.Open(qry,conn)
        
        # Paste results in dataframe
        output_df = pd.DataFrame(list(rs.GetRows(rs.RecordCount))).T
        
        # Rename columns if dictionary is given
        if rename_dict is not None:
            output_df = rename_cube_df(output_df,rename_dict)
        
        # Close record set
        rs.Close()
        rs = None
    except:
        print('Issue in function run_mdx_query, data could not be fetched')
        print(sys.exc_info()[0])
        raise
    
    return output_df
###############################################################################
def rename_df_columns(df,colnames_df):
    """ df: dataframe containing key rate sensitivities data
        colnames_df: ordered list of shocks output columns in a dataframe
        This function takes the output of the Key Rate Sensitivities query and
    renames the columns and cleans the dataframe.
    """
    # Put column names into a dictionary to be used to rename df columns
    shocks_colnames_dict = {}
    for i in range(len(colnames_df)):
        shocks_colnames_dict.update({i+1:colnames_df.loc[i,0]})
    shocks_colnames_dict.update({0:'CUSIP'})
    
    # Add column names to the shocks dataframe and drop NA columns and rows
    df.rename(index=str,columns=shocks_colnames_dict,inplace=True)
    df.dropna(axis='columns',how='all',inplace=True)
    df = df.loc[:,pd.notnull(df.columns)]
    if 'Base' in df.columns.tolist():
        df = df.loc[pd.notnull(df.CUSIP) & pd.notnull(df.Base),:]
    else:
        df = df.loc[pd.notnull(df.CUSIP),:]
    
    return df
###############################################################################
def fetch_qrm_valuation_data(filename,file_location):
    """ filename: Excel file linked to the QRM OLAP valuation run report
        eg: '20170428_QRM_KRS.xlsm'
    file_location: location of the file. eg: 'P:\\Portfolio VaR\\'
    no_cusip_info: parameters used to query the information on instruments
        that have no CUSIP in the report (CUSIP value is '')
    This function takes an Excel file linked to a QRM OLAP report and fetches
    data directly from the OLAP report. It then outputs two dataframes: one 
    containing information about the CUSIPs in the valuation run (Market Value,
    BPV, Currency, ...)  and the other containing the results of the valuation 
    runs, both key rate sensitivities and parallel shocks.
    """
    ##### Create needed COM objects
    connection = win32com.client.Dispatch(r'ADODB.Connection')
    ado_catalog = win32com.client.Dispatch(r'ADOMD.Catalog')
    xlApp = win32com.client.Dispatch("Excel.Application")
    
    ##### Get connection string from Excel file and get cube details
    # Get Excel object if open already, otherwise open it now and close after
    try:
        WB = xlApp.Workbooks(filename)
        already_open = True
    except:
        WB = xlApp.Workbooks.Open(file_location + filename)
        already_open = False
    
    # Get connections and connection strings. Clean strings if needed
    connString_list = []
    for c in WB.Connections:
        OLAP_xl_connection = c
        connString = OLAP_xl_connection.OLEDBConnection.Connection
        if connString[:6] == 'OLEDB;':
            connString = connString[6:]
        connString_list += [connString]
    
    # Close file it it wasn't already open
    if not(already_open):
        WB.Close()
    
    # Details: Dollar Number Columns to adjust for swaps
    dollar_nb_cols = ['Market Value','Economic Value','Vega',\
        'Dollar Option Cost']
    
    # Dataframe to contain parallel and key rate shocks results if it exists
    IP_parallel = None
    all_KRS = None
    ### Loop through different connections
    for connString in connString_list:
        # Open connection by first closing it if already open
        if connection.State:
            connection.Close()
        connection.Open(connString)
        
        # Get cube name
        ado_catalog.ActiveConnection = connection
        ocube = ado_catalog.CubeDefs(0)
        cube_name = ocube.Name
        
        # Get report's date
        date_df = run_mdx_query(cube_name,connection,\
            '[Portfolio Run].[Assumption Set].[AllPortfolio Run]',\
            '[Portfolio Run].[Market As Of Date].Members')
        date = fc.format_date(date_df.iloc[1,0])
        
        
        ##### Query cube for data: create needed query pieces
        ### Account and source conditions to split portfolio:
        # Core portfolio, Summit swaps and Wall Street instruments are separated
        bond_rows = '[CUSIP].[CUSIP].Members'
        ## Create conditions to filter different accounts
        accounts_cond = '[Account].[Account].['
        # Core portfolio
        core_accounts = ['AFS Investment Securities','HTM Investment Securities',\
            'BOLI']
        cond_IP_core = '{'
        for a in core_accounts:
            cond_IP_core += accounts_cond + a + '],'
        cond_IP_core = cond_IP_core[:-1] + \
            '},[Custom3].&[0]'
        # Summit swaps: paying and receiving legs are split
        summit_accounts = ['AFS Security Swaps','Clipper Swaps',\
            'Cross Currency Swaps']
        cond_summit = '{'
        for a in summit_accounts:
            cond_summit += accounts_cond + a + '],'
        cond_summit = cond_summit[:-1] + \
            '},[Custom3].&[0],[Source File].[Summit]'
        pay_cond = '[Measures].[Paid/Dlvrd/Fxd Leg]'
        rec_cond = '[Measures].[Rcvd /Float Leg]'
        pay_accrued = '[Measures].[Accrued Interest - Paid/Dlvrd Leg]'
        rec_accrued = '[Measures].[Accrued Interest - Rcvd Leg]'
        # Wall Street instruments
        WSS_source = ',[Source File].[WSSMM]'
        WSS_cond_AFS = '[Account].[Account].[AFS Investment Securities]'+WSS_source
        WSS_cond_HTM = '[Account].[Account].[HTM Investment Securities]'+WSS_source
        WSS_cond = cond_IP_core + ',[Source File].[WSSMM]'
        WSS_rows = '[Custom1].[Custom1].Members'
        
        
        ### We get instrument details, then shocks results (KRS and parallel) 
        # and currency of each bond
        # Details
        details_cond = '[Scenario Type and Name].[Base]'
        details_cols_list = ['Face Amount','Book Value','Market Value',
            'Economic Value','OAD','BPV','Convexity','Vega','Dollar Option Cost',
            'MTM Spread','Avg CPR','WAL','Spread BPV','Maturity Date',
            'Accrued Interest']
        # Shocks results
        shocks_cond = '[CF TYPE].[Total Value]'
        cols_shocks = '[ValScen].[Scenario].Members'
        condition_shocks = '[Measures].[Value]' + ',' + shocks_cond
        # Currency
        ccy_cond = '[Measures].[Market Value]'
        
        
        ### IP Core
        # Create query column piece and dictionary to rename output columns
        cols_details = ''
        details_cols_dict = {}
        # Add each columns to query and dictionary
        for i in range(len(details_cols_list)):
            cols_details += '[Measures].[' + details_cols_list[i] + '],'
            details_cols_dict.update({i+1:details_cols_list[i]})
        # Add CUSIP as the first column and adjust query piece
        details_cols_dict.update({0:'CUSIP'})
        cols_details = cols_details[:-1]
        
        ## Query IP core details
        IP = run_mdx_query(cube_name,connection,cols_details,bond_rows,
            cond_IP_core + ',' + details_cond,rename_dict=details_cols_dict)
        IP = IP.loc[pd.notnull(IP.CUSIP) & pd.notnull(IP.BPV),:]
        
        ## Query IP core shocks results
        IP_shocks = run_mdx_query(cube_name,connection,cols_shocks,bond_rows,
                               cond_IP_core + ',' + condition_shocks)
        
        # Get key rate shocks column names using existing CUSIP
        dummy_cusip = IP['CUSIP'][1]
        cols_shocks_colnames = '[CUSIP].[CUSIP].&[' + dummy_cusip + ']'
        shocks_colnames_df = run_mdx_query(cube_name,connection,cols_shocks_colnames,
                                        cols_shocks,condition_shocks)
        IP_shocks = rename_df_columns(IP_shocks,shocks_colnames_df)
        
        ## IP Currency Information
        cols_ccy_qry = '[Currency].[Currency].Members'
        IP_ccy = run_mdx_query(cube_name,connection,cols_ccy_qry,bond_rows,
            cond_IP_core + ',' + details_cond + ',' + ccy_cond)
        # Get column names and rename IP_ccy columns, removing column with no name
        ccy_colnames_df = run_mdx_query(cube_name,connection,bond_rows,
            cols_ccy_qry,cond_IP_core + ',' + details_cond  + ',' +ccy_cond).\
            iloc[:,:2]
        IP_ccy = rename_df_columns(IP_ccy,ccy_colnames_df)
        
        
        ### Summit main details
        summit = run_mdx_query(cube_name,connection,cols_details,bond_rows,
            cond_summit + ',' + details_cond,rename_dict=details_cols_dict)
        summit = summit.loc[pd.notnull(summit.CUSIP) & pd.notnull(summit.BPV),:]
#        # Duplicate for pay and receive leg
#        summit_pay = summit.copy()
#        summit_pay.loc[:,'CUSIP'] = summit_pay.loc[:,'CUSIP'] + '_P'
#        summit_rec = summit.copy()
#        summit_rec.loc[:,'CUSIP'] = summit_rec.loc[:,'CUSIP'] + '_R'
        
        ## Splitting main details between receiving and paying
        swap_dict = details_cols_dict.copy()
        swap_dict.update({np.int(max(swap_dict.keys())+1):'Swap'})
        swap_dict.update({np.int(max(swap_dict.keys())+1):'AccruedLeg'})
        summit_pay = run_mdx_query(cube_name,connection,cols_details+',' +pay_cond+
            ', ' + pay_accrued,\
            bond_rows,cond_summit + ',' + details_cond,rename_dict=swap_dict)
        summit_pay = summit_pay.loc[pd.notnull(summit_pay.CUSIP) & pd.notnull(summit_pay.BPV),:]
        summit_rec = run_mdx_query(cube_name,connection,cols_details+','+rec_cond+
            ', ' + rec_accrued,\
            bond_rows,cond_summit + ',' + details_cond,rename_dict=swap_dict)
        summit_rec = summit_rec.loc[pd.notnull(summit_rec.CUSIP) & pd.notnull(summit_rec.BPV),:]
        summit_pay.loc[:,'CUSIP'] = summit_pay.loc[:,'CUSIP'] + '_P'
        summit_rec.loc[:,'CUSIP'] = summit_rec.loc[:,'CUSIP'] + '_R'
        # Use leg specific accrued interest if available, else half of total
        summit_pay.loc[:,'Accrued Interest'] = summit_pay.apply(lambda x:\
            x.AccruedLeg if pd.notnull(x.AccruedLeg) else \
            x['Accrued Interest'] / 2,axis=1)
        summit_rec.loc[:,'Accrued Interest'] = summit_rec.apply(lambda x:\
            x.AccruedLeg if pd.notnull(x.AccruedLeg) else \
            x['Accrued Interest'] / 2,axis=1)
        
        # Adjust dollar number columns for both legs of the swap
        summit_pay.loc[:,'Swap'] = summit_pay.loc[:,'Swap'] / \
                      summit_pay.loc[:,'Market Value']
        summit_rec.loc[:,'Swap'] = summit_rec.loc[:,'Swap'] / \
                      summit_rec.loc[:,'Market Value']
        summit_pay[dollar_nb_cols] = summit_pay[dollar_nb_cols].mul(\
                  summit_pay.Swap,axis=0)
        summit_rec[dollar_nb_cols] = summit_rec[dollar_nb_cols].mul(\
                  summit_rec.Swap,axis=0)
        summit_pay = fc.remove_columns(summit_pay,'Swap')
        summit_rec = fc.remove_columns(summit_rec,'Swap')
        
        
        ## Summit shocks results
        # Pay leg
        shocks_summit_cond_payleg = ','.join([cond_summit,shocks_cond,pay_cond])
        summit_shocks_payleg = run_mdx_query(cube_name,connection,cols_shocks,bond_rows,
            shocks_summit_cond_payleg)
        summit_shocks_payleg = rename_df_columns(summit_shocks_payleg,shocks_colnames_df)
        summit_shocks_payleg.loc[:,'CUSIP'] = summit_shocks_payleg.loc[:,'CUSIP'] + '_P'
        # Receiving leg
        shocks_summit_cond_recleg = ','.join([cond_summit,shocks_cond,rec_cond])
        summit_shocks_recleg = run_mdx_query(cube_name,connection,cols_shocks,bond_rows,
            shocks_summit_cond_recleg)
        summit_shocks_recleg = rename_df_columns(summit_shocks_recleg,shocks_colnames_df)
        summit_shocks_recleg.loc[:,'CUSIP'] = summit_shocks_recleg.loc[:,'CUSIP'] + '_R'
        
        ## Summit currency information
        # Summit Swaps Pay legs
        summit_pay_ccy_cond = ','.join([cond_summit,pay_cond,details_cond])
        summit_pay_ccy = run_mdx_query(cube_name,connection,cols_ccy_qry,bond_rows,
            summit_pay_ccy_cond)
        ccy_colnames_df = run_mdx_query(cube_name,connection,bond_rows,
            cols_ccy_qry,summit_pay_ccy_cond).iloc[:,:2]
        summit_pay_ccy = rename_df_columns(summit_pay_ccy,ccy_colnames_df)
        summit_pay_ccy.loc[:,'CUSIP'] = summit_pay_ccy.loc[:,'CUSIP'] + '_P'
        
        # Summit Swaps Receiving legs
        # Cross currency swaps receiving legs in dollars have no currency listed
        summit_rec_ccy_cond = ','.join([cond_summit,rec_cond,details_cond])
        summit_rec_ccy = run_mdx_query(cube_name,connection,cols_ccy_qry,bond_rows,
            summit_rec_ccy_cond)
        ccy_colnames_df = run_mdx_query(cube_name,connection,bond_rows,
            cols_ccy_qry,summit_rec_ccy_cond).iloc[:,:2]
        summit_rec_ccy = rename_df_columns(summit_rec_ccy,ccy_colnames_df)
        summit_rec_ccy.loc[:,'CUSIP'] = summit_rec_ccy.loc[:,'CUSIP'] + '_R'
        # Add value under USD for bonds with no currency
        for i in range(len(summit_rec_ccy)):
            if pd.isnull(summit_rec_ccy.iloc[i,1:]).all():
                summit_rec_ccy.iloc[i,:]['USD (QRM)'] = 1.
        
        
        ### WSS instruments: query data, then do checks and mapping
        ## Query shocks results core details
        WSS = run_mdx_query(cube_name,connection,cols_details,WSS_rows,
            WSS_cond + ',' + details_cond,rename_dict=details_cols_dict)
        WSS = WSS.loc[pd.notnull(WSS.CUSIP) & pd.notnull(WSS.BPV),:]
        # Get AFS and HTM classification of Wall Street instruments
        WSS_AFS = run_mdx_query(cube_name,connection,cols_details,WSS_rows,
            WSS_cond_AFS + ',' + details_cond,rename_dict=details_cols_dict)
        WSS_HTM = run_mdx_query(cube_name,connection,cols_details,WSS_rows,
            WSS_cond_HTM + ',' + details_cond,rename_dict=details_cols_dict)
        WSS_info = WSS_AFS.loc[pd.notnull(WSS_AFS.CUSIP),['CUSIP']]
        WSS_info.loc[:,'AFS_HTM'] = 'AFS'
        WSS_info = WSS_info.append(\
            WSS_HTM.loc[pd.notnull(WSS_HTM.CUSIP),['CUSIP']])
        WSS_info.loc[pd.isnull(WSS_info.AFS_HTM),'AFS_HTM'] = 'HTM'
        
        ## Query WSS core shocks results
        WSS_shocks = run_mdx_query(cube_name,connection,cols_shocks,WSS_rows,
            WSS_cond + ',' + condition_shocks)
        WSS_shocks = rename_df_columns(WSS_shocks,shocks_colnames_df)
        
        ## Query WSS instruments currency
        WSS_ccy = run_mdx_query(cube_name,connection,cols_ccy_qry,WSS_rows,
            WSS_cond + ',' + details_cond + ',' + ccy_cond)
        ccy_colnames_df = run_mdx_query(cube_name,connection,WSS_rows,
            cols_ccy_qry,WSS_cond + ',' + details_cond  + ',' +ccy_cond).iloc[:,:2]
        WSS_ccy = rename_df_columns(WSS_ccy,ccy_colnames_df)
        
        
        ### Merge data from the 3 sources together
        IP = IP.append(summit_pay[IP.columns]).append(summit_rec[IP.columns]).\
                      append(WSS)
        IP_shocks = IP_shocks.append(summit_shocks_payleg).\
            append(summit_shocks_recleg).append(WSS_shocks)
        IP_ccy = IP_ccy.append(summit_pay_ccy).append(summit_rec_ccy).\
            append(WSS_ccy)
        
        # If there are several OLAP connections one of them should contain
        # the parallel shocks and the other the key rate sensitivities
        if 'UP 200' in IP_shocks.columns.tolist():
            IP_parallel = IP_shocks.copy()
        elif '12M KRS' in IP_shocks.columns.tolist():
            all_KRS = IP_shocks.copy()
    
    ### Consolidate parallel and key rate shock results in one dataframe
    if all_KRS is not None:
        all_shocks = all_KRS
        if IP_parallel is not None:
            # Get list of parallel dataframe columns to keep
            IP_parallel = fc.remove_columns(IP_parallel,\
                ['Base','EVE - Bear Steepener'])
            parallel_cols = [x for x in IP_parallel.columns if x \
                not in all_shocks.columns.tolist()] + ['CUSIP']
            all_shocks = pd.merge(all_shocks,IP_parallel[parallel_cols],\
                on='CUSIP',how='outer')
    else:
        print('No Key Rate Sensitivities found in QRM report')
        return
    
    ### Recompute BPV and Convexity for Summit instruments, split Spread BPV
    summit_bonds = summit_pay.CUSIP.tolist() + summit_rec.CUSIP.tolist()
    summit_shocks = all_shocks.loc[all_shocks.CUSIP.isin(summit_bonds),\
        ['CUSIP','UP 25','DOWN 25']]
    summit_shocks.loc[:,'SummitBPV'] = 2 * (summit_shocks.loc[:,'DOWN 25'] - \
                     summit_shocks.loc[:,'UP 25'])
    summit_shocks.loc[:,'SummitConvexity'] = (summit_shocks.loc[:,'DOWN 25'] +\
                     summit_shocks.loc[:,'UP 25']) / np.power(0.25,2)
    IP = pd.merge(IP,summit_shocks[['CUSIP','SummitBPV','SummitConvexity']],\
                  on='CUSIP',how='left')
    IP.loc[:,'BPV'] = IP.apply(lambda x: x.SummitBPV if \
          pd.notnull(x.SummitBPV) else x.BPV,axis=1)
    IP.loc[:,'SummitConvexity'] = IP.apply(lambda x: x.SummitConvexity if \
          pd.notnull(x.SummitConvexity) else x.BPV,axis=1)
    IP.loc[:,'Spread BPV'] = IP.apply(lambda x: x['Spread BPV'] / 2 if \
          pd.notnull(x.SummitBPV) else x['Spread BPV'],axis=1)
    IP = fc.remove_columns(IP,['SummitBPV','SummitConvexity'])
    
    ### Back out currency for all instruments
    IP_ccy.set_index('CUSIP',inplace=True)
    ccy_columns = IP_ccy.columns.tolist()
    # Get number of columns with non null entries (should be one for CUSIPs)
    IP_ccy.loc[:,'NbCcy'] = IP_ccy.apply(lambda x: pd.notnull(x).sum(),axis=1)
    # Replace numbers with the currency's 3-char ID (non empty entries only)
    for ccy in ccy_columns:
        IP_ccy.loc[pd.notnull(IP_ccy.loc[:,ccy]),ccy] = ccy[:3]
    # Set currency for each row as name of column with some data (if only one
    # column with data, otherwise set to Mutliple)
    IP_ccy.loc[:,'Currency'] = IP_ccy.apply(lambda x: 'Multiple' if x.NbCcy > 1 
        else [y for y in x if pd.notnull(y)][0],axis=1)
    IP_ccy.reset_index(inplace=True)
    IP_ccy = IP_ccy.loc[:,['CUSIP','Currency']]
    
    # Add currency information to main details
    IP = pd.merge(IP,IP_ccy,left_on='CUSIP',right_on='CUSIP',how='left')
    
    
    ### Non-Intrader securities: save extra data
    WSS_info.loc[:,'AssetClass'] = 'Foreign Government'
    
    # Summit instruments: get book of each swap
    summit_extract = ms_access_data_mgmt.fetch_files('summit_extract',\
        columns={'summit_extract':['DEAL_NBR','BOOK']},
        cond={'summit_extract':[['AsOfDate','=',date]]}).\
        rename(columns={'DEAL_NBR':'CUSIP','BOOK':'Book'})
    # Add details to each swap based on their book and bond specific hedges
    summit_books_list = fc.remove_columns(ms_access_data_mgmt.fetch_files(\
        'summit_books_list'),['Version','TimeStamp'])
    summit_swap_bond_map = ms_access_data_mgmt.fetch_files(\
        'summit_swap_bond_map')
    summit_extract = pd.merge(summit_extract,summit_books_list,\
        on='Book',how='left')
    # Keep instruments that belong in the Investment Portfolio only
    summit_extract = summit_extract.loc[summit_extract.In_IP == 'Yes',:]
    summit_extract = pd.merge(summit_extract,summit_swap_bond_map[\
        ['Swap','Security']],left_on='CUSIP',right_on='Swap',how='left').\
        rename(columns={'Security':'Security_Mapped'})
    # All swaps are classified as AFS
    summit_extract.loc[:,'AFS_HTM'] = 'AFS'
    # Add details to both legs of the swaps
    summit_extract.loc[:,'CUSIP'] = summit_extract.loc[:,'CUSIP'] + '_P'
    summit_pay_details = pd.merge(summit_pay[['CUSIP']],summit_extract,\
        on='CUSIP',how='left')
    summit_extract.loc[:,'CUSIP'] = summit_extract.apply(lambda x:\
        x.CUSIP[:-2] + '_R',axis=1)
    summit_details = pd.merge(summit_rec[['CUSIP']],summit_extract,\
        on='CUSIP',how='left').append(summit_pay_details)
    
    # Merge Wall Street and Summit instrument details
    # Add currency and set as credit spread
    non_intrader = pd.merge(WSS_info.append(summit_details),IP_ccy,\
        on='CUSIP',how='left')
    non_intrader.loc[:,'SpreadIndex_ID'] = 'NO_SPREAD'
    # Get existing non intrader bond details
    non_intrader_existing = fc.remove_columns(ms_access_data_mgmt.\
        fetch_files('non_intrader_bonds'),['Version','TimeStamp'])
    non_intrader_cols = non_intrader_existing.columns.tolist()
    non_intrader = non_intrader.loc[:,non_intrader_cols]
    # Compare new non-intrader bonds with existing in DB, keep new ones only
    new_non_intrader = fc.compare_df(non_intrader,non_intrader_existing,\
        non_intrader_cols)
    
    if len(new_non_intrader) > 0:
        fc.upload_to_db('non_intrader_bonds',df = new_non_intrader)  
    
    
    ### Get rid of empty columns, and add + to name of KRS up shock columns
    all_shocks.dropna(axis='columns',how='all',inplace=True)
    all_shocks = all_shocks.loc[:,pd.notnull(all_shocks.columns)]
    KRS_pos_shocks_cols = {}
    for c in all_shocks.columns:
        if c[-4:] == ' KRS':
            KRS_pos_shocks_cols.update({c:c + '+'})
    all_shocks.rename(index=str,columns=KRS_pos_shocks_cols,inplace=True)
    
    
    ### Check completeness and market value of Summit and Wall Street bonds
    ## Summit
    summit_comp = summit.loc[:,['CUSIP','Face Amount']].rename(columns=\
        {'CUSIP':'QRM_CUSIP','Face Amount':'QRM_Par'})
    # Get list of swaps that belong to IP
    summit_extract = ms_access_data_mgmt.fetch_files('summit_extract',\
        columns={'summit_extract':['DEAL_NBR','BOOK','EXCHANGE_RATE_PAYLEG',\
        'CONT_NOTIONAL_AMT_PAYLEG']},
        cond={'summit_extract':[['AsOfDate','=',date]]}).\
        rename(columns={'DEAL_NBR':'CUSIP','BOOK':'Book',\
        'EXCHANGE_RATE_PAYLEG':'FX','CONT_NOTIONAL_AMT_PAYLEG':'Par_Local'})
    summit_extract.loc[:,'Par'] = summit_extract.loc[:,'Par_Local'] * \
        summit_extract.loc[:,'FX']
    books_in_IP = summit_books_list.loc[summit_books_list.In_IP == 'Yes',
        'Book'].tolist()
    books = [x for x in summit_extract.Book.tolist() if x in books_in_IP]
    summit_extract = summit_extract.loc[summit_extract.Book.isin(books),:]
    # Restrict QRM report to swaps in IP according to Summit
    summit_comp = summit_comp.loc[summit_comp.QRM_CUSIP.isin(\
        summit_extract.CUSIP.tolist()),:]
    # Show comparison per book
    summit_comp = pd.merge(summit_comp,summit_extract[['CUSIP','Book']],\
        left_on='QRM_CUSIP',right_on='CUSIP',how='left')
    QRM_agg = summit_comp.groupby('Book').agg({'QRM_CUSIP':['count'],\
        'QRM_Par':['sum']}).reset_index()
    summit_agg = summit_extract.groupby('Book').agg({'CUSIP':['count'],\
        'Par':['sum']}).reset_index()
    summit_agg = pd.merge(summit_agg,QRM_agg,on='Book',how='left')
    print('\n\nSummit completeness check:\n')
    print(summit_agg[['Book','CUSIP','QRM_CUSIP','Par','QRM_Par']])
    
    # Get list of bonds missing from QRM (if any)
    summit_missing = [x for x in summit_extract.CUSIP.tolist() if x not in \
        summit_comp.QRM_CUSIP.tolist()]
    if len(summit_missing)>0:
        summit_extract_original = ms_access_data_mgmt.fetch_files(\
            'summit_extract',cond={'summit_extract':[['AsOfDate','=',date]]})
        print('\nSummit instruments not in QRM report: \n')
        print summit_extract_original.loc[\
            summit_extract_original.DEAL_NBR.isin(summit_missing),:]
    
    ## Wall Street
    # Get Wall Street report date (one month behind) and extract
    WSS_dates = np.unique(ms_access_data_mgmt.fetch_files('WSS_extract',\
        columns={'WSS_extract':['AsOfDate']}))
    WSS_date = [fc.format_date(x) for x in WSS_dates if \
        np.abs((np.datetime64(pd.to_datetime(date)) - x)/np.timedelta64(1,'D')\
        - 30.) < 5][0]
    WSS_extract = ms_access_data_mgmt.fetch_files('WSS_extract',\
        columns={'WSS_extract':['DEAL_NUMBER','CURR_BAL','BASE_EQUIV_RATE',\
        'CCY']},cond={'WSS_extract':[['AsOfDate','=',WSS_date]]}).\
        rename(columns={'DEAL_NUMBER':'CUSIP','CCY':'Currency',\
        'CURR_BAL':'Par_Local','BASE_EQUIV_RATE':'FX_WSS'})
    # Convert FX in USD per local currency (nb below 1 for currencies in WSS)
    WSS_extract.loc[:,'FX'] = WSS_extract.apply(lambda x:\
        x.FX_WSS if x.FX_WSS < 1. else 1./x.FX_WSS,axis=1)
    WSS_extract.loc[:,'Par'] = WSS_extract.loc[:,'Par_Local'] * \
        WSS_extract.loc[:,'FX']
    # Keep instruments in Wall Street extract
    WSS_QRM = IP.loc[IP.CUSIP.isin(WSS_extract.CUSIP.tolist()),:].rename(\
        columns={'CUSIP':'QRM_CUSIP','Face Amount':'QRM_Par'})
    WSS_QRM_agg = WSS_QRM.groupby('Currency').agg({'QRM_CUSIP':['count'],\
        'QRM_Par':['sum']}).reset_index()
    WSS_agg = WSS_extract.groupby('Currency').agg({'CUSIP':['count'],\
        'Par':['sum']}).reset_index()
    WSS_agg = pd.merge(WSS_agg,WSS_QRM_agg,on='Currency',how='left')
    print('\n\nWall Street System completeness check:\n')
    print(WSS_agg[['Currency','CUSIP','QRM_CUSIP','Par','QRM_Par']])
    
    # Get list of bonds missing from QRM (if any)
    WSS_missing = [x for x in WSS_extract.CUSIP.tolist() if x not in \
        WSS_QRM.QRM_CUSIP.tolist()]
    if len(WSS_missing)>0:
        WSS_extract_original = ms_access_data_mgmt.fetch_files('WSS_extract',\
            cond={'WSS_extract':[['AsOfDate','=',WSS_date]]})
        non_empty_cols = []
        missing_df = WSS_extract_original.loc[\
            WSS_extract_original.DEAL_NUMBER.isin(WSS_missing),:]
        for c in WSS_extract_original.columns:
            if pd.notnull(missing_df[c]).any():
                non_empty_cols += [c]
        print('\nWall Street instruments not in QRM report: \n')
        print missing_df[non_empty_cols]
    
    
    ### If connection is still open close it
    if connection.State:
        connection.Close()

    return IP, all_shocks
###############################################################################
#def parse_qrm_valuation_excel_file(filename,file_location,no_cusip_info):
#    """ Parameters for this function are the same as the ones for function
#    fetch_qrm_valuation_data. The outputs are the same, the difference is
#    that this function parses the Excel file and not the OLAP report directly.
#    It can only be used if the Excel file has been formatted in the expected
#    way beforehand: tabs KRS, IP_Details, IP_Currencies contain the information
#    needed with the expected columns (including the column CUSIP). The tabs
#    should contain the CUSIPs in the Investment Portfolio, as well as the 
#    CTE Swaps and Wall Street System instruments manually added (except in
#    the currencies tab, their currency is taken from no_cusip_info).
#    """
#    ### Load file and the three tabs of interest
#    os.chdir(file_location)
#    QRM_wb = pd.ExcelFile(filename)
#    # All sensitivities
#    KRS = ms_access_data_mgmt.excel_tab_to_df(QRM_wb,filename,'KRS')
#    # Details for each bond
#    IP_details = ms_access_data_mgmt.excel_tab_to_df(\
#        QRM_wb,filename,'IP_Details',header_offset=0)
#    # Currencies for each bond
#    bond_currencies = ms_access_data_mgmt.excel_tab_to_df(\
#        QRM_wb,filename,'IP_Currencies',header_offset=0)
#    
#    
#    ### Keep non null CUSIPs in key rate and details tabs
#    KRS = KRS.loc[pd.notnull(KRS.CUSIP),:]
#    # Remove empty CUSIPs in details tab
#    IP_details = IP_details.loc[pd.notnull(IP_details.CUSIP),:]
#    
#    ### Get currency of each bond
#    # Keep entries with an ID, delete column ALL
#    bond_currencies = bond_currencies.loc[pd.notnull(bond_currencies.CUSIP),:]
#    bond_currencies.set_index('CUSIP',inplace=True)
#    bond_currencies = fc.remove_columns(bond_currencies,'ALL')
#    # Replace numbers with the currency's 3-character ID
#    for ccy in bond_currencies.columns:
#        bond_currencies.loc[pd.notnull(bond_currencies.loc[:,ccy]),ccy] = ccy[:3]
#    # Create column Currency for each bond
#    bond_currencies.loc[:,'Currency'] = bond_currencies.apply(lambda x:\
#        [y for y in x if pd.notnull(y)][0],axis=1)
#    # Reset index column as CUSIP
#    bond_currencies.reset_index(inplace=True)
#    # Add currency of instruments with no CUSIP: no CUSIP info data
#    bond_currencies = bond_currencies.append(\
#        no_cusip_info.loc[:,['CUSIP','Currency']])
#    
#    # Add currency to details tab
#    IP_details = IP_details.merge(bond_currencies.loc[:,['CUSIP','Currency']],
#                                  how='left')
#
#    return IP_details, KRS
###############################################################################
def number_to_cusip(raw_cusips):
    """ raw_cusips: list of CUSIPs as they appear in the HFV file
    This function takes a list of CUSIPs (coming from the HFV file for 
    instance) and finds the ones that might be transformed into numbers once
    pasted into Excel (because they are of the form xEy or xEEy where x and y
    are numbers, making the CUSIPs similar to scientific format numbers).
    It returns a dataframe containing 2 columns: Security with the original
    CUSIP, and number for the equivalent CUSIP tranformed into a number
    """
    results_dict = {}
    # Loop through Securities and find CUSIPs of interest by trying to 
    # transform them into numbers
    for c in raw_cusips:
        try:
            y = np.float(c)
            results_dict.update({c:y})
        except:
            pass
    
    df = pd.DataFrame(list(results_dict.iteritems()),
                      columns=['Security','Number'])
    return df
###############################################################################
def fix_cusip_number(df,cusip_as_number,df_col):
    """ df: dataframe with a column containing CUSIPs that might be formatted
        as numbers
        cusip_as_number: dataframe containing a map from Security (properly
        formatter as a CUSIP) to number (equivalent in number). It is the 
        output of function number_to_cusip.
        df_col: name of the column in df where the CUSIPs are located
        returns the original dataframe where only the column df_col has been
        modified, with CUSIPs formatted as numbers fixed
    """
    # Loop trhough the CUSIP column of the dataframe and find CUSIPs in number
    # format
    for i in df.index:
        try:
            y = np.float(df.loc[i,df_col])
            # Compute difference with numbers in cusip_as_number to find
            # the closest one
            cusip_as_number.loc[:,'Diff'] = np.abs(cusip_as_number.Number-y)
            # Replace CUSIP if difference is smaller than 1
            if min(cusip_as_number.loc[:,'Diff']) <= 1.0:
                closest_idx = min(enumerate(cusip_as_number.Diff.tolist()),\
                                  key=itemgetter(1))[0]
                # Replace CUSIP in original dataframe
                df.loc[i,df_col] = cusip_as_number.loc[closest_idx,'Security']
        except:
            pass
    
    return df
###############################################################################
def adjust_UST_from_QRM(hfv_prelim,IP_details,KRS):
    """ hfv_prelim: preliminary HFV file
        IP_details: IP_details as extracted from QRM report
        KRS: Key Rates Sensitivities report as extracted from QRM report
        This function looks in the HFV file to find US Treasuries and adjusts
    the QRM extracts if there is a difference in the listed securities in the 
    HFV and QRM due to transfer of US Treasuries from Boston to GmbH that led
    some of the UST tickets to be renamed (eg: CUSIP goes from 912828J43 to
    US912828J439 in the HFV file while all staying as 912828J43 in QRM)
    """
    ## Get list of US Treasuries in HFV file
    hfv_prelim.loc[:,'SEC1'] = hfv_prelim.apply(lambda x:\
        np.str(x['SEC 1']).upper(),axis=1)
    hfv_UST = hfv_prelim.loc[hfv_prelim.SEC1 == 'TREASURIES',:]
    hfv_UST_bonds = np.unique(hfv_UST.Security).tolist()
    
    ## From bonds with modified CUSIP (with prefix 'US') get original CUSIP
    possible_UST_cusip = [x[2:-1] for x in hfv_UST_bonds if x[:2] == 'US']
    UST_cusips = np.unique(hfv_UST_bonds + possible_UST_cusip)
    
    ## Compare Notional amounts in HFV per UST with QRM extract
    UST_agg = hfv_UST.groupby('Security').sum()['Par/Curr_Face USD_Equiv'].\
        reset_index()
    UST_agg.columns = ['Security','Par']
    hfv_qrm = pd.merge(pd.DataFrame(data=UST_cusips,columns=['Security']),
                       UST_agg,on='Security',how='left')
    hfv_qrm.loc[pd.isnull(hfv_qrm.Par),'Par'] = 0
    hfv_qrm = pd.merge(hfv_qrm,IP_details[['CUSIP','Face Amount']],
        left_on='Security',right_on='CUSIP',how='left').rename(\
        columns={'Face Amount':'ParQRM'})
    hfv_qrm.loc[pd.isnull(hfv_qrm.ParQRM),'ParQRM'] = 0
    hfv_qrm.loc[:,'Diff'] = hfv_qrm.loc[:,'Par'] - hfv_qrm.loc[:,'ParQRM']
    
    # Only keep bonds for which there is a difference
    hfv_qrm = hfv_qrm.loc[np.abs(hfv_qrm.Diff) > 1.,:]
    
    if len(hfv_qrm) > 0:
        # Add rows for UST in HFV but not in QRM
        hfv_qrm.loc[:,'QRM_CUSIP'] = hfv_qrm.apply(lambda x:\
            x.Security[2:-1] if pd.isnull(x.CUSIP) else None,axis=1)
        hfv_qrm.loc[pd.isnull(hfv_qrm.CUSIP),'CUSIP'] = hfv_qrm['Security']
        
        copy_bonds = np.unique(hfv_qrm.loc[pd.notnull(hfv_qrm.QRM_CUSIP),\
            'QRM_CUSIP'])
        for c in copy_bonds:
            hfv_qrm.loc[hfv_qrm.QRM_CUSIP == c,'ParQRM'] = \
                hfv_qrm.loc[hfv_qrm.CUSIP == c,'ParQRM'].values[0]
        
        # Split metrics between different instances of the same CUSIP in QRM
        hfv_qrm.loc[:,'Ratio'] = hfv_qrm.loc[:,'Par'] / hfv_qrm.loc[:,'ParQRM']
        
        ## Add new rows to QRM outputs
        details_copy = pd.merge(IP_details.loc[IP_details.CUSIP.isin(\
            copy_bonds),:].copy(),hfv_qrm[['QRM_CUSIP','Security']],
            left_on='CUSIP',right_on='QRM_CUSIP',how='left')
        KRS_copy = pd.merge(KRS.loc[KRS.CUSIP.isin(copy_bonds),:].copy(),\
            hfv_qrm[['QRM_CUSIP','Security']],
            left_on='CUSIP',right_on='QRM_CUSIP',how='left')
        details_copy.loc[:,'CUSIP'] = details_copy.loc[:,'Security']
        KRS_copy.loc[:,'CUSIP'] = KRS_copy.loc[:,'Security']
        IP_details = IP_details.append(details_copy[IP_details.columns.tolist()],\
            ignore_index=True)
        KRS = KRS.append(KRS_copy[KRS.columns.tolist()],ignore_index=True)
        
        ## Add ratio and scale necessary rows and columns
        # Details dataframe
        IP_details = pd.merge(IP_details,hfv_qrm[['CUSIP','Ratio']],on='CUSIP',
                              how='left')
        scaled = pd.notnull(IP_details.Ratio)
        details_cols = ['Face Amount','Book Value','Market Value',\
            'Economic Value','BPV','Convexity','Vega','Dollar Option Cost',\
            'Spread BPV']
        IP_details.loc[scaled,details_cols] = IP_details.loc[scaled,details_cols].\
            mul(IP_details.loc[scaled,'Ratio'],axis=0)
        # Key Rate Sensitivities
        KRS = pd.merge(KRS,hfv_qrm[['CUSIP','Ratio']],on='CUSIP',how='left')
        scaled = pd.notnull(KRS.Ratio)
        KRS_cols = KRS.columns.tolist()[1:]
        KRS.loc[scaled,KRS_cols] = KRS.loc[scaled,KRS_cols].\
            mul(KRS.loc[scaled,'Ratio'],axis=0)
            
        # Remove unnecessary columns
        IP_details = fc.remove_columns(IP_details,'Ratio')
        KRS = fc.remove_columns(KRS,'Ratio')
        
    return IP_details, KRS
###############################################################################



